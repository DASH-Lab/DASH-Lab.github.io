---
layout: page
permalink: /Dataset_kor/

---
<p><a href="../Dataset/">Eng</a></p>


<h1 class="page-title">COCO spliced datasets</h1>
  <p> 우리는 <a href="https://cocodataset.org/#home">COCO dataset</a>데이터셋을 활용하여 조작된 데이터셋을 생성했습니다. 이 데이터셋에서 제공된 레이블 (정답 마스크)이 포함되어 있어, 먼저 마스크를 적용해 원본 이미지에서 원하는 부분을 식별했습니다. 그런 다음 이 특정 영역을 다른 이미지에 적용하여 조작했습니다. 각 이미지에는 약 8~10개의 조작용 객체가 사용되었으며, 그 결과 약 90만 개의 조작된 이미지가 생성되었습니다.<p><br>
  
 <p align=center><img  loading="lazy" border=0  src="/Publications/Screen Shot 2023-12-11 at 2.15.08 PM.png" width="780"></p> 

<h1 class="page-title">Satellite Forgery Image Dataset</h1>
  <p> 우리는 <a href="http://deepglobe.org/">DeepGlop dataset</a> 데이터셋과 <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w39/Horvath_Manipulation_Detection_in_Satellite_Images_Using_Deep_Belief_Networks_CVPRW_2020_paper.pdf">Deep Belief networks</a>. 네트워크에서 제안된 방법을 사용하여 위성 조작 이미지를 생성했습니다. 해상도가 1000×1000인 293개의 정사보정(orthorectified) 이미지를 수집하였으며, 이 중 100개의 이미지를 사용해 조작된 이미지를 만들었습니다. 19개의 다양한 객체를 100개의 이미지에 합성하여 총 500개의 조작된 이미지와 정답 마스크를 생성했습니다. 19개의 객체에는 로켓, 비행기, 드론 이미지 등이 포함되어 있습니다. 아래 그림은 조작된 데이터셋의 예시를 보여줍니다.<p><br>
  
 <p align=center><img  loading="lazy" border=0  src="/Publications/satellite_forgery.png" width="780"></p> 


<h1 class="page-title">RWDF-23 Dataset</h1>
  <p>RWDF-23 데이터셋은 21개국에서 4가지 언어를 대상으로 하는 2,000개의 딥페이크 비디오로부터 수집되었고, 수집된 플랫폼은 Reddit, YouTube, TikTok 그리고 Bilibili입니다. 저희는 기존 연구를 넘어 데이터셋의 스코프를 확장함으로써, 광범위한 real-world 딥페이크 콘텐츠를 캡처하였고, 이는 끊임없이 진화하는 온라인 플랫폼 환경을 반영합니다. 또한, creator, manipulation strategy, purpose, real-world content production method 등 다양한 측면을 포괄하는 종합적인 분석을 진행하였습니다. 이를 통해, 다양한 맥락에서 딥페이크의 뉘앙스와 특성에 대한 통찰력을 얻을 수 있을 것입니다. 마지막으로, 비디오 콘텐츠 외에도 시청자의 댓글과 상호작용을 수집하여, 딥페이크 콘텐츠에 대한 사용자들의 참여를 탐색할 수 있게 하였습니다.  <p><br>
  
 <p align=center><img border=0  src="/Publications/rwdf23_cikm23.png" width="780"></p> 
 
<h1 class="page-title">FakeAVCeleb Dataset</h1>
  <p>FakeAVCeleb에서, 저희는 딥페이크 비디오뿐만 아니라 립싱크로부터 합성된 fake 오디오도 포함하는 새로운 Audio-Video 딥페이크 데이터셋을 제안하였습니다. 저희의 FakeAVCeleb은 가장 유명한 최신 딥페이크 제작 방식으로 만들어졌습니다. 더 사실적인 데이터셋을 만들기 위해, 저희는 4개 인종(Caucasian, Black, East Asian, South Asian) 연예인의 YouTube 영상을 선택하여 인종에 bias 되는 문제를 해결하였습니다.
    <p><br>
  
 <p align=center><img border=0  src="/img/teaser.png" width="1080"></p> 


<h1 class="page-title">VFP290K dataset</h1>
  <p>Vision-based Fallen Person (VFP290K) 데이터셋은 49개의 배경, 131개의 장면을 갖는 178개의 비디오로부터 낙상사고를 당한 사람에 대해 294,714개의 프레임을 추출하여 만들어졌습니다. 저희는 object detection 모델에 따른 성능 변화를 광범위한 실험을 통해 비교하였고, feature의 효과를 입증할 수 있었습니다. 또한, 저희는 낙상 감지 시스템의 성능을 측정하여 데이터세트를 평가하였습니다. 저희는 VFP290K 데이터셋을 사용하여, 2020 AI Grand Challenge의 비정상 행동 탐지 track의 첫 번째 라운드에서 1위를 달성하였고, 이는 지능형 CCTV나 감시 시스템과 같은 곳에 확대 적용될 가능성을 보여줍니다. <p><br>
  
 <p align=center><img border=0  src="/img/datasets/VFP290k/VFP.JPG"  width="1080"></p> 


<h1 class="page-title">SKKU AGC Anomaly Detection Dataset</h1>
  <p>SKKU AGC Anomaly Detection Dataset은 다양한 장소에서 낮과 밤 모두에 대해 보행자가 내려다 보이는 높이에 카메라를 고정하여 촬영하였으며 Detection Data와 Classificaiton Data로 구성되어 있습니다. Adnomal event는 사람의 머리가 땅에 닿는 경우입니다.<p><br>
  
<h6><b>1. Detection Data</b></h6>
  <p>1920x1080 크기의 이미지와 anomaly label(.xml)로 구성되어 있으며, 이미지들은 day와 night 폴더에 있으며, label들은 day_anno, night_anno 폴더에 있습니다.</p>
  <p>day: 3000<br>night: 2000</p><br>

<h6><b>2. Classification Data</b></h6>
  <p>사람을 크롭한 이미지로 구성되어 있으며, nomal과 falldown의 두가지 클래스가 있습니다. 정상 이미지는 normal day와 normal night 폴더에 있고, falldown 이미지는 falldown_day와 falldown_night 폴더에 있습니다.</p>
  <p>normal_day: 3200<br>normal_night: 1300<br>falldown_day: 3700<br>falldown_night: 900</p>
  <br>
<p><b>Dataset Link</b></p>
  <a href="https://drive.google.com/drive/folders/1JfEMxKb70GSEEUKMBqr62UFOsMbpPK8s?usp=sharing">SKKU AGC Anomaly Detection Dataset</a><br><br>
<hr>
<p><b>Examples of Detection data</b></p>
<table>
  <tr>
    <td width="33%" valign=top>
      <p align=center><img border=0 width=240 height=135 src="/img/datasets/VFP290k/AGC_detection.jpg"></p>
    </td>
    <td width="33%" valign=top>
      <p align=center><img border=0 width=240 height=135 src="/img/datasets/VFP290k/AGC_detection3.jpg"></p>
    </td>
    <td width="33%" valign=top>
      <p align=center><img border=0 width=240 height=135 src="/img/datasets/VFP290k/AGC_detection4.jpg"></p>
    </td>
  </tr>
  <tr>
    <td width="33%" valign=top>
      <p align=center><img border=0 width=240 height=135 src="/img/datasets/VFP290k/AGC_detection1.jpg"></p>
    </td>
    <td width="33%" valign=top>
      <p align=center><img border=0 width=240 height=135 src="/img/datasets/VFP290k/AGC_detection2.jpg"></p>
    </td>
    <td width="33%" valign=top>
      <p align=center><img border=0 width=240 height=135 src="/img/datasets/VFP290k/AGC_detection5.jpg"></p>
    </td>
  </tr>
</table>
<br>
<hr>
<p><b>Examples of Classification data</b></p>
<table>
  <tr>
    <td width="33%" valign=top>
      <p align=center><img border=0  src="/img/datasets/VFP290k/AGC_classification.jpg"></p>
    </td>
    <td width="33%" valign=top>
      <p align=center><img border=0  src="/img/datasets/VFP290k/AGC_classification1.jpg"></p>
    </td>
    <td width="33%" valign=top>
      <p align=center><img border=0  src="/img/datasets/VFP290k/AGC_classification2.jpg"></p>
    </td>
  </tr>
  <tr>
    <td width="33%" valign=top>
      <p align=center><img border=0  src="/img/datasets/VFP290k/AGC_classification3.jpg"></p>
    </td>
    <td width="33%" valign=top>
      <p align=center><img border=0  src="/img/datasets/VFP290k/AGC_classification4.jpg"></p>
    </td>
    <td width="33%" valign=top>
      <p align=center><img border=0  src="/img/datasets/VFP290k/AGC_classification5.jpg"></p>
    </td>
  </tr>
</table>
