<!DOCTYPE html>
<html lang="en">

  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1.0"/>
    
    <meta http-equiv='cache-control' content='no-cache'> 
    <meta http-equiv='expires' content='0'> 
    <meta http-equiv='pragma' content='no-cache'>

    <title>Publications</title>
    <link rel="icon" type="image/png" href="../img/dash_logo/lab_icon.png"> 
	
    <meta name="description" content="Developing Usable and Secure Technology after Better Understanding Data, Machines, and Humans" />

	
	<!-- flickity courasel -->
    <link rel="stylesheet" href="../assets/flickity.css" media="screen">
	<!-- Font Awesome Icons -->
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link rel="stylesheet" type="text/css" media="screen,projection" href="/assets/materialize.css" />
    <link rel="stylesheet" type="text/css" media="screen,projection" href="/assets/main.css">
    <link rel="canonical" href="http://localhost:4000/Publications/" />
    <link rel="alternate" type="application/rss+xml" title="DASH LAB" href="/feed.xml" />

	
    
</head>


  <body>

    <nav class="light-blue darken-4" role="navigation"  style="height: 90%;vertical-align: middle;">
    <div class="nav-wrapper container">
        <a id="logo-container" class="brand-logo" href="/">
            <a  href="https://dash-lab.github.io/">
                <img width="213" height="58" src="../img/dash_logo/lab_logo_text.png" alt="data" class="image" style="margin:24px">
 
                
            </a>
            
        </a>
    
    

          <ul class="right hide-on-med-and-down"  style="margin:24px">
            <li ><a href="/" style="font-size:17px">Home </a></li>
            
                
                    
                    
                
                    
                    
                    <li><a href="/Professor/" style="font-size:17px">Professor</a></li>
                    
                
                    
                    
                    <li><a href="/Dataset/" style="font-size:17px">Dataset</a></li>
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
                    
                    
                    <li><a href="/Members/" style="font-size:17px">Members</a></li>
                    
                
                    
                    
                    <li><a href="/News/" style="font-size:17px">News</a></li>
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
                    
                    
                    <li><a href="/Projects/" style="font-size:17px">Projects</a></li>
                    
                
                    
                    
                    <li><a href="/Publications/" style="font-size:17px">Publications</a></li>
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
                          
          </ul>

          <ul id="nav-mobile" class="side-nav">
            <li><a href="#">Home</a></li>
                            
                
                    
                    
                
                    
                    
                    <li><a href="/Professor/">Professor</a></li>
                    
                
                    
                    
                    <li><a href="/Dataset/">Dataset</a></li>
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
                    
                    
                    <li><a href="/Members/">Members</a></li>
                    
                
                    
                    
                    <li><a href="/News/">News</a></li>
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
                    
                    
                    <li><a href="/Projects/">Projects</a></li>
                    
                
                    
                    
                    <li><a href="/Publications/">Publications</a></li>
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
                    
                    
                
             
          </ul>        

      <a href="#" data-activates="nav-mobile" class="button-collapse"><i class="material-icons">menu</i></a>
    </div>
</nav>


    <main>
        <div class="container">

    <style>
                /* 표 기반 텍스트 블록을 컴팩트하게 */
        #publications-content table{
          border-collapse:collapse;
          width:100%;
          border: none;
        }
        #publications-content thead th{
          text-align:left;
          padding:0 0 4px 0;           /* 제목 아래 약간의 간격만 */
          line-height:1.2;
           border-bottom: none;
        }
        #publications-content tbody td{
          padding:0;                   /* tr/td 기본 패딩 제거 */
        }
        
        #publications-content small{
          font-size:0.92em;            /* small을 너무 작지 않게 */
        }

        .year-header {
            cursor: pointer;
            font-size: 20px;
            font-weight: bold;
            margin: 10px 0;
            color: #007BFF;
        }
        .year-header:hover {
            text-decoration: underline;
        }
        .publications {
            display: none;
            margin-left: 20px;
        }
        .year-section {
            display: none;
        }
        .year-section.active {
            display: block;
        }
        .pagination {
            text-align: center;
            margin: 40px 0;
            padding: 20px;
        }
        .pagination button {
            background-color: #f8f9fa;
            border: 1px solid #dee2e6;
            color: #007bff;
            padding: 8px 16px;
            margin: 0 4px;
            cursor: pointer;
            border-radius: 4px;
            font-weight: bold;
        }
        .pagination button:hover {
            background-color: #e9ecef;
        }
        .pagination button.active {
            background-color: #007bff;
            color: white;
            border-color: #007bff;
        }
        .pagination button:disabled {
            background-color: #e9ecef;
            color: #6c757d;
            cursor: not-allowed;
        }

        /* abstract 토글용 */
        .abstract-toggle {
            cursor: pointer;
            display: inline-flex;
            align-items: center;
            gap: 4px;
            font-size: 0.85em;
            color: #007bff;
        }
        .abstract-toggle img {
            height: 16px;
            width: 16px;
        }
        .abstract-content {
            display: none;
            margin-top: 4px;
        }
        .abstract-content.open {
            display: block;
        }

    </style>

<h1 class="page-title">Publications</h1>
<center>
<a href="https://github.com/DASH-Lab/deepfakeResearch" target="_blank"> 
<img loading="lazy" alt="deepfakes" src="https://img.shields.io/badge/Deepfakes Research-Visit-brightgreen" height="22" />
</a>
	
<a href="https://github.com/DASH-Lab/anomaly_detection_research" target="_blank"> 
<img loading="lazy" alt="anomaly" src="https://img.shields.io/badge/Anomaly Detection-Visit-brightgreen" height="22" />
</a>

<a href="https://github.com/DASH-Lab/ML_privacy_research" target="_blank"> 
<img loading="lazy" alt="privacyResearch" src="https://img.shields.io/badge/ML Privacy Research-Visit-brightgreen" height="22" />
</a>

<a href="https://github.com/DASH-Lab/FaceRecogRecovery" target="_blank"> 
<img loading="lazy" alt="FaceRecovery" src="https://img.shields.io/badge/Face Recog Recovery-Visit-brightgreen" height="22" />
</a>	
	
	
</center>

<div class="pagination">
  <button onclick="previousYear()" id="prevBtn">◄ Previous</button>
  
    <button onclick="showYear('2026')" class="year-btn active" data-year="2026">2026</button>
  
    <button onclick="showYear('2025')" class="year-btn" data-year="2025">2025</button>
  
    <button onclick="showYear('2024')" class="year-btn" data-year="2024">2024</button>
  
    <button onclick="showYear('2023')" class="year-btn" data-year="2023">2023</button>
  
    <button onclick="showYear('2022')" class="year-btn" data-year="2022">2022</button>
  
    <button onclick="showYear('2021')" class="year-btn" data-year="2021">2021</button>
  
    <button onclick="showYear('2020')" class="year-btn" data-year="2020">2020</button>
  
    <button onclick="showYear('2019')" class="year-btn" data-year="2019">2019</button>
  
    <button onclick="showYear('2018')" class="year-btn" data-year="2018">2018</button>
  
  
    <button onclick="showYear('older')" class="year-btn" data-year="older">2017 &amp; Earlier</button>
  
  <button onclick="nextYear()" id="nextBtn">Next ►</button>
</div>

<div id="publications-content">
  
  
    
    <div class="year-section active" id="year-2026">
      <h4 style="margin-top:40px"><b>2026</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Machine Pareidolia: Protecting Facial Images with Emotional Editing</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Binh M. Le and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      AAAI Conference on Artificial Intelligence
                      (AAAI)
                      , 2026
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The proliferation of facial recognition (FR) systems has raised privacy concerns in the digital realm, as malicious uses of FR models pose a significant threat. Traditional countermeasures, such as makeup style transfer, have suffered from low transferability in black-box settings and limited applicability across various demographic groups, including males and individuals with darker skin tones. To address these challenges, we introduce a novel facial privacy protection method, dubbed MAP, a pioneering approach that employs human emotion modifications to disguise original identities as target identities in facial images. Our method uniquely fine-tunes a score network to learn dual objectives, target identity and human expression, which are jointly optimized through gradient projection to ensure convergence at a shared local optimum. Additionally, we enhance the perceptual quality of protected images by applying local smoothness regularization and optimizing the score matching loss within our network. Empirical experiments demonstrate that our innovative approach surpasses previous baselines, including noise-based, makeup-based, and freeform attribute methods, in both qualitative fidelity and quantitative metrics. Furthermore, MAP proves its effectiveness against an online FR API and shows advanced adaptability in uncommon photographic scenarios.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/aaai2026_binhle.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>AEON: Adaptive Embedding Optimized Noise for Robust Watermarking in Diffusion Models</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Muhammad Shahid Muneer and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      The IEEE/CVF Winter Conference on Applications of Computer Vision
                      (WACV)
                      , 2026
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The widespread use of synthetic image generation models and the challenges associated with authenticity preservation have fueled the demand for robust watermarking methods to safeguard authenticity and protect the copyright of synthetic images. Existing watermarking methods embed. Invisible signatures in synthetic images often compromise image quality and remain susceptible to multiple watermark removal attacks, including reconstruction and forgery methods. To overcome this issue, we propose a novel watermarking approach, AEON, which seamlessly integrates the watermark into the latent diffusion process and ensures the watermark aligns with scene semantics in the final image. Unlike existing invisible in-diffusion watermarking and traditional hash-based methods, our approach adapts the neural synthesized hash-based watermark to the semantics of the generated image during the intermediate diffusion process instead of embedding traditional hashes with the initial noise. This facilitates visual coherence in the generated image while enhancing adversarial robustness and resilience against single or multiple adversarial and traditional watermark removal attacks. Our proposed approach a) modulates the noise sampling in each diffusion denoising iteration through a learnable watermark embedding, b) optimizes consistency, reconstruction, and similarity loss, enforcing local and global alignment between the watermark structure and the underlying image content, and c) generates a strong watermark by allowing late embedding of the watermark in the diffusion process. Empirical results demonstrate the effectiveness of the proposed approach in retaining quality and its robustness against cumulative adversarial attacks.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/2026_WACV_Shahid_AEON.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
    </div>
  
    
    <div class="year-section" id="year-2025">
      <h4 style="margin-top:40px"><b>2025</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>RUAGO: Effective and Practical Retain-Free Unlearning via Adversarial Attack and OOD Generator</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sangyong Lee,
                            
                          
                            
                              Sangjun Chung,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Neural Information Processing Systems
                      (NeurIPS)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>With increasing regulations on private data usage in AI systems, machine unlearning has emerged as a critical solution for selectively removing sensitive information from trained models while preserving their overall utility. While many existing unlearning methods rely on the retain data to mitigate the performance decline caused by forgetting, such data may not always be available (retain-free) in real-world scenarios. To address this challenge posed by retain-free unlearning, we introduce RUAGO, utilizing adversarial soft labels to mitigate over-unlearning and a generative model pretrained on out-of-distribution (OOD) data to effectively distill the original model’s knowledge. We introduce a progressive sampling strategy to incrementally increase synthetic data complexity, coupled with an inversion-based alignment step that ensures the synthetic data closely matches the original training distribution. Our extensive experiments on multiple benchmark datasets and architectures demonstrate that our approach consistently outperforms existing retain-free methods and achieves comparable or superior performance relative to retain-based approaches, demonstrating its effectiveness and practicality in real-world, data-constrained environments.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/nips2025_sangyong.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Through the Lens: Benchmarking Deepfake Detectors Against Moiré-Induced Distortions</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Razaib Tariq,
                            
                          
                            
                              Minji Heo,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Shahroz Tariq
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Neural Information Processing Systems
                      (NeurIPS)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Dataset Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Deepfake detection remains a pressing challenge, particularly in real-world settings where smartphone-captured media from digital screens often introduces Moiré artifacts that can distort detection outcomes. This study systematically evaluates state-of-the-art (SOTA) deepfake detectors on Moiré-affected videos an issue that has received little attention. We collected a dataset of 12,832 videos, spanning 35.64 hours, from Celeb-DF, DFD, DFDC, UADFV, and FF++ datasets, capturing footage under diverse real-world conditions, including varying screens, smartphones, lighting setups, and camera angles. To further examine the influence of Moiré patterns on deepfake detection, we conducted additional experiments using our DeepMoiréFake, referred to as (DMF) dataset, and two synthetic Moiré generation techniques. Across 15 top-performing detectors, our results show that Moiré artifacts degrade performance by as much as 25.4%, while synthetically generated Moiré patterns lead to a 21.4% drop in accuracy. Surprisingly, demoiréing methods, intended as a mitigation approach, instead worsened the problem, reducing accuracy by up to 16%. These findings underscore the urgent need for detection models that can robustly handle Moiré distortions alongside other real-world challenges, such as compression, sharpening, and blurring. By introducing the DMF dataset, we aim to drive future research toward closing the gap between controlled experiments and practical deepfake detection.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/neurips-25-Razaib.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Minji Heo and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Multi-step or hybrid deepfakes, generated through successively applying different deepfake creation methods such as face-swapping, GAN-based generation, and Diffusion refinement, can pose an emerging challenge for detection models trained on single-step forgeries. While prior studies focus on isolated manipulations, little is known about model behavior under such compositional manipulation pipelines. In this work, we introduce FakeChain, a large-scale benchmark comprising 1-, 2-, and 3-Step manipulated face images synthesized using five state-of-the-art generators, including face-swap, GAN, and Diffusion models. Using this dataset, we analyze detection performance and spectral properties across manipulation depths, generator combinations, and quality settings. Our findings reveal that detection performance highly depends on the final manipulation step, with F1-score dropping by up to 58.83% when it differs from training. Detectors rely on shallow cues from the last stage, limiting generalization across multi-step forgeries. We also observe architectural differences in robustness to compression, with attention-based models being more sensitive than CNN-based ones. These insights highlight the need for detection models that account for manipulation history and benchmarks such as FakeChain that reflect the evolving nature of deepfake synthesis pipelines. We share some sample of our code here.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/2025_CIKM_minji.PNG" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Seeing Through the Blur: Unlocking Defocus Maps for Deepfake Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Minsun Jeon and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The rapid advancement of generative AI has enabled the mass production of photorealistic synthetic images, blurring the boundary between authentic and fabricated visual content. This challenge is particularly evident in deepfake scenarios involving facial manipulation, but also extends to broader AI-generated content (AIGC) cases that feature fully synthesized scenes. As such content becomes increasingly difficult to distinguish from reality, the integrity of visual media is undergoing threat. To address this issue, we propose a physically interpretable deepfake detection framework and demonstrate that defocus blur can serve as an effective forensic signal. Defocus blur is a depth-dependent optical phenomenon that naturally occurs in camera-captured images due to lens focus and scene geometry. In contrast, synthetic images often lack realistic depth-of-field (DoF) characteristics, resulting in globally sharp or physically inconsistent blur patterns. To capture these discrepancies, we construct a defocus blur map and use it as a discriminative feature for detecting manipulated content. Our approach is supported by three in-depth feature analyses, and experimental results confirm that defocus blur provides a reliable and interpretable cue for identifying synthetic images. We aim for our defocus-based detection pipeline and interpretability tools to contribute meaningfully to ongoing research in media forensics.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/DoF_G.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Anomaly Detection for Advanced Driver Assistance System with NCDE-based Normalizing Flow</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Kangjun Lee,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              Youngho Jun,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>For electric vehicles, the Adaptive Cruise Control (ACC) in Advanced Driver Assistance Systems (ADAS) is designed to assist braking based on driving conditions and user patterns. However, the driving data collected during development are limited and lack diversity, leading to late or aggressive braking. Moreover, it is necessary to effectively identify anomalies in braking patterns, which is critical for self-driving autonomous vehicles. We propose Graph Neural Controlled Differential Equation Normalizing Flow (GDFlow), which leverages Normalizing Flow (NF) with Neural Controlled Differential Equations (NCDE) to learn the distribution of normal driving patterns. Our approach captures spatio-temporal information from sensor data and accurately models continuous changes in driving patterns. Additionally, we introduce a quantilebased maximum likelihood objective to improve the likelihood estimate of normal data at the margin of the distribution. We validate GDFlow using real-world electric vehicle driving data that we collected from Hyundai IONIQ5 and GV80EV. Our model achieves state-of-the-art (SOTA) performance compared to nine baselines across four dataset configurations of different vehicle types and drivers. Furthermore, our model outperforms the latest anomaly detection methods across four time series benchmark datasets. Our approach demonstrates superior efficiency in inference time compared to existing methods. We plan to deploy GDFlow in the Hyundai Genesis GV90 by March 2026.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/2025CIKM_kangjun_minha.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>MU-OT: Effective and Unified Machine Unlearning with Optimal Transport for Feature Realignment</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Sangjun Chung and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Short Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=2</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Machine unlearning has emerged as a significant research topic in response to the increasing demands for data privacy and compliance with privacy regulations. The main challenge is to eliminate the influence of a specific subset of training data from a pretrained model while preserving the model’s performance on the retain set without retraining it from scratch. In this paper, we propose a novel efficient unlearning framework based on Optimal Transport, which can effectively work on class and instance-wise unlearning tasks. By analyzing and comparing the feature spaces of the original and retrained models, we formulate the unlearning problem as a distribution alignment task between the forget set and the retain set. We guide the feature distribution of the forget set, which initially forms distinct, structured patterns, to align with that of the retain set. In addition, we introduce a class-aware cost function for optimal transport that encourages inter-class transport, thereby enhancing the forgetting process. Extensive experiments on three public benchmark datasets demonstrate its superior effectiveness compared to previous SOTA methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/2025_CIKM_sangjun.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Beyond Masking: Landmark-based Representation Learning and Knowledge-Distillation for Audio-Visual Deepfake Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Chan Park,
                            
                          
                            
                              Muhammad Shahid Muneer,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Short Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=2</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Audio-visual deepfake detection methods demonstrate strong performance on academic datasets but fail significantly when applied to real-world deepfake content. To address the shortcomings of previous approaches, we introduce a landmark-guided knowledge-distillation framework, featuring two core innovations that enable the effective detection of real-world deepfakes. First, we propose Landmark-based Distillation (LBD), motivated by I-JEPA's representation learning approach. LBD utilizes KL-divergence to align facial landmark predictions from visual and audio encoders, enforcing focus on geometric facial features rather than spurious background information. Second, we introduce Multimodal Temporal Information Alignment (MTIA), which employs contrastive learning to enhance temporal consistency between audio and visual representations. We conduct extensive experiments on academic datasets and web-based deepfakes collected from diverse social media platforms, serving as real-world examples. Our proposed landmark-guided distillation framework achieves computational efficiency while improving multimodal video deepfake detection performance across a diverse range of deepfakes compared to existing methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/2025_CIKM_Chan.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Learning Interpersonal Similarities in Multiple Fingers via Fingerprint Landmark-Aware Recognition Network</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jiwon Kim,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Youjin Shin
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      IEEE International Joint Conference on Biometrics
                      (IJCB)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In fingerprint biometric systems, fingerprint recognition traditionally focuses on identifying individuals based on the distinct fingerprints of different fingers, which is finger-specific identity recognition (FsIR). However, real-world applications often require recognizing the same individual using fingerprints from different fingers, which is finger-agnostic identity recognition (FaIR). The FaIR task has proven challenging due to the prevailing assumption in the biometric field that there is no correlation between an individual’s different fingerprints. To address this issue, we propose a novel system, IP-Fing, which can learn the interpersonal similarity across the fingers. By using a pre-trained localization encoder to capture interpersonal fingerprint landmarks and the ArcFace marginal logit function, our IP-Fing recognition system can match a fingerprint query to all fingerprints of the same person while distinguishing them from others. We assess our method using comprehensive tests on two fingerprint datasets: our private fingerprint dataset, KORFing, which only has one sample per finger available, and the public fingerprint dataset, CASIA-v5, which has a few missing fingerprint samples for the task of finger-agnostic identity recognition (FaIR). IP-Fing achieves the best AUC with an average of 95.3074 across the two datasets, showing that our method is more effective in applying FaIR than conventional methods. Furthermore, IP-Fing demonstrates superior AUC with an average of 98.4631 across two datasets in the task of traditional finger-specific identity recognition (FsIR).</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/IJCB_jiwon.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              PRIYANKA SINGH,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Irena Irmalasari,
                            
                          
                            
                              Saakshi Gupta,
                            
                          
                            
                              and Dev Gupta
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Multimedia
                      (MM)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The proliferation of deepfake technologies poses urgent challenges and serious risks to digital integrity, particularly within critical sectors such as forensics, journalism, and the legal system. While existing detection systems have made significant progress in classification accuracy, they typically function as black-box models—offering limited transparency and minimal support for human reasoning. This lack of interpretability hinders their usability in real-world decision-making contexts, especially for non-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to Explanation), a novel multimodal framework that integrates visual, semantic, and narrative layers of explanation to make deepfake detection interpretable and accessible. The framework consists of three modular components: (1) a deepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual captioning module that generates natural language summaries of manipulated regions, and (3) a narrative refinement module that uses a fine-tuned Large Language Model (LLM) to produce context-aware, user-sensitive explanations. We instantiate and evaluate the framework on the DF40 benchmark, the most diverse deepfake dataset to date. Experiments demonstrate that our system achieves competitive detection performance while providing high-quality explanations aligned with Grad-CAM activations. Human evaluation with non-expert participants confirms the perceived usefulness, understandability, and trustworthiness of the generated narratives. By unifying prediction and explanation in a coherent, human-aligned pipeline, this work offers a scalable approach to interpretable deepfake detection—advancing the broader vision of trustworthy and transparent AI systems in adversarial media environments.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/Sharoz.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>PromptFlare: Prompt-Generalized Defense via Cross-Attention Decoy in Diffusion-Based Inpainting</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hohyun Na,
                            
                          
                            
                              Seunghoo Hong,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Multimedia
                      (MM)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The success of diffusion models has enabled effortless, high-quality image modifications that precisely align with users' intentions, thereby raising concerns about their potential misuse by malicious actors. Previous studies have attempted to mitigate such misuse through adversarial attacks. However, these approaches heavily rely on image-level inconsistencies, which pose fundamental limitations in addressing the influence of textual prompts. In this paper, we propose PromptFlare, a novel adversarial protection method designed to protect images from malicious modifications facilitated by diffusion-based inpainting models. Our approach leverages the cross-attention mechanism to exploit the intrinsic properties of prompt embeddings. Specifically, we identify and target shared token of prompts that are invariant and semantically uninformative, injecting adversarial noise to suppress the sampling process. Extensive experiments on the EditBench dataset demonstrate that our method achieves state-of-the-art performance across various CLIP-based and traditional metrics while significantly reducing computational overhead and GPU memory usage. These findings highlight PromptFlare as a robust and efficient protection against unauthorized image manipulations.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/figure2.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Inzamamul Alam,
                            
                          
                            
                              Md Tanvir Islam,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Multimedia
                      (MM)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The increasing realism of content generated by GANs and diffusion models has made deepfake detection significantly more challenging. Existing approaches often focus solely on spatial or frequency-domain features, limiting their generalization to unseen manipulations. We propose the Spectral Cross-Attentional Network (SpecXNet), a dual-domain architecture for robust deepfake detection. The core \textbf{Dual-Domain Feature Coupler (DDFC)} decomposes features into a local spatial branch for capturing texture-level anomalies and a global spectral branch that employs Fast Fourier Transform to model periodic inconsistencies. This dual-domain formulation allows SpecXNet to jointly exploit localized detail and global structural coherence, which are critical for distinguishing authentic from manipulated images. We also introduce the \textbf{Dual Fourier Attention (DFA)} module, which dynamically fuses spatial and spectral features in a content-aware manner. Built atop a modified XceptionNet backbone, we embed the DDFC and DFA modules within a separable convolution block. Extensive experiments on multiple deepfake benchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly under cross-dataset and unseen manipulation scenarios, while maintaining real-time feasibility. Our results highlight the effectiveness of unified spatial-spectral learning for robust and generalizable deepfake detection.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/high-level-dig.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Combating Dataset Misalignment for Robust AI-Generated Image Detection in the Real World</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hyeongjun Choi,
                            
                          
                            
                              Inho Jung,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3709022.3736541" target="_blank">
                      ACM ASIACCS Workshop on Security Implications of Deepfakes and Cheapfakes
                      (WDC)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>AI-generated images are increasingly prevalent on the web, raising concerns about the real-world applicability of detection methods. While current detectors perform well on benchmark datasets, they suffer significant performance degradation on real-world datasets. Misalignment within benchmark datasets, caused by discrepancies in how data from different classes are encoded or transformed, leads models to learn shortcuts. These shortcuts make detectors overly reliant on factors such as image compression, causing biased predictions of real-world images that inevitably undergo compression. In this work, we reveal the misalignment in widely used benchmark datasets and demonstrate that aligning datasets improves model robustness and generalizability. Additionally, we propose leveraging pre-trained visual encoders to further enhance performance in real-world scenarios. Our approach achieves significant performance gains, highlighting the importance of dataset alignment for real-world AI-generated image detection.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/WDC25.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Seunghoo Hong,
                            
                          
                            
                              Geonho Son,
                            
                          
                            
                              J.Y. Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.48550/arxiv.2510.00778" target="_blank">
                      International Conference on Computer Vision
                      (ICCV)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive performance.In this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/DIA-ICCV2025.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Translation of Text Embedding via Delta Vector to Suppress Strongly Entangled Content in Text-to-Image Diffusion Models</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Seunghoo Hong†,
                            
                          
                            
                              Eunseo Koh†,
                            
                          
                            
                              Tae-Young Kim†,
                            
                          
                            
                              Jae-Pil Heo,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      International Conference on Computer Vision
                      (ICCV)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Text-to-Image (T2I) diffusion models have made significant progress in generating diverse high-quality images from textual prompts. However, these models still face challenges in suppressing content that is strongly entangled with specific words. For example, when generating an image of "Charlie Chaplin", a "mustache" consistently appears even if explicitly instructed not to include it, as the concept of "mustache" is strongly entangled with "Charlie Chaplin". To address this issue, we propose a novel approach to directly suppress such entangled content within the text embedding space of diffusion models. Our method introduces a delta vector that modifies the text embedding to weaken the influence of undesired content in the generated image, and we further demonstrate that this delta vector can be easily obtained through a zero-shot approach. Furthermore, we propose a Selective Suppression with Delta Vector (SSDV) method to adapt the delta vector into the cross-attention mechanism, enabling more effective suppression of unwanted content in regions where it would otherwise be generated. Additionally, we enabled more precise suppression in personalized T2I models by optimizing the delta vector, which previous baselines were unable to achieve. Extensive experimental results demonstrate that our approach significantly outperforms existing methods, both in terms of quantitative and qualitative metrics.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/SSDV.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>SpecGuard: Spectral Projection-based Advanced InvisibleWatermarking</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Inzamamul Alam,
                            
                          
                            
                              Md Tanvir Islam,
                            
                          
                            
                              Khan Muhammad,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      International Conference on Computer Vision
                      (ICCV)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Watermarking embeds imperceptible patterns into images for authenticity verification. However, existing methods often lack robustness against various transformations primar ily including distortions, image regeneration, and adver sarial perturbation, creating real-world challenges. In this work, we introduce SpecGuard, a novel watermarking ap proach for robust and invisible image watermarking. Un like prior approaches, we embed the message inside hid den convolution layers by converting from the spatial do main to the frequency domain using spectral projection of a higher frequency band that is decomposed by wavelet pro jection. Spectral projection employs Fast Fourier Trans form approximation to transform spatial data into the fre quency domain efficiently. In the encoding phase, a strength factor enhances resilience against diverse attacks, includ ing adversarial, geometric, and regeneration-based distor tions, ensuring the preservation of copyrighted information. Meanwhile, the decoder leverages Parseval’s theorem to ef fectively learn and extract the watermark pattern, enabling accurate retrieval under challenging transformations. We evaluate the proposed SpecGuard based on the embedded watermark’s invisibility, capacity, and robustness. Compre hensive experiments demonstrate the proposed SpecGuard outperforms the state-of-the-art models.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ProposedModel-01.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>HiDF: A Human-Indistinguishable Deepfake Dataset</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Chaewon Kang,
                            
                          
                            
                              Seoyoon Jeong,
                            
                          
                            
                              Jonghyun Lee,
                            
                          
                            
                              Daejin Choi,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Jinyoung Han
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3711896.3737399" target="_blank">
                      ACM SIGKDD Conference on Knowledge Discovery and Data Mining
                      (KDD)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Dataset Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The rapid development and prevalence of generative AI have made it easy for people to create high-quality deepfake images and videos, but their abuses have also increased exponentially. To mitigate potential social disruption, it is crucial to quickly detect the authenticity of each deepfake content hidden in a sea of information. While researchers have worked on developing deep learning-based methods, the deepfake datasets utilized in these studies are far from the real world in terms of their qualities; most popular deepfake datasets are human-distinguishable. To address this problem, we present a novel deepfake dataset, HiDF, a high-quality and humanindistinguishable deepfake dataset consisting of 62 K images and 8 K videos. HiDF is a meticulously curated dataset that includes diverse subjects that have undergone rigorous quality checks. A comparison of the quality between HiDF and existing deepfake datasets demonstrates that HiDF is human-indistinguishable. Hence, it can be a valuable benchmark dataset for deepfake detection tasks.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/KDD2025_HiDF.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>SEE: Spherical Embedding Expansion for Improving Deep Metric Learning (Extended Abstract)</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Binh M. Le and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.24963/ijcai.2024/1214" target="_blank">
                      International Joint Conference on Artificial Intelligence
                      (IJCAI)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We introduce the Spherical Embedding Expansion (SEE) method. SEE aims to uncover the latent semantic variations in training data. Especially, our method augments the embedding space with synthetic representations based on Max-Mahalanobis distribution (MMD) centers, which maximize the dispersion of these synthetic features without increasing computational costs.We evaluated the efficacy of SEE on four renowned standard benchmarks for the image retrieval task. The results demonstrate that SEE consistently enhances the performance of conventional methods when integrated with them, setting a new benchmark for deep metric learning performance across all settings.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/binhle_pakdd2024.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Toward a robust approach to multivariate time series anomaly detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Jungwook Shon and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1117/12.3050401" target="_blank">
                      Metrology, Inspection, and Process Control XXXIX
                      (JM3)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =1.5</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Anomaly detection in semiconductor manufacturing is critical for maintaining yield and reducing costs, especially in high-volume production environments where inspections are resource-intensive. This study presents a robust, unsupervised deep learning framework for multivariate anomaly detection that addresses limitations of existing Fault Detection and Classification (FDC) systems. The proposed approach leverages a Transformer-based model enhanced with Aggregated z-normalization to mitigate distribution drift, and employs Peaks-Over-Threshold (POT) for adaptive thresholding. The framework achieved an F1 score of 0.9827 and a precision of 0.9866 on semiconductor datasets, with minimal false alarms validated through extensive ablation studies. The solution is designed for scalability and adaptability in industrial settings, with future work focused on improving detection of single-spike anomalies and borderline cases to enhance operational reliability.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/SPIE_jw.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Self-Disclosure of Mental Health via Deepfakes: Testing the Effects of Self-Deepfakes on Affective Resistance and Intentions to Seek Mental Health Support</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jiyoung Lee,
                            
                          
                            
                              Christopher Michael Dobmeier,
                            
                          
                            
                              Minji Heo,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      International Conference on Computational Social Science
                      (IC2S2)
                      , 2025
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Theoretically, the current study revisits traditional video self-modeling approaches, which leverage the self-referencing effect to enhance engagement, within the context of deepfake technology by integrating the self-referencing effect and the uncanny valley effect. This study reveals that the synthesized nature of self-representations in deepfakes introduces artificiality that triggers discomfort, thereby increasing resistance to mental health self-disclosure messages. This discomfort underscores a significant limitation of deepfake technology in sensitive contexts, as individuals—especially those with higher baseline levels of mental health who find greater relevance to the topic—may be reluctant to engage with messages that present uncanny or distorted self-representations. Our findings emphasize the importance of future research to systematically investigate the boundaries of self-referencing in AI-driven synthetic media, focusing on how the degree of resemblance influences perceptions of personal relevance, evokes emotional resistance, and varies across individual differences. Furthermore, as deepfake technology finds its way into the healthcare sector, practitioners must remain mindful that while it offers innovative possibilities, it may also stir emotional resistance.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/self_disclosure.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>SoK: Systematization and Benchmarking of Deepfake Detectors in a Unified Framework</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Binh M. Le,
                            
                          
                            
                              Jiwon Kim,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Kristen Moore,
                            
                          
                            
                              Alsharif Abuadbba,
                            
                          
                            
                              and Shahroz Tariq
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/eurosp63326.2025.00055" target="_blank">
                      IEEE European Symposium on Security and Privacy
                      (EuroS&amp;P)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This paper extensively reviews and analyzes state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria categorize detectors into 4 high-level groups and 13 fine-grained sub-groups, aligned with a unified conceptual framework we propose. This classification offers practical insights into the factors affecting detector efficacy. We evaluate the generalizability of 16 leading detectors across comprehensive attack scenarios, including black-box, white-box, and gray-box settings. Our systematized analysis and experiments provide a deeper understanding of deepfake detectors and their generalizability, paving the way for future research and the development of more proactive defenses against deepfakes.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/2025_EuroS&amp;P.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Towards Safe Synthetic Image Generation On the Web: A Multimodal Robust NSFW Defense and Million Scale Dataset</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Muhammad Shahid Muneer and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3701716.3715526" target="_blank">
                      ACM Web Conference
                      (WWW)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Short Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Defensive mechanisms such as NSFW and post-hoc security filters are implemented in T2I models to mitigate the misuse of T2I models and develop a safe online ecosystem for web users. However, recent work unveiled how these methods can easily fail to prevent misuse. In particular, careful adversarial attacks on text and image modalities can easily outplay defensive measures. Moreover, there is no robust millionscale multimodal NSFW dataset with both prompt and image pairs with adversarial examples. In this work, we propose a large-scale prompt and image dataset, generated using open-source diffusion models. Also, we develop a multimodal classification model to distinguish safe and NSFW text and images, which has robustness against adversarial attacks, and directly alleviates the current challenges. Our extensive experimental results show that our model shows good performance against existing SOTA NSFW detection methods in terms of accuracy and recall, and drastically reduced the Attack Success Rate (ASR) in multimodal adversarial attack scenarios.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/WWW2025_shahid.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Fairness and Robustness in Machine Unlearning</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Khoa Tran and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3701716.3715598" target="_blank">
                      ACM Web Conference
                      (WWW)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Short Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Our study presents fairness Conjectures for a well-trained model, based on the variance-bias trade-off characteristic, and considers their relevance to robustness. Our Conjectures are supported by experiments conducted on the two most widely used model architectures—ResNet and ViT—demonstrating the correlation between fairness and robustness: the higher fairness-gap is, the more the model is sensitive and vulnerable. In addition, our experiments demonstrate the vulnerability of current state-of-the-art approximated unlearning algorithms to adversarial attacks, where their unlearned models suffer a significant drop in accuracy compared  to the exact-unlearned models.We claim that our fairness-gap measurement and robustness metric should be used to evaluate the unlearning algorithm. Furthermore, we demonstrate that unlearning in the intermediate and last layers is sufficient and cost-effective for time and memory complexity.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/WWW2025_khoa.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Saliency-Aware Diffusion Reconstruction for Effective Invisible Watermark Removal</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Inzamamul Alam,
                            
                          
                            
                              Md Tanvir Islam,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3701716.3715519" target="_blank">
                      ACM Web Conference
                      (WWW)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Short Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This paper introduces a novel Saliency-Aware Diffusion Reconstruction (SADRE) framework for watermark elimination on the web, combining adaptive noise injection, region-specific perturbations, and advanced diffusion-based reconstruction. SADRE disrupts embedded watermarks by injecting targeted noise into latent representations guided by saliency masks although preserving essential image features. A reverse diffusion process ensures high-fidelity image restoration, leveraging adaptive noise levels determined by watermark strength. Our framework is theoretically grounded with stability guarantees and achieves robust watermark removal across diverse scenarios. Empirical evaluations on state-of-the-art (SOTA) watermarking techniques demonstrate SADRE’s superiority in balancing watermark disruption and image quality, achieving the best performance in PSNR, SSIM, Wasserstein Distance, and Bit Recovery Accuracy. By bridging the gap between theoretical robustness and practical effectiveness, SADRE sets a new benchmark for watermark elimination, offering a flexible and reliable solution for real-world web contents.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/WWW2025_inzi.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>GAN or DM? In-depth Analysis and Evaluation of AI-generated Face Data for Generalizable Deepfake Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Hyeongjun Choi and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3672608.3707733" target="_blank">
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we train popular deep neural networks using face data generated by various generative models and thoroughly analyze their generalizability. Our results reveal significant differences in model performance based on the forgery method used to generate the training data. Notably, we identify specific scenarios that significantly enhance model generalization, contradicting previous research finding that models trained on DM-generated data would achieve higher generalization performance than those trained on GAN-generated data. These findings emphasize the crucial role of training data selection in enhancing the generalization capabilities of deepfake detectors. By strategically selecting and combining datasets, we can develop more robust detection systems, laying a foundation for future research in creating reliable and universal deepfake detection methods</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/SAC_hyeongjun.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>X3A: Efficient Multimodal Deepfake Detection with Score-Level Fusion</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Chan Park,
                            
                          
                            
                              Bohyun Moon,
                            
                          
                            
                              Minsun Jeon,
                            
                          
                            
                              Jee-weon Jung,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3672608.3707934" target="_blank">
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we propose X3A, an efficient multimodal video deepfake detection model exploiting two powerful unimodal models with probabilistic score-level fusion. X3A leverages the advantage of using raw visual and audio inputs without relying on hand-crafted features. We conducted the extensive experiments on multiple different multimodal deepfake benchmark datasets and achieved superior performance on multimodal deepfake detection, successively detecting entirely and partially manipulated scenarios. Our X3A model demonstrates an accuracy of 0.9960 AUC of 0.9999 on the most challenging AVDeepfake1M benchmark, surpassing all existing models.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/Park_SCD_2025.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>High-Fidelity Face Age Transformation via Hierarchical Encoding and Contrastive Learning</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hakjun Moon,
                            
                          
                            
                              Dayeon Woo,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3672608.3707795" target="_blank">
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We introduce a novel GAN-based face age transformation framework utilizing Hierarchical Encoding and Contrastive Learning (HECL). Specifically, we incorporate a multi-level encoder that extracts and analyzes age-related features at different levels of detail, such as facial texture, structure, and skin tone.
 We also combined a contrastive learning approach in the discriminator to finetune the differentiation between age groups. These modifications enhance identity preservation and provide better control over aging through strategic loss functions, addressing shortcomings in existing models, which often struggle with modifying subtle face and hair texture, color, or volume during age progression. HECL outperforms SOTA models in realism and versatility, generating high-quality face images. We demonstrate superior identity preservation performance in metrics, also receiving better qualitative approval from human evaluators.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/Moon_SCD_2025.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Development of Deep Learning-Based Algorithm for Extracting Abnormal Deceleration Patterns</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Youngho Jun,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              Kangjun Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.3390/wevj16010037" target="_blank">
                      World Electric Vehicle Journal
                      (WEVJ)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =2.6</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The smart regenerative braking system for EV can reduce unnecessary brake operation by assisting in braking of the vehicle according to the driving situation, road slope, and driver’s preference. This system maintains the distance between the ego and front vehicles without controlling the brake pedal. Since the strength of regenerative braking is generally determined based on calibration data determined during the vehicle development process, some driver could suffer inconvenience when the regenerative braking is activated differently from their driving habits. In order to solve this problem, various deep learning-based algorithms are developed to provide driving stability by learning the driving data. Among those artificial intelligence algorithms, anomaly detection algorithms can successfully separate the deceleration data in abnormal driving situations, and the resulting refined deceleration data can be used to train the regression model to achieve better driving stability. This study evaluates the performance of a personalized driving assistance system by applying driver characteristic data, obtained through an anomaly detection algorithm, to vehicle control.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/Jun_WEVJ_2025.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>MIRACLE: Malware image recognition and classification by layered extraction</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Inzamamul Alam,
                            
                          
                            
                              Md. Samiullah,
                            
                          
                            
                              S M Asaduzzaman,
                            
                          
                            
                              Upama Kabir,
                            
                          
                            
                              A. M. Aahad,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/s10618-024-01078-z" target="_blank">
                      Data Mining and Knowledge Discovery
                      (DMKD)
                      , 2025
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =5.3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We propose a novel approach, Malware Image Recognition &amp; Classification by Layered Extraction (MIRACLE), by implementing our own spatial convolutional neural network (Sp-CNN) with sufficient regularization and data augmentation to identify and classify malware in images effectively and efficiently. Our proposed method is developed based on analyzing malware binary structure, which is segmented as headers and section, symbolic information lies on section segment. Our Sp-CNN can extract that symbolic information from the top of the hidden layer constructively. We have evaluated our model with as MalImg, Microfsoft-Big, Malevis and Android Malware dataset. We achieved accuracy of 99.87% for MalImg, 99.81% for Microsoft-Big, and 99.22% for Malevis in our test dataset, respectively. Our proposed method surpasses Google's InceptionV3, ResNet50, EfficientNetB1, VGG16, VGG19, and other state-of-the-art (SOTA) methods in terms of performance.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/DMKD_Figure_1.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
    </div>
  
    
    <div class="year-section" id="year-2024">
      <h4 style="margin-top:40px"><b>2024</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Synthetic Data Generation Research Trends</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Minsun Jeon and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Information Security and Cryptography-Winter
                      (CISC-W)
                      , 2024
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>With the growing need to simultaneously address privacy protection and data utilization, synthetic data, a powerful anonymization technique, is gaining attention. This paper examines the types of synthetic data, key generation methods for different target subjects, and various application cases. Through this exploration, we aim to provide a more detailed understanding of synthetic data's advantages and potential applications, as well as insights into future research directions for expanding its use.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/minsun_saftey_2.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Prioritizing Safety: A Two-Stage Not Safe For Work and Deepfake Detection Framework</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Minsun Jeon and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Korean Artificial Intelligence Association
                      (KAIA)
                      , 2024
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Deepfake content is being created automatically in large quantities, but it must still be reported manually by victims, making rapid responses difficult. Despite the prevalence of sexually exploitative deepfake, no existing approach has combined Not Safe For Work (NSFW) detection with deepfake detection. To address this issue, this study proposes a novel integrated process that first implements NSFW detection to assess urgency and identify sexual components before proceeding to deepfake detection. To verify the effectiveness of this process, we generated eight FaceSwap images. In addition, we utilized these images to evaluate the performance of the NSFW and deepfake detection models, achieving an accuracy of 87.5% and 100%, respectively. The results demonstrated the viability of a sequential detection approach. This research highlights the importance of combining NSFW and deepfake detection for more efficient and urgent content moderation, providing a practical tool for law enforcement and victim support organizations. In our findings, this research presents a paradigm that enables rapid responses to address the harms caused by deepfake content effectively and promotes a more proactive approach to content moderation.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/minsun_Safety.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>An Empirical Study of Black-Box Based Membership Inference Attacks on a Real-World Dataset</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Yujeong Kwon,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Hyungjoon Koo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-031-87496-3_9" target="_blank">
                      International Symposium on Foundations and Practice of Security
                      (FPS)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The recent advancements in artificial intelligence drive the widespread adoption of Machine-Learning-as-a-Service platforms, which offer valuable services. However, these pervasive utilities in the cloud environment unavoidably encounter security and privacy issues. In particular, a membership inference attack (MIA) poses a threat by recognizing the presence of a data sample in a training set for the target model. Although prior MIA approaches underline privacy risks repeatedly by demonstrating experimental results with standard benchmark datasets such as MNIST and CIFAR, the effectiveness of such techniques on a real-world dataset remains questionable. We are the first to perform an in-depth empirical study on black-box-based MIAs that hold realistic assumptions, including six metric-based and three classifier-based MIAs with the high-dimensional image dataset that consists of identification (ID) cards and driving licenses. Additionally, we introduce the Siamese-based MIA that shows similar or better performance than the state-of-the-art approaches and suggest training a shadow model with autoencoder-based reconstructed images. Our major findings show that the performance of MIA techniques against too many features may be degraded; the MIA configuration or a sample's properties can impact the accuracy of membership inference on members and non-members.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/membership_attack.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>LoLI-Street: Benchmarking Low-Light Image Enhancement and Beyond</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Md Tanvir Islam,
                            
                          
                            
                              Inzamamul Alam,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Saeed Anwar,
                            
                          
                            
                              Ik Hyun Lee,
                            
                          
                            
                              and Khan Muhammad
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-981-96-0917-8_20" target="_blank">
                      Asian Conference on Computer Vision
                      (ACCV)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We introduce a new large-scale dataset “LoLI-Street” (Low-Light Images of Streets) with 33k paired low-light and well-exposed images from street scenes in developed cities, covering 19k object classes for object detection, including Person, Bicycle, Car, Bus, Motorcycle, and Traffic Light, etc. LoLI-Street dataset also features 1,000 real low-light test images, providing a benchmark for evaluating models under real-world conditions. Furthermore, we propose a transformer and diffusion-based LLIE model named “TriFuse”. Leveraging the LoLI-Street dataset, we train and evaluate our TriFuse and other SOTA models to benchmark our dataset. Comparing various models, the feasibility of our dataset for generalization is evident in testing across different mainstream datasets by significantly enhancing low-quality images and object detection for practical applications in autonomous driving and surveillance systems. The benchmark dataset and the evaluation code will be released to ensure reproducibility.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ACCV-2024-2.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Bridging Optimal Transport and Jacobian Regularization by Optimal Trajectory for Enhanced Adversarial Defense</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Binh M. Le,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-981-96-0963-5_7" target="_blank">
                      Asian Conference on Computer Vision
                      (ACCV)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Deep neural networks, particularly in vision tasks, are notably susceptible to adversarial perturbations. To overcome this chal lenge, developing a robust classifier is crucial. In light of the recent advancements in the robustness of classifiers, we delve deep into the intricacies of adversarial training and Jacobian regularization, two pivotal defenses. Our work is the first carefully analyzes and characterizes these two schools of approaches, both theoretically and empirically, to demonstrate how each approach impacts the robust learning of a classifier. Next, we propose our novel Optimal Transport with Jacobian regularization  method, dubbed OTJR, bridging the input Jacobian regularization with the a output representation alignment by leveraging the optimal transport theory. In particular, we employ the Sliced Wasserstein distance that can efficiently push the adversarial samples’ representations closer to those of clean samples, regardless of the number of classes within the dataset. The SW distance provides the adversarial samples’ movement directions, which are much more informative and powerful for the Jacobian regularization. Our empirical evaluations set a new standard in the domain, with our method achieving commendable accuracies of 52.57% on CIFAR-10 and 28.36% on CIFAR-100 datasets under the AutoAttack. Further validating our model’s practicality, we conducted real-world tests by subjecting internet-sourced images to online adversarial attacks. These demonstrations highlight our model’s capability to counteract sophisticated adversarial perturbations, affirming its significance and applicability in real-world scenarios.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ACCV-2024-1.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Adaptive Clustering and Step-Size Optimization in Collaborative Distributed Diffusion-Based AIGC: Balancing Performance and Resource Utilization</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Zeliang Xu,
                            
                          
                            
                              Dong In Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/ictc62082.2024.10826855" target="_blank">
                      International Conference on Information and Communication Technology Convergence
                      (ICTC)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This paper proposes a novel cloud-edge collaborative distributed diffusion model for AI-generated content (AIGC) such as image generation, which integrates adaptive clustering techniques with dynamic step-size optimization. The proposed model addresses the challenges of heterogeneous edge devices in real-world deployments. Experimental results demonstrate significant improvements in performance and efficiency with
a 38.8% reduction in average generation time and a 15.6% increase in image quality (evaluate via CLIP score). The system shows enhanced resource utilization, improving cloud and edge utilization by 16.1% and 36.6%, respectively. This research contributes to the advancement of collaborative distributed diffusion model, offering a scalable and adaptive framework for efficient
AIGC services in dynamic environments along with potential applications extending to other computationally intensive tasks in cloud-edge systems.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ICTC 2024.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>A real-world pharmacovigilance study on cardiovascular adverse events of tisagenlecleucel using machine learning approach</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Juhong Jung,
                            
                          
                            
                              Ju Hwan Kim,
                            
                          
                            
                              Ji-Hwan Bae,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Hyesung Lee,
                            
                          
                            
                              and Ju-Young Shin
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1038/s41598-024-64466-x" target="_blank">
                      Scientific Reports
                      
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF=3.9</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this study, gradient boosting machine algorithm-based model was fitted to identify safety signals of serious cardiovascular AEs reported for tisagenlecleucel in the World Health Organization Vigibase up until February 2024. Input dataset, comprised of positive and negative controls of tisagenlecleucel based on its labeling information and literature search, was used to train the model. Then, we implemented the model to calculate the predicted probability of serious cardiovascular AEs defined by preferred terms included in the important medical event list from European Medicine Agency. There were 467 distinct AEs from 3,280 safety cases reports for tisagenlecleucel, of which 363 (77.7%) were classified as positive controls, 66 (14.2%) as negative controls, and 37 (7.9%) as unknown AEs. The prediction model had area under the receiver operating characteristic curve of 0.76 in the test dataset application. Of the unknown AEs, six cardiovascular AEs were predicted as the safety signals: bradycardia (predicted probability 0.99), pleural effusion (0.98), pulseless electrical activity (0.89), cardiotoxicity (0.83), cardio-respiratory arrest (0.69), and acute myocardial infarction (0.58). Our findings underscore vigilant monitoring of acute cardiotoxicities with tisagenlecleucel therapy.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/nature-2024.webp" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Satellite State Prediction and Maneuver Detection Analysis Using NCDEs</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Kangjun Lee and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-031-78189-6_15" target="_blank">
                      International Conference on Pattern Recognition
                      (ICPR)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Satellite orbit propagation (SOP) are of prime importance in the prevention of collision and completion of the assigned task of the satellites. In the past, orbit prediction and propagation have relied on physics-based mathematical model. However, as the number of satellites and their data increases, it is crucial to explore the data-driven orbit propagation based on the advanced machine learning methods. In this work, we propose a novel deep learning-based framework to forecast future satellite orbit states. The proposed framework employs a model based on Neural Controlled Differential Equations (NCDEs) to train orbit prediction models, and our approach captures features from past satellite state values at both fixed and dynamic time intervals. The experimental results on Korea Aerospace Research Institute (KARI)’s KOMPSAT-3 and 5 datasets demonstrate that the proposed framework outperforms the other eight data-driven baseline forecasting models.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/kari.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>SSMT: Few-Shot Traffic Forecasting with Single Source Meta-transfer Learning</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Kishor Kumar Bhaumik,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              Fahim Faisal Niloy,
                            
                          
                            
                              Amin Ahsan Ali,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-031-78195-7_4" target="_blank">
                      International Conference on Pattern Recognition
                      (ICPR)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Traffic forecasting in Intelligent Transportation Systems (ITS) is vital for intelligent traffic prediction. Yet, ITS often relies on data from traffic sensors or vehicle devices, where certain cities might not have all those smart devices or enabling infrastructures. Also, recent studies have employed meta-learning to generalize spatial-temporal traffic networks, utilizing data from multiple cities for effective traffic forecasting for data-scarce target cities. However, collecting data from multiple cities can be costly and time-consuming. To tackle this challenge, we introduce Single Source Meta-Transfer Learning (SSMT ) which relies only on a single source city for traffic prediction. Our method harnesses this transferred knowledge to enable few-shot traffic forecasting, particularly when the target city possesses limited data. Specifically, we use memory-augmented attention to store the heterogeneous spatial knowledge from the source city and selectively recall them for the data-scarce target city. We extend the idea of sinusoidal positional encoding to establish meta-learning tasks by leveraging diverse temporal traffic patterns from the source city. Moreover, to capture a more generalized representation of the positions we introduced a meta-positional encoding that learns the most optimal representation of the temporal pattern across all the tasks. We experiment on five real-world benchmark datasets to demonstrate that our method outperforms several existing methods in time series traffic prediction.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ssmt.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>MIXAD: Memory-Induced Explainable Time Series Anomaly Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Minha Kim,
                            
                          
                            
                              Kishor Kumar Bhaumik,
                            
                          
                            
                              Amin Ahsan Ali,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-031-78189-6_16" target="_blank">
                      International Conference on Pattern Recognition
                      (ICPR)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>For modern industrial applications, accurately detecting and diagnosing anomalies in multivariate time series data is essential. Despite this need, most state-of-the-art methods often prioritize detection performance over model interpretability. Addressing this gap, we introduce MIXAD (Memory-Induced Explainable Time Series Anomaly Detection), a model designed for interpretable anomaly detection. MIXAD leverages a memory network alongside spatiotemporal processing units to understand the intricate dynamics and topological structures inherent in sensor relationships. We also introduce a novel anomaly scoring method that detects significant shifts in memory activation patterns during anomalies. Our approach not only ensures decent detection performance but also outperforms state-of-the-art baselines by 34.30% and 34.51% in interpretability metrics.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/mixad.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>UGAD: Universal Generative AI Detector utilizing Frequency Fingerprints</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Inzamamul Alam,
                            
                          
                            
                              Muhammad Shahid Muneer,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3627673.3680085" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In the wake of a fabricated explosion image at the Pentagon, an ability to discern real images from fake counterparts has never been more critical. Our study introduces a novel multi-modal approach to detect AI-generated images amidst the proliferation of new generation methods such as Diffusion models. Our method, UGAD, encompasses three key detection steps: First, we transform the RGB images into YCbCr channels and apply an Integral Radial Operation to emphasize salient radial features. Secondly, the Spatial Fourier Extraction operation is used for a spatial shift, utilizing a pre-trained deep learning network for optimal feature extraction. Finally, the deep neural network classification stage processes the data through dense layers using softmax for classification. Our approach significantly enhances the accuracy of differentiating between real and AI-generated images, as evidenced by a 12.64% increase in accuracy and 28.43% increase in AUC compared to ex- isting state-of-the-art methods. Also, we integrated and deployed
1 our approach to detect real-world deepfakes in our system.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cikm-inzi-2024.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Blind-Match: Efficient Homomorphic Encryption-Based 1:N Matching for Privacy-Preserving Biometric Identification</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hyunmin Choi,
                            
                          
                            
                              Jiwon Kim,
                            
                          
                            
                              Chiyoung Song,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Hyoungshick Kim
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3627673.3680017" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We present Blind-Match, a novel biometric identification system that leverages homomorphic encryption (HE) for efficient and privacy- preserving 1:N matching. Blind-Match introduces a HE-optimized cosine similarity computation method, where the key idea is to divide the feature vector into smaller parts for processing rather than comput- ing the entire vector at once. By optimizing the number of these parts, Blind-Match minimizes execution time while ensuring data privacy through HE. Blind-Match achieves superior performance compared to state-of-the-art methods across various biometric datasets. On the LFW face dataset, Blind-Match attains a 99.63% Rank-1 ac- curacy with a 128-dimensional feature vector, demonstrating its robustness in face recognition tasks. For fingerprint identification, Blind-Match achieves a remarkable 99.55% Rank-1 accuracy on the PolyU dataset, even with a compact 16-dimensional feature vector, significantly outperforming the state-of-the-art method, Blind-Touch, which achieves only 59.17%. Furthermore, Blind-Match showcases practical efficiency in large-scale biometric identification scenarios, such as Naver Cloud’s FaceSign, by processing 6,144 biometric samples in 0.74 seconds using a 128-dimensional feature vector.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/CIKM2024-choi.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Deep Journey Hierarchical Attention Networks for Conversion Predictions in Digital Marketing</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Girim Ban,
                            
                          
                            
                              Hyeonseok Yun,
                            
                          
                            
                              Banseok Lee,
                            
                          
                            
                              David Sung,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3627673.3680066" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In digital marketing, precise audience targeting is crucial for campaign efficiency. However, digital marketing agencies often struggle with incomplete user profiles and interaction details from Advertising Identifier (ADID) data in user behavior modeling. To address this, Korea Telecom (KT), a leading telecommunication and big data service provider in South Korea, introduces the Deep Journey Hierarchical Attention Networks (DJHAN). This novel method enhances conversion predictions by leveraging heterogeneous action sequences associated with ADIDs and encapsulating these interactions into structured journeys. These journeys are hierarchically aggregated to effectively represent ADID’s behavioral attributes. Moreover, DJHAN incorporates three specialized attention mechanisms: temporal attention for time-sensitive contexts, action attention for emphasizing key behaviors, and journey attention for highlighting influential journeys in the purchase conversion process. Emprically, DJHAN surpasses state-of-the-art (SOTA) models across three diverse datasets, including real-world data from NasMedia, a leading media representative in Asia. In backtesting simulations with three advertisers, DJHAN outperforms existing baselines, achieving the highest improvements in Conversion Rate (CVR) and Return on Ad Spend (ROAS) across three advertisers, demonstrating its practical potential in digital marketing.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/CIKM2024-ban.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Preserving Old Memories in Vivid Detail: Human-Interactive Photo Restoration Framework</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Seung-Yeon Back,
                            
                          
                            
                              Geonho Son,
                            
                          
                            
                              Dahye Jeong,
                            
                          
                            
                              Eunil Park,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3627673.3679215" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Photo restoration technology enables preserving visual memories in photographs. However, physical prints are vulnerable to various forms of deterioration, ranging from physical damage to loss of image quality, etc. While restoration by human experts can improve the quality of outcomes, it often comes at a high price in terms of cost and time for restoration. In this work, we present the AI- based photo restoration framework composed of multiple stages, where each stage tailored to enhance and restore specific types of photo damage, accelerating and automating the photo restoration process. By integrating these techniques into a unified architecture, our framework aims to offer a one-stop solution for restoring old and deteriorated photographs. Furthermore, we present a novel old photo restoration dataset due to the lack of publicly available dataset for our evaulation.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/CIKM2024-Back.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Continuous Memory Representation for Anomaly Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Joo Chan Lee,
                            
                          
                            
                              Taejune Kim,
                            
                          
                            
                              Eunbyung Park,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Jong Hwan Ko
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-031-72983-6_25" target="_blank">
                      European Conference on Computer Vision
                      (ECCV)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=2</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>There have been significant advancements in anomaly detection in an unsupervised manner, where only normal images are available for training. Several recent methods aim to detect anomalies based on a memory, comparing or reconstructing the input with directly stored normal features (or trained features with normal images). However, such memory-based approaches operate on a discrete feature space implemented by the nearest neighbor or attention mechanism, suffering from poor generalization or an identity shortcut issue outputting the same as input, respectively. Furthermore, the majority of existing methods are designed to detect single-class anomalies, resulting in unsatisfactory performance when presented with multiple classes of objects. To tackle all of the above challenges, we propose CRAD, a novel anomaly detection method for representing normal features within a “continuous” memory,enabled by transforming spatial features into coordinates and mapping them to continuous grids. Furthermore, we carefully design the grids tailored for anomaly detection, representing both local and global normal features and fusing them effectively. Our extensive experiments demonstrate that CRAD successfully generalizes the normal features and mitigates the identity shortcut, furthermore, CRAD effectively handles diverse classes in a single model thanks to the high-granularity continuous representation. In an evaluation using the MVTec AD dataset, CRAD significantly outperforms the previous state-of-the-art method by reducing 65.0% of the error for multi-class unified anomaly detection.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/eccv.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Patch-wise vector quantization for unsupervised medical anomaly detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Taejune Kim,
                            
                          
                            
                              Yun-Gyoo Lee,
                            
                          
                            
                              Inho Jeong,
                            
                          
                            
                              Soo-Youn Ham,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.patrec.2024.06.028" target="_blank">
                      Pattern Recognition Letters
                      
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =5.1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Radiography images inherently possess globally consistent structures while exhibiting significant diversity in local anatomical regions, making it challenging to model their normal features through unsupervised anomaly detection. Since unsupervised anomaly detection methods localize anomalies by utilizing discrepancies between learned normal features and input abnormal features, previous studies introduce a memory structure to capture the normal features of radiography images. However, these approaches store extremely localized image segments in their memory, causing the model to represent both normal and pathological features with the stored components. This poses a significant challenge in unsupervised anomaly detection by reducing the disparity between learned features and abnormal features. Furthermore, with the diverse settings in radiography imaging, the above issue is exacerbated: more diversity in the normal images results in stronger representation of pathological features. To resolve the issues above, we propose a novel pathology detection method called Patch-wise Vector Quantization (P-VQ). Unlike the previous methods, P-VQ learns vector-quantized representations of normal "patches" while preserving its spatial information by incorporating vector similarity metric. Furthermore, we introduce a novel method for selecting features in the memory to further enhance the robustness against diverse imaging settings. P-VQ even mitigates the "index collapse" problem of vector quantization by proposing top-k% dropout. Our extensive experiments on the BMAD benchmark demonstrate the superior performance of P-VQ against existing state-of-the-art methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/prletters.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Exploring the Impact of Moiré Pattern on Deepfake Detectors</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Razaib Tariq,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/icip51287.2024.10647902" target="_blank">
                      International Conference on Image Processing
                      (ICIP)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Deepfake detection is critical in mitigating the societal threats posed by manipulated videos. While various algorithms have been developed for this purpose, challenges arise when detectors operate externally, such as on smartphones, when users take a photo of deepfake images and upload on the Internet. One significant challenge in such scenarios is the presence of Moiré patterns, which degrade image quality and confound conventional classification algorithms, including deep neural networks (DNNs). The impact of Moiré patterns remains largely unexplored for deepfake detectors. In this study, we investigate how camera-captured deepfake videos from digital screens affect detector performance. We conducted experiments using two prominent datasets, CelebDF and FF++, comparing the performance of four state-of-the-art detectors on camera-captured deepfake videos with introduced Moiré patterns. Our findings reveal a significant decline in detector accuracy, with none achieving above 68% on average. This underscores the critical need to address Moiré pattern challenges in real-world deepfake detection scenarios.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ICIP_workshop.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Decomposed Attention Segment Recurrent Neural Network for Orbit Prediction</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              SeungWon Jeong,
                            
                          
                            
                              Soyeon Woo,
                            
                          
                            
                              Daewon Chung,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Youjin Shin
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3637528.3671546" target="_blank">
                      ACM SIGKDD Conference on Knowledge Discovery and Data Mining
                      (KDD)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>As the focus of space exploration shifts from national agencies to private companies, the interest in space industry has been steadily increasing. With the increasing number of satellites, the risk of collisions between satellites and space debris has escalated, potentially leading to significant property and human losses. Therefore,
accurately modeling the orbit is critical for satellite operations. In this work, we propose the Decomposed Attention Segment Recurrent Neural Network (DASR) model, adding two key components, Multi-Head Attention and Tensor Train Decomposition, to SegRNN for orbit prediction. The DASR model applies Multi-Head Attention before segmenting at input data and before the input of the GRU layers. In addition, Tensor Train (TT) Decomposition is applied to the weight matrices of the Multi-Head Attention in both the encoder and decoder. For evaluation, we use three real-world satellite datasets from the Korea Aerospace Research Institute (KARI),
which are currently operating: KOMPSAT-3, KOMPSAT-3A, and KOMPSAT-5 satellites. Our proposed model demonstrates superior performance compared to other SOTA baseline models. We demonstrate that our approach is 94.13% higher predictive performance than the second-best model in the KOMPSAT-3 dataset, 89.79% higher in the KOMPSAT-3A dataset, and 76.71% higher in the KOMPSAT-3 dataset.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/SIGKDD24.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>DynaPP: A Dynamic Resolution Model with Patch Packing for Fast Online Video Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Changrok So,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Jong Hwan Ko
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/ijcnn60899.2024.10649922" target="_blank">
                      International Joint Conference on Neural Networks
                      (IJCNN)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Online video detection becomes more challenging with higher resolution as computational costs increase proportionally with increasing resolution. To address this issue, we present a novel approach, DynaPP, which arranges object candidate regions into a compact form. DynaPP performs resource intensive whole-image inference only on sparse key frames, employing reduced resolutions for inference on other frames. Additionally, we propose transforming a 1-stage detector into a dynamic resolution model to facilitate frame inference at reduced resolutions. Here, the dynamic resolution model signifies a model capable of inferring all resolutions, distinguishing itself from typical models by not having restricted inferable resolutions. Unlike prior studies introducing new model structures for multi-resolution models, our work demonstrates that slight modifications to existing models can convert them to dynamic resolution models. DynaPP showcases substantial acceleration in video detection across four representative video datasets: AUAIR (5.5×), UAVDT (3.67×), VisDrone (2.73×), and ImageNet VID (3.69×), while maintaining a mean average precision with a small loss (≤2.2). Furthermore, we observed that our method achieves a detection acceleration of up to 8.84×, depending on the video clip.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ijcnn.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Disrupting Diffusion-based Inpainters with Semantic Digression</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Geonho Son,
                            
                          
                            
                              Juhun Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.48550/arxiv.2407.10277" target="_blank">
                      International Joint Conference on Artificial Intelligence
                      (IJCAI)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The fabrication of visual misinformation on the web and social media has increased exponentially with the advent of foundational text-to-image diffusion models. Namely, Stable Diffusion inpainters allow the synthesis of maliciously inpainted images of personal and private figures, and copyrighted contents, also known as deepfakes. To combat such generations, a disruption framework, namely Photoguard, has been proposed, where it adds adversarial noise to the context image to disrupt their inpainting synthesis. While their framework suggested a diffusion-friendly approach, the disruption is not sufficiently strong and it requires a significant amount of GPU and time to immunize the context image. In our work, we re-examine both the minimal and favorable conditions for a successful inpainting disruption, proposing DDD, a "Digression guided Diffusion Disruption" framework. First, we identify the most adversarially vulnerable diffusion timestep range with respect to the hidden space. Within this scope of noised manifold, we pose the problem as a semantic digression optimization. We maximize the distance between the inpainting instance's hidden states and a semantic-aware hidden state centroid, calibrated both by Monte Carlo sampling of hidden states and a discretely projected optimization in the token space. Effectively, our approach achieves stronger disruption and a higher success rate than Photoguard while lowering the GPU memory requirement, and speeding the optimization up to three times faster.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ijcai24_joseph.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>iFakeDetector: Real Time Integrated Web-based Deepfake Detection System</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Kangjun Lee,
                            
                          
                            
                              Inho Jung,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.24963/ijcai.2024/1016" target="_blank">
                      International Joint Conference on Artificial Intelligence
                      (IJCAI)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Deepfake detection research has been actively conducted in the past. While many deepfake detectors have been proposed, validating the practicality of such systems against real-world settings has not been explored much. Indeed, there might be gaps and disparities when they are applied in the real world. In this work, we developed a real time integrated web-based deepfake detection system, iFakeDetector, which incorporates the recent high performing deepfake detectors, and enables easy access for non-expert users to evaluate deepfake videos. Our system takes a deepfake video as input, allowing users to upload videos and select different detectors, and provides detection results on whether the uploaded video is a deepfake or not. Furthermore, we provide an analysis tool that enables the video to be analyzed on a frame-by-frame basis with the probability of each frame being manipulated. Finally, we tested and deployed iFakeDetector in a real-world scenario to verify its practicality and feasibility.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/kangjun_ijcai24.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Gradient Alignment for Cross-Domain Face Anti-Spoofing</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Binh M. Le and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/cvpr52733.2024.00026" target="_blank">
                      IEEE/CVF Conference on Computer Vision and Pattern Recognition
                      (CVPR)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Recent advancements in domain generalization (DG) for face anti-spoofing (FAS) have garnered considerable attention. Traditional methods have focused on designing learning objectives and additional modules to isolate domain-specific features while retaining domain-invariant characteristics in their representations. In this paper, we introduce GAC-FAS, a novel learning objective that encourages the model to converge towards an optimal flat minimum without necessitating additional learning modules. Unlike conventional sharpness-aware minimizers, GAC-FAS identifies ascending points for each domain and regulates the generalization gradient updates at these points to align coherently with empirical risk minimization (ERM) gradient updates. This unique approach specifically guides the model to be robust against domain shifts. We demonstrate the efficacy of GAC-FAS through rigorous testing on challenging cross-domain FAS datasets, where it establishes state-of-the-art performance.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/binh_cvpr24.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Beyond the Screen: Evaluating Deepfake Detectors under Moiré Pattern Effects</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Razaib Tariq,
                            
                          
                            
                              Minji Heo,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Shahroz Tariq
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/cvprw63382.2024.00446" target="_blank">
                      CVPR Workshop on Media Forensics
                      (CVPRW)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The detection of deepfakes is crucial for mitigating the societal impact of falsified video content. Despite the development of various algorithms for this purpose, challenges arise for detectors in real-world scenarios, especially when users capture deepfake content from screens and upload it online or when detectors operate on external devices like smartphones, requiring the capture of potential deepfakes through the camera for evaluation. A significant challenge in these scenarios is the presence of Moir ́e patterns, which degrade image quality and complicate conventional classification methods, notably deep neural networks (DNNs). However, the impact of Moir ́e patterns on the effectiveness of deepfake detection systems has not been adequately explored. This study aims to investigate how capturing deepfake videos via digital screen cameras affects the accuracy of detection mechanisms. We introduced the Moir ́e patterns by capturing the display of a monitor using a smartphone camera and conducted empirical evaluations using four widely recognized datasets: CelebDF, DFD, DFDC, and FF++. We compare the performance of twelve SOTA detectors on deepfake videos captured under the influence of Moir ́e patterns. Our findings reveal a performance decrease of up to 33.1 and 31.3 percentage points for image and video-based detectors. Therefore, highlighting the challenges posed by Moir ́e patterns and other naturally induced artifacts is critical for improving the effectiveness of real-world deepfake detection effort.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/Moire_CVPR_Workshop.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Revisiting 30 years of the Network Time Protocol</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3589335.3651998" target="_blank">
                      ACM Web Conference
                      (WWW)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Short Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Since the inception of the Internet and WWW, providing the time among multiple nodes on the Internet has been one of the most critical challenges. David Mills is the pioneer to provide time on the Internet, inventing the Network Time Protocol (NTP), and synchronizing the clocks in computer systems. Now, the NTP is predominantly used on the Internet and WWW. In this paper, we revisit the NTP, and present the overview of the NTP. And, we highlight the advanced research effort, the SpaceNTP, to synchronize the clocks among space assets, which is the fundamental medium to provide the web services in space.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/simon_theweb24.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Saliency-Aware Time Series Anomaly Detection for Space Applications</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Sangyup Lee and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-981-97-2242-6_26" target="_blank">
                      Pacific-Asia Conference on Knowledge Discovery and Data Mining
                      (PAKDD)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Our proposed method utilizes saliency detection,
similar to anomaly detection, to identify the most significant region and effectively detect abnormal data. In this work, We propose a novel
framework, Saliency-aware Anomaly Detection (SalAD), for detecting anomalies in multivariate time series data. SalAD comprises three main
components: 1) a saliency detection module to remove redundant data, 2) an unsupervised saliency-aware forecasting model, and 3) a saliencyaware
anomaly score to differentiate anomalies. We evaluate our model using the real-world Korea Aerospace Research Institute (KARI) orbital element dataset, which includes six orbital elements and unexpected disturbances from satellites, as well as conducting extensive experiments on four benchmark datasets to demonstrate its effectiveness and superiority over other baselines. The SalAD framework has been deployed on the K3A and K5 satellites.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/sam_pakdd2024.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>SEE: Spherical Embedding Expansion for Improving Deep Metric Learning</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Binh Minh Le and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-981-97-2253-2_11" target="_blank">
                      Pacific-Asia Conference on Knowledge Discovery and Data Mining
                      (PAKDD)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The primary goal of deep metric learning is to construct a comprehensive embedding space that can effectively represent samples originating from both intra- and inter-classes. Although extensive prior work has explored diverse metric functions and innovative training strategies, much of this work relies on default training data. Consequently, the potential variations inherent within this data remain largely unexplored, constraining the model's robustness to unseen images.In this context, we introduce the Spherical Embedding Expansion (dubbed SEE) method. SEE aims to uncover the latent semantic variations in training data. Especially, our method augments the embedding space with synthetic representations based on Max-Mahalanobis distribution (MMD) centers, which maximize the dispersion of these synthetic features without increasing computational costs.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/binhle_pakdd2024.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Relation-Aware Label Smoothing for Self-KD</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Jeongho Kim and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-981-97-2253-2_16" target="_blank">
                      Pacific-Asia Conference on Knowledge Discovery and Data Mining
                      (PAKDD)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Although self-knowledge distillation shows remarkable performance improvement with fewer resources than conventional teacher-student based KD approaches, existing self-KD methods still require additional time and memory for training. We propose Relation-Aware Label Smoothing for Self-Knowledge Distillation (RAS-KD) that regularizes the student model itself by utilizing the inter-class relationships between class representative vectors with a light-weight auxiliary classifier. Compared to existing self-KD methods that only consider the instance-level knowledge, we show that proposed global-level knowledge is sufficient to achieve competitive performance while being extremely efficient training cost. Also, we achieve extra performance improvement through instance-level supervision.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/jeongho_kdd2024.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>STLGRU: Spatio-Temporal Lightweight Graph GRU for Traffic Flow Prediction</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Kishor Kumar Bhaumik,
                            
                          
                            
                              Fahim Faisal Niloy,
                            
                          
                            
                              Saif Mahmud,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-981-97-2266-2_23" target="_blank">
                      Pacific-Asia Conference on Knowledge Discovery and Data Mining
                      (PAKDD)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We propose Spatio-Temporal Lightweight Graph GRU, namely STLGRU,
a novel traffic forecasting model for predicting traffic flow accurately. Specifically, our proposed STLGRU can effectively capture dynamic local and global spatial-temporal relations of traffic networks using memory-augmented attention and gating mechanism in a continuously synchronized manner. Moreover, instead of employing separate temporal and spatial components, we show that our memory module and gated unit can successfully learn the spatial-temporal dependencies, with reduced memory usage and fewer parameters. Extensive experimental results on three real-world public traffic datasets demonstrate that our method can not only achieve state-of-the-art performance but also exhibit competitive computational efficiency.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/kishor_pakdd2024.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Development of Deep Learning-based Algorithm for Extracting Abnormal Deceleration Patterns</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Youngho Jun,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              Kangjun Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-981-97-2266-2_23" target="_blank">
                      International Electric Vehicle Symposium &amp; Exhibition
                      (EVS)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The smart regenerative braking system for EV can reduce unnecessary brake operation by assisting in braking of the vehicle according to the driving situation, road slope, and driver’s preference. This system maintains the distance between the ego and front vehicles without controlling the brake pedal. Since the strength of regenerative braking is generally determined based on calibration data determined during the vehicle development process, some driver could suffer inconvenience when the regenerative braking is activated differently from their driving habits. In order to solve this problem, various deep learning-based algorithms are developed to provide driving stability by learning the driving data. Among those artificial intelligence algorithms, anomaly detection algorithms can successfully separate the deceleration data in abnormal driving situations, and the resulting refined deceleration data can be used to train the regression model to achieve better driving stability. In this study, we extensively compare and evaluate the performance of clustering and anomaly detection methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/minha_EVS37.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Source-Free Online Domain Adaptive Semantic Segmentation of Satellite Images Under Image Degradation</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Fahim Faisal Niloy,
                            
                          
                            
                              Kishor Kumar Bhaumik,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/icassp48485.2024.10447965" target="_blank">
                      IEEE International Conference on Acoustics, Speech and Signal Processing
                      (ICASSP)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this paper, we address source-free and online domain adaptation, i.e., test-time adaptation (TTA), for satellite im- ages subject to various forms of image degradation. Towards achieving this goal, we propose a novel TTA approach involv- ing two effective strategies. First, we progressively estimate the global Batch Normalization (BN) statistics of the target distribution with incoming data stream. Leveraging these statistics during inference has the ability to effectively reduce domain gap. Furthermore, we enhance prediction quality by refining the predicted masks using global class centers. Both strategies employ dynamic momentum for fast and stable convergence. Notably, our method is back-propagation-free and hence fast and lightweight, making it highly suitable for on-the-fly adaptation to new domain. Through comprehen- sive experiments across various domain adaptation scenarios, we demonstrate the robust performance of our method.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/kishor_icassp24.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>All but One: Surgical Concept Erasing with Model Preservation in Text-to-Image Diffusion Models</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              SeungHoo Hong,
                            
                          
                            
                              Juhun Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1609/aaai.v38i19.30107" target="_blank">
                      AAAI Conference on Artificial Intelligence
                      (AAAI)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Text-to-Image models such as Stable Diffusion have shown impressive image generation synthesis, thanks to the utilization of large-scale datasets. However, these datasets may contain sexually explicit, copyrighted, or undesirable content, which allows the model to directly generate them. Given that retraining these large models on individual concept deletion requests is infeasible, fine-tuning algorithms have been developed to tackle concept erasing in diffusion models. While these algorithms yield good concept erasure, they all present one of the following issues: 1) the semantics of the prompts change over time, 2) long and inefficient training exposes the model to more harm, and 3) the spatial structure distribution of each generated image is not preserved after fine-tuning. These issues severely degrade the original utility of generative models. In this work, we present a new approach that solves all of these challenges. We take inspiration from the concept of classifier guidance and propose a surgical update on the classifier guidance term while constraining the unconditional score term. Furthermore, our algorithm empowers the user to select an alternative to the erasing concept, allowing for more controllability. Our experimental results show that our algorithm not only erases the target concept effectively but also preserves the model's generation capability.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/joseph_aaai23.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Layer Attack Unlearning: Fast and Accurate Machine Unlearning via Layer Level Attack and Knowledge Distillation</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hyunjune Kim,
                            
                          
                            
                              Sangyong Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1609/aaai.v38i19.30118" target="_blank">
                      AAAI Conference on Artificial Intelligence
                      (AAAI)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we propose a fast and novel machine unlearning paradigm at the layer level called layer attack unlearning, which is highly accurate and fast compared to existing machine unlearning algorithms. We introduce the Partial-PGD algorithm to locate the samples to forget efficiently. In addition, we only use the last layer of the model inspired by the Forward-Forward algorithm for unlearning process. Lastly, we use Knowledge Distillation (KD) to reliably learn the decision boundaries from the teacher using soft label information to improve accuracy performance. We conducted extensive experiments with SOTA machine unlearning models and demonstrated the effectiveness of our approach for accuracy and end-to-end unlearning performance.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/kim_aaai23.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Blind-Touch: Homomorphic Encryption-Based Distributed Neural Network Inference for Privacy-Preserving Fingerprint Authentication</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hyunmin Choi,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Hyoungshick Kim
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1609/aaai.v38i20.30200" target="_blank">
                      AAAI Conference on Artificial Intelligence
                      (AAAI)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This paper introduces Blind-Touch, a novel machine learning-based fingerprint authentication system that leverages homomorphic encryption to address these privacy concerns. Homomorphic encryption allows for computations on encrypted data without decrypting it. Therefore, Blind-Touch can keep fingerprint data encrypted on the server while performing machine learning operations. Blind-Touch integrates three techniques to address the computational challenges of using homomorphic encryption for machine learning: (1) A distributed machine learning architecture that divides inference tasks between the client and server, thereby reducing encrypted computations on the server; (2) A data compression method that reduces client-server communication costs; and (3) A cluster architecture that improves scalability with the number of registered users. Blind-Touch achieves high accuracy on two benchmark fingerprint datasets, with a 93.6% F1-score for the PolyU dataset and a 98.2% F1-score for the SOKOTO dataset. Moreover, Blind-Touch can match a fingerprint among 5,000 in about 0.65 seconds.With its privacyfocused design, high accuracy, and efficiency, Blind-Touch is a promising alternative to conventional fingerprint authentication
for web and cloud applications.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/choi_aaai23.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Hardening Interpretable Deep Learning Systems: Investigating Adversarial Threats and Defenses</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Eldor Abdukhamidov,
                            
                          
                            
                              Mohammed Abuhamad,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Eric Chan-Tin,
                            
                          
                            
                              and Tamer Abuhmed
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/tdsc.2023.3341090" target="_blank">
                      IEEE Transactions on Dependable and Secure Computing
                      
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =6.8</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This work introduces two attacks, AdvEdge and AdvEdge+, which deceive both the target deep learning model and the coupled interpretation model. We assess the effectiveness of proposed attacks against four deep learning model architectures coupled with four interpretation models that represent different categories of interpretation models. Our experiments include the implementation of attacks using various attack frameworks. We also explore the attack resilience against three general defense mechanisms and potential countermeasures. Our analysis shows the effectiveness of our attacks in terms of deceiving the deep learning models and their interpreters, and highlights insights to improve and circumvent the attacks.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/Hardening Interpretable.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>RAAD: Reinforced Adversarial Anomaly Detector</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon S Woo,
                            
                          
                            
                              Daeyoung Yoon,
                            
                          
                            
                              Yuseung Gim,
                            
                          
                            
                              and Eunseok Park
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3605098.3635920" target="_blank">
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We propose a novel framework called Reinforced Adversarial Anomaly Detector (RAAD) based on Reinforcement Learning to mine and detect anomalies or attacks in the presence of very few attack or anomaly patterns in time-series. Our approach uses two adversarial agents, where one agent acts as an attacker and the other as a defender. The attacker agent learns a policy to disturb the defender agent by effectively sampling the defender’s worst-performing trajectories from synthetically generated states provided by the environment, while the defender agent learns a policy that can distinguish between the normal and abnormal (attack) states. Upon successful training of two adversarial policies, the defender agent can effectively evaluate whether a new observation follows the distribution of normal states. In particular, RAAD overcomes the inherent overfitting issue, which other approaches have, through adversarial training and Reinforcement Learning. Using multiple real-world anomaly and attack detection datasets, we demonstrate that RAAD outperforms the several other baseline approaches in identifying abnormal patterns.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/RAAD.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Action Attention GRU: A Data-Driven Approach for Enhancing Purchase Predictions in Digital Marketing</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Girim Ban,
                            
                          
                            
                              Simon S Woo,
                            
                          
                            
                              and David Sung
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3605098.3635958" target="_blank">
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We present a data-driven model, the Action Attention bidirectional Gated Recurrent Unit (AAGRU) to effectively learn sequences of user behaviors without explicit knowledge of the actors or targets for conversion prediction. Tailored to predict impending purchases based on ADID’s customer journey, AAGRU leverages two pivotal components: the Action Block and the Interval Block. The former adeptly captures salient actions in the journey through attention mechanisms, while the latter discerns temporal nuances, such as impulse and deliberate buying tendencies. This tailored approach enables digital marketing agencies to identify latent customers primed for purchase, thus optimizing targeted advertising and conversion strategies. Our experimental results affirm AAGRU’s superiority over extant deep learning models. Significantly, in simulations, AAGRU demonstrated impressive performance against our company’s best audience group.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/AAGRU.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Real-Time User-guided Adaptive Colorization with Vision Transformer</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Gwanghan Lee,
                            
                          
                            
                              Saebyeol Shin,
                            
                          
                            
                              Taeyoung Na,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/wacv57701.2024.00054" target="_blank">
                      2024 IEEE/CVF Winter Conference on Applications of Computer Vision
                      (WACV)
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We propose a novel efficient ViT architecture for real-time interactive colorization, AdaColViT determines which redundant image patches and layers to reduce in the ViT. Unlike existing methods, our novel pruning method alleviates performance drop and flexibly allocates computational resources of input samples, effectively achieving actual acceleration. In addition, we demonstrate through extensive experiments on ImageNet-ctest10k, Oxford 102flowers, and CUB-200 datasets that our method outperforms the baseline methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/Screen Shot 2023-11-25 at 2.52.37 PM.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>EAE-GAN: Emotion-Aware Emoji Generative Adversarial Network for Computational Modeling Diverse and Fine-Grained Human Emotions</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              SangEun Lee,
                            
                          
                            
                              Seoyun Kim,
                            
                          
                            
                              Yeonju Chu,
                            
                          
                            
                              JeongWon Choi,
                            
                          
                            
                              Eunil Park,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/tcss.2023.3329434" target="_blank">
                      IEEE Transactions on Computational Social Systems
                      
                      , 2024
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =5.0</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>With the growing ubiquity and broad usage, emojis are widely used as a universal visual language, which complements the intentions and emotions beyond the textual data. Despite the critical role of representing emotion, existing emojis neglect the subtle and complex properties of human emotion in that only countable and finite face emojis exist in a categorical manner. In this article, we propose a novel approach to facial emoji generation, which can control the emotional degree of generated emojis for more complex and detailed usage on online conversations. In other words, we develop a new emotion aware emoji generative adversarial network, which is capable of generating an emoji that expresses a given emotion distribution. In this way, our approach aims to map fine grained emotions to expressive emojis. Both quantitative and qualitative evaluation demonstrate that our approach can successfully generate high quality emoji like images by representing a wide range of emo tions. To the best of our knowledge, this is the first approach to use the deep generative model from the standpoint of the emoji’s emotional role, which can further promote more interactive and effective online communication.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/TCS.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
    </div>
  
    
    <div class="year-section" id="year-2023">
      <h4 style="margin-top:40px"><b>2023</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Extreme Environment Rotated Object Detection Network</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Giljun Lee,
                            
                          
                            
                              Junyaup Kim,
                            
                          
                            
                              Gwanghan Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.5626/jok.2023.50.11.966" target="_blank">
                      Journal of KIISE
                      
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This paper proposes E^2RDet. This algorithm effectively modifies the structure of the Yolov7 object detection model, enabling it to accurately detect objects represented by oriented bounding boxes (OBB) in SAR images. This algorithm improves the object detection model architecture and loss function to facilitate learning of an object's dynamic (orientation) posture. Using various training datasets, E^2RDet demonstrates performance improvements across three benchmark SAR datasets. This indicates that existing HBB object detection models can train and perform object detection on objects represented by OBBs.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/E^2RDet.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>KappaFace: Adaptive Additive Angular Margin Loss for Deep Face Recognition</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Chingis Oinar,
                            
                          
                            
                              Binh M. Le,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/access.2023.3338648" target="_blank">
                      IEEE Access
                      
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =3.47</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Imbalanced learning might include both classes having different learning difficulties or different numbers of available training samples. We hypothesize that it significantly affects the generalization ability of the deep face models. Inspired by this, we introduce a novel adaptive strategy, called KappaFace, to modulate the relative importance based on class learning difficulty and its imbalance. Due to the von Mises-Fisher distribution, our proposed KappaFace loss can intensify margins for difficult-to-learn or under-represent classes while relaxing that of counter classes. Experiments conducted on popular facial benchmarks demonstrate that our proposed method achieves superior performance to the state-of-the-art methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ching_TIP23.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Occupational Gender Bias in Large Language Models evaluated on multiple languages</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Seung-yeon Back,
                            
                          
                            
                              Eun-Ju Park,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                       CIKM Workshop on Large Language Models’ Interpretation and Trustworthiness
                      (LLIMT)
                      , 2023
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In our study, we turn our attention specifically to the bias issues at the intersection of gender and occupations within LLM-generated text. Our research seeks to address this concern by examining how gender bias is reflected in responses generated by LLMs, with a focus on the fields of gender and occupation. We aim to explore these biases not only in English, but also in Korean language, thereby expanding the scope of our investigation to different linguistic and cultural contexts. Through these investigations, our research aims to provide a comprehensive comparison of bias patterns across different languages and cultures. Ultimately, we seek to contribute to the ongoing dialogue surrounding ethical concerns in LLMs and offer implications for future developments in the field of natural language processing.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/seungyeon_llm.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Anomaly and Novelty detection for Satellite and Drone systems (ANSD '23)</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Daewon Chung,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Youjin Shin
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3583780.3615306" target="_blank">
                      ACM Workshop on International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In recent times, there has been a notable surge in the amount of vision and sensing/time-series data obtained from drones and satellites. This data can be utilized in various fields, such as precision agriculture, disaster management, environmental monitoring, and others. However, the analysis of such data poses significant challenges due to its complexity, heterogeneity, and scale. Furthermore, it is critical to identify anomalies and maintain/monitor the health of drones and satellite systems to enable the aforementioned applications and sciences. This workshop presents an excellent opportunity to explore solutions that specifically target the detection of anomalies and novel occurrences in drones and satellite systems and their data. The workshop is designed to promote knowledge exchange, collaboration, and innovation in Anomaly and Novelty detection for Satellite and Drone systems. Through this platform, researchers, practitioners, and industry experts are expected to come together to explore and discuss the latest developments, challenges, and opportunities in analyzing and maintaining the health of drone and satellite systems, in addition to detecting anomalies and novelties in the associated vision and time-series data. The primary objective of the workshop is to facilitate in-depth discussions on various techniques, methodologies, and applications related to anomaly and novelty detection. Participants will be encouraged to share their ideas and experiences on how best to identify new research directions and potential collaborations. Ultimately, the workshop aims to enhance the capabilities of leveraging drone and satellite systems for diverse applications such as precision agriculture, disaster management, and environmental monitoring. By the end of the workshop, participants are expected to gain valuable insights into state-of-the-art approaches and establish connections with peers. This will provide an opportunity for them to contribute to the advancement of knowledge in this domain, leading to more efficient and effective utilization of drone and satellite systems. For more information, visit our website at ANSD'23.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/Call For Papers.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>KID34K: A Dataset for Online Identity Card Fraud Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Eun-Ju Park,
                            
                          
                            
                              Seung-Yeon Back,
                            
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3583780.3615122" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>To mitigate the risks associated with fraudulent ID card verification, we present a novel dataset for classifying cases where the ID card images that users upload to the verification system are genuine or digitally represented. Our dataset is replicas designed to resemble real ID cards, making it available while avoiding privacy issues. Through extensive experiments, we demonstrate that our dataset is effective for detecting digitally represented ID card images, not only in our replica dataset but also in the dataset consisting of real ID cards.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/kid34k_cikm23.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>UNDO: Effective and Accurate Unlearning Method for Deep Neural Networks</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Sangyong Lee and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3583780.3615235" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Short Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=2</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we propose a novel two-step unlearning approach UNDO. First, we selectively disrupt the decision boundary of forgetting data at the coarse-grained level. However, this can also inadvertently affect the decision boundary of other remaining data, lowering the overall performance of classification task. Hence, we subsequently repair and refining the decision boundary for each class at the fine-grained level by introducing a loss for maintain the overall performance, while completely removing the class. We conducted extensive experiments with SOTA models over two datasets, and demonstrated the effectiveness and efficiency of our approach for unlearning, compared to other methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/undo_cikm23.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>SAFE: Sequential Attentive Face Embedding with Contrastive Learning for Deepfake Video Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Juho Jung,
                            
                          
                            
                              Chaewon Kang,
                            
                          
                            
                              Jeewoo Yoon,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Jinyoung Han
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3583780.3615279" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Short Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=2</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This paper proposes a novel sequential attentive face
embedding, SAFE, that can capture facial dynamics in a deepfake video. The proposed SAFE can effectively integrate global and local dynamics of facial features revealed in a video sequence using contrastive learning. Through a comprehensive comparison with the state-of-the-art methods on the DFDC (Deepfake Detection
Challenge) dataset and the FaceForensic++ benchmark, we show that our model achieves the highest accuracy in detecting deepfake videos on both datasets.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/safe_cikm23.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Towards Understanding of Deepfake Videos in the Wild</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Beomsang Cho,
                            
                          
                            
                              Binh M. Le,
                            
                          
                            
                              Jiwon Kim,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Alsharif Abuadbba,
                            
                          
                            
                              and Kristen Moore
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.48550/arxiv.2309.01919" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Our contributions in this IRB-approved study are to bridge this knowledge gap from current real-world deepfakes by providing in-depth analysis.We first present the largest and most diverse and recent deepfake dataset (RWDF-23) collected from the wild to date, consisting of 2,000 deepfake videos collected from 4 platforms targeting 4 different languages span created from 21 countries: Reddit, YouTube, TikTok, and Bilibili. By expanding the dataset's scope beyond the previous research, we capture a broader range of real-world deepfake content, reflecting the ever-evolving landscape of online platforms. Also, we conduct a comprehensive analysis encompassing various aspects of deepfakes, including creators, manipulation strategies, purposes, and real-world content production methods. This allows us to gain valuable insights into the nuances and characteristics of deepfakes in different contexts. Lastly, in addition to the video content, we also collect viewer comments and interactions, enabling us to explore the engagements of internet users with deepfake content.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/rwdf23_cikm23.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Quality-Agnostic Deepfake Detection with Intra-model Collaborative Learning</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Binh M. Le and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/iccv51070.2023.02045" target="_blank">
                      IEEE/CVF International Conference on Computer Vision
                      (ICCV)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we propose a universal intra-model collaborative learning framework to enable the effective and simultaneous detection of different quality of deepfakes. That is, our approach is the quality-agnostic deepfake detection method, dubbed QAD. In particular, by observing the upper bound of general error expectation, we maximize the dependency between intermediate representations of images from different quality levels via Hilbert-Schmidt Independence Criterion. In addition, an Adversarial Weight Perturbation module is carefully devised to enable the model to be more robust against image corruption while boosting the overall model’s performance. Extensive experiments over seven popular deepfake datasets demonstrate the superiority of our QAD model over prior SOTA benchmarks.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/qad_iccv23_arch.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Manipulated ID Card Classification using Deep Neural Networks</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hakjun Moon,
                            
                          
                            
                              Eunju Park,
                            
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              Kwansik Yoon,
                            
                          
                            
                              Yeonah Seo,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Information Security and Cryptography-Summer
                      (CISC-S)
                      , 2023
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>2023 한국정보보호학화 하계학술대회 딥러닝 기반 신원 인증 시스템에 대해 제시하였으며, 비대면 상황에서 주민등록증이나 운전면허증과 같은 신분증의 진위를 확인하는 문제에 집중하였다. 딥러닝과 특징 추출 기법을 이용하여 신분증 이미지가 실물인지, 혹은 디지털 방식으로 조작되었는지 판별하도록 모델을 학습하였으며, 최대 96.6%의 높은 분류 정확도를 보였다. 이런 결과는 신원 인증과 보안의 중요성이 갈수록 부각되는 현재 사회에서 중요한 의미를 가진다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/moon_hanconf23.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Selective unlearning for DNN based model</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Song-Chan Jin and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Information Security and Cryptography-Summer
                      (CISC-S)
                      , 2023
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>본 논문에서 제안하는 선택적 망각이란 딥러닝 모델이 일부 지식을 선택적으로 잊어버리는 것을 의미하며, 개인정보 보호를 위해 도입되었다. 이를 위해 데이터 재수정 및 모델 재학습 등의 방법이 있지만, 이러한 방법들은 일반적으로 계산량이 많거나 모델의 성능을 크게 저하시키는 문제가 있어서 이에 대한 대안으로 작은 데이터셋으로 다른 데이터들에 대한 지식은 유지한 채 특정 데이터들에 대한 지식만 잊는 경사 상승법을 소개하고 있다. 본 논문에서는 경사 상승법을 통하여 기존 재학습 기법 대비 9배 적은 계산량으로 선택적 망각을 수행할 수 있다는 결과를 얻었다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/songchan_hanconf23.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>HRFNet: High-Resolution Forgery Network for Localizing Satellite Image Manipulation</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Fahim Faisal Niloy,
                            
                          
                            
                              Kishor Kumar Bhaumik,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/icip49359.2023.10221974" target="_blank">
                      IEEE International Conference on Image Processing
                      (ICIP)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Existing high-resolution satellite image forgery localization methods rely on patch-based or downsampling-based training. Both of the training methods have major drawbacks, such as, inaccurate boundary between pristine and forged region, generation of unwanted artifacts, etc. To tackle aforementioned challenges, inspired from the high-resolution image segmentation literature, we propose a novel model called HRFNet to effectively enable satellite image forgery localization. Specifically, equipped with shallow and deep branches, our model can successfully integrate RGB and resampling features in both global and local manner to localize forgery more accurately. We experiment on popular satellite image manipulation dataset to demonstrate that our method achieves the best performance, while the memory requirement and processing speed are not compromised compared to existing methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/hrfnet_arch.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Expectation-Maximization via Pretext-Invariant Representations</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Chingis Oinar,
                            
                          
                            
                              Binh M. Le,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/access.2023.3289589" target="_blank">
                      IEEE Access
                      
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =3.47</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we explain and propose a novel self-supervised objective, Expectation-Maximization via Pretext-Invariant Representations (Empir), which enhances Expectation-Maximization-based optimization in BYOL-like algorithms by enforcing augmentation invariance within a local region of k nearest neighbors, resulting in consistent representation learning. In other words, we propose Expectation-Maximization as a core task of asymmetric architectures. We show that it consistently outperforms other SOTA algorithms by a decent margin. We also demonstrate its transfer learning capabilities on downstream image recognition tasks.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/empir_arch.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>IMF: Integrating Matched Features Using Attentive Logit in Knowledge Distillation</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              Hanbeen Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.24963/ijcai.2023/108" target="_blank">
                      International Joint Conference on Artificial Intelligence
                      (IJCAI)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, to address the student model's limitation, we propose a novel flexible KD framework, Integrating Matched Features using Attentive Logit in Knowledge Distillation (IMF). Our approach introduces an intermediate feature distiller (IFD) to improve the overall performance of the student model by directly distilling the teacher's knowledge into branches of student models.The generated output of IFD, which is trained by the teacher model, is effectively combined by attentive logit.We use only a few blocks of the student and the trained IFD during inference, requiring an equal or less number of parameters.Through extensive experiments, we demonstrate that IMF consistently outperforms other state-of-the-art methods with a large margin over the various datasets in different tasks without extra computation.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/imf_ijcai.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Exploiting Inconsistencies in Object Representations for Deepfake Video Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Kishor Kumar Bhaumik and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3595353.3595885" target="_blank">
                      ACM ASIACCS Workshop on Security Implications of Deepfakes and Cheapfakes
                      (WDC)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Deepfake videos are mostly generated in a frame-by-frame manner, which leaves visible object-level inconsistencies in both temporal and spatial dimensions. In this paper, we propose a novel deepfake video detection method that exploits this important clue. Specifically, we extract object representations using vision transformers from video frames and then model the object-level coherence in both intra-frame and inter-frame manner. We experiment on benchmark dataset to show that our method outperforms several existing methods in deepfake video detection.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/wdc_kishor.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Why Do Facial Deepfake Detectors Fail?</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Binh Le,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Alsharif Abuadbba,
                            
                          
                            
                              Kristen Moore,
                            
                          
                            
                              and Simon Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3595353.3595882" target="_blank">
                      ACM ASIACCS Workshop on Security Implications of Deepfakes and Cheapfakes
                      (WDC)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Recent rapid advancements in deepfake technology have allowed the creation of highly realistic fake media, such as video, image, and audio. These materials pose significant challenges to human authentication, such as impersonation, misinformation, or even a threat to national security. To keep pace with these rapid advancements, several deepfake detection algorithms have been proposed, leading to an ongoing arms race between deepfake creators and deepfake detectors. Nevertheless, these detectors are often unreliable and frequently fail to detect deepfakes. This study highlights the challenges they face in detecting deepfakes, including (1) the pre-processing pipeline of artifacts and (2) the fact that generators of new, unseen deepfake samples have not been considered when building the defense models. Our work sheds light on the need for further research and development in this field to create more robust and reliable detectors.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/face_detect_engine.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Distance adaptive graph convolutional gated network-based smart air quality monitoring and health risk prediction in sensor-devoid urban areas</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahzeb Tariq,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              SangYoun Kim,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and ChangKyoo Yoo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.scs.2023.104445" target="_blank">
                      Journal of Sustainable Cities and Society
                      
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =10.696</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Rapid urbanization and economic growth have increased air pollution, threatening human health and life expectancy, especially in developing nations. Strong air quality early warning systems for city sustainability have recently garnered attention. The present early warning frameworks in urban environments can only forecast air quality where sufficient sensor data is available. We propose a spatiotemporal sensor fusion-based distance adaptive graph convolutional gated network that predicts primary pollutants at multiple megacity locations and temporal horizons. Our remotely forecasted concentrations at a sensorless site matched city air quality distribution. The framework also solves critical problems of early warning systems related to long-term sensor failure and prediction at a new location in the city.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/tariq_civil.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>DID We Miss Anything?: Towards Privacy-Preserving Decentralized ID Architecture</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Siwon Huh,
                            
                          
                            
                              Myungkyu Shim,
                            
                          
                            
                              Jihwan Lee,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Hojoon Lee
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/tdsc.2023.3235951" target="_blank">
                      IEEE Transactions on Dependable and Secure Computing
                      
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =7.32</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Decentralized Identity (DID) is emerging as a new digital identity management scheme that promises users complete control of their personal data and identification without central authority involvement. The World Wide Web Consortium (W3C) has drafted the DID standard and provided reference implementations. We conduct a security analysis of the W3C DID standard and the reference universal resolver implementation, focusing on user privacy in the DID resolving process. The universal resolver is the key component in the architecture that processes DID requests and DID document retrievals. Our analysis demonstrates that privacy issues can arise due to the imprudent design of the universal resolver. Furthermore, we found that side-channels in the DID document caching schemes of real-world DID services can entail privacy concerns. Motivated by our security analysis, we present a novel  DID resolving design, called Oblivira, to enable obliviously DID resolving. Oblivira is a secure resolving agent with a small footprint that enforces the universal resolver to resolve requests without knowing their content. We also propose a privacy-preserving DID document caching scheme that eliminates side-channels. Our evaluation results show that Oblivira only incurs approximately 2.6\% of overhead on average with different resolver settings (3, 6, and 12 threads).</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="TDSC.PNG" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Evaluating Racial Bias in Face Recognition APIs using Deepfakes</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sowon Jeon,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      IEEE Computer Magazine
                      
                      , 2023
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =3.56</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Deep learning algorithms enable rapid growth in web-based services such as natural language processing, speech recognition, and facial recognition. Simultaneously, online fairness and trust remain unresolved. For example, racial bias in web-based face recognition services can lead to inaccurate results, causing severe technical and social issues and widespread distrust in AI-based systems. Deepfake on social media has posed several credibility issues. We evaluate the racial bias in face recognition APIs using real and deepfake celebrity images. We use deepfake generation methods to introduce small, imperceptible changes to the real images to shift the racial class of predictions. As a result, we show how deepfake images exacerbated racial bias in Amazon, Microsoft, and Naver web-based face recognition APIs. The findings are significant because they reveal similar vulnerabilities to those previously discovered through adversarial attacks but through a significantly different method.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/shahroz_ieee_computer.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Design and evaluation of highly accurate smart contract code vulnerability detection framework</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sowon Jeon,
                            
                          
                            
                              Gilhee Lee,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/s10618-023-00981-1" target="_blank">
                      Data Mining and Knowledge Discovery
                      
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =3.67</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this paper, we present SmartConDetect as a tool for detecting security vulnerabilities in Solidity smart contracts. SmartConDetect is a static analysis tool that extracts code fragments from Solidity smart contracts and uses a pre-trained BERT model to find susceptible code patterns. To demonstrate the performance of SmartConDetect, we use two public datasets, and our dataset (SmartConDataset) collected from the real-world Ethereum blockchain network. Our experimental results show that SmartConDetect significantly outperforms all state-of-the-art methods, achieving 90.9\% F1-score when using our own dataset. Specifically, SmartConDetect is about 2 times faster than SmartCheck in detection. Furthermore, we conduct a real-world case study to analyze the distribution of detected vulnerabilities.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="sowon.JPG" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>A-ColViT : Real-time Interactive Colorization by Adaptive Vision Transformer</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Gwanghan Lee,
                            
                          
                            
                              Saebyeol Shin,
                            
                          
                            
                              Donggeun Ko,
                            
                          
                            
                              Jiyeon Jung,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      AAAI Workshop on Practical Deep Learning in the Wild
                      (PDLW)
                      , 2023
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Vision transformer has been used to alleviate this problem by using multi-head self attention to propagate user hints to distant relevant areas in the image. However, despite the success of vision transformers in colorizing the image and selectively colorizing the regions with user propagation hints, heavy underlying ViT architecture and the large number of required parameters hinder active real-time user interaction for colorization applications. Thus, in this work, we propose a novel efficient ViT architecture for real-time interactive colorization, A-ColViT that adaptively prunes the layers of vision transformer for every input sample. This method flexibly allocates computational resources of input samples, effectively achieving actual acceleration. In addition, we demonstrate through extensive experiments on ImageNet-ctest10k, Oxford 102flower, and CUB-200 datasets that our method outperforms the state-of-the-art approach and achieves actual acceleration.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/gwanhan_aaai23.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>S-ViT: Sparse Vision Transformer for Accurate Face Recognition</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Geunsu Kim,
                            
                          
                            
                              Gyudo Park,
                            
                          
                            
                              Soohyeok Kang,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3555776.3577640" target="_blank">
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we propose a Sparse Vision Transformer (S-ViT) based on the Vision Transformer (ViT) architecture to improve the face recognition tasks. After the model is trained, S-ViT tends to have a sparse distribution of weights compared to ViT, so we named it according to these characteristics. Unlike the conventional ViT, our proposed S-ViT adopts image Relative Positional Encoding (iRPE) method for positional encoding. Also, S-ViT has been modified so that all token embeddings, not just class token, participate in the decoding process. Through extensive experiment, we showed that S-ViT achieves better performance in closed-set than the other baseline models, and showed better performance than the baseline ViT-based models. We also show that the use of ArcFace loss functions yields greater performance gains in S-ViT than in baseline models. In addition, S-ViT has an advantage in cost-performance trade-off because it tends to be more robust to the pruning technique than the underlying model, ViT. Therefore, S-ViT offers the additional advantage, which can be applied more flexibly in the target devices with limited resources.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/kim_sac23.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>MGCMA: Multi-scale Generator with Channel-wise Mask Attention to generate Synthetic Contrast-enhanced Chest Computed Tomography</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              Yun-Gyoo Lee,
                            
                          
                            
                              Donggeun Ko,
                            
                          
                            
                              Taejune Kim,
                            
                          
                            
                              Soo-Youn Ham,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3555776.3578618" target="_blank">
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Medical images, including computed tomography (CT) assist doctors and physicians in diagnosing anatomic structures and various internal pathologies. In CT, intravenous contrast media is often applied, which are chemicals developed to aid in the characterization of pathology by enhancing the capabilities of an imaging modality to differentiate between different biological tissues. Especially, with the use of contrast media, thorough examinations of the patients can be possible. However, contrast media can have severe adverse and side effects such as hypersensitive reaction to generalized seizures. Yet, without contrast media, it is difficult to diagnose patients that have disorders in the internal organs. With the help of DNN models, especially generative adversarial network (GAN), contrast-enhanced CT (CECT) images can be synthetically generated from non-contrast CT (NCCT) images. GANs or autoencoder-based models have been proposed to generate contrast-enhanced CT images; however, the synthesized image does not fully reflect and have crucial spots where contrast has not been synthesized. Thus, in order to enhance the quality of the CECT image, we propose MGCMA, a multi-scale generator with a channel-wise mask attention module for generating synthetic CECT images from NCCT images. Our extensive experiments demonstrate that our model outperforms other baseline models in various metrics such as SSIM and LPIPS. Also, generated images from our approach achieve plausible outcomes from the domain experts' (e.g., physicians and radiologists) evaluations.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/jho_sac23.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Rotated-DETR: an End-to-End Transformer-based Oriented Object Detector for Aerial Images</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Giljun Lee,
                            
                          
                            
                              Jinbeom Kim,
                            
                          
                            
                              Taejune Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3555776.3577745" target="_blank">
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Oriented object detection in aerial images is a challenging task due to the highly complex backgrounds and objects with arbitrary oriented and usually densely arranged. Existing oriented object detection methods adopt CNN-based methods, and they can be divided into three types: two-stage, one-stage, and anchor-free methods. All of them require non-maximum suppression (NMS) to eliminate the duplicated predictions. Recently, object detectors based on the transformer remove hand-designed components by directly solving set prediction problems via performing bipartite matching, and achieve state-of-the-art performances in general object detection. Motivated by this research, we propose a transformer-based oriented object detector named  Rotated DETR with oriented bounding boxes (OBBs) labeling. We embed the scoring network to reduce the tokens corresponding to the background. In addition, we apply a proposal generator and iterative proposal refinement in order to provide proposals with angle information to the transformer decoder. Rotated DETR achieves state-of-the-art performance on the single-stage and anchor-free oriented object detectors on DOTA, UCAS-AOD, and DIOR-R datasets with only 10\% feature tokens. In the experiment, we show the effectiveness of the scoring network and iterative proposal refinement.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/jb_sac23.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>An overhead-free region-based JPEG framework for task-driven image compression</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Seonghye Jeong,
                            
                          
                            
                              Seongmoon Jeong,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Jong Hwan Ko
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.patrec.2022.11.020" target="_blank">
                      Pattern Recognition Letters
                      
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =5.67</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>An increasing amount of captured images are streamed to a remote server or stored in a device for deep neural network (DNN) inference. In most cases, raw images are compressed with encoding algorithms such as JPEG to cope with resource limitations. However, the standard JPEG optimized for human visual systems may induce significant accuracy loss in DNN inference tasks. In addition, the standard JPEG compresses all regions in an image at the same quality level, while some areas may not contain valuable information for the target task. In this paper, we propose a target-driven JPEG compression framework that performs region-adaptive quantization of the DCT coefficients. The region-based quality map is generated from an end-to-end trainable neural network. In addition, we present a deep learning approach to remove the requirement of storing the overhead information induced by the region-based encoding process. Our framework can be easily implemented on devices with commonly used JPEG and also produce images that achieve a higher compression rate with minimum degradation of the classification accuracy.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/jeong_prl.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>CFL-Net: Image Forgery Localization Using Contrastive Learning</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Fahim Faisal Niloy,
                            
                          
                            
                              Kishor Kumar Bhaumik,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/wacv56688.2023.00462" target="_blank">
                      IEEE/CVF Winter Conference on Applications of Computer Vision
                      (WACV)
                      , 2023
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Conventional forgery localizing methods usually rely on different forgery footprints such as JPEG artifacts, edge inconsistency, camera noise, etc., with cross-entropy loss to locate manipulated regions. However, these methods have the disadvantage of over-fitting and focusing on only a few specific forgery footprints. On the other hand, real-life manipulated images are generated via a wide variety of forgery operations and thus, leave behind a wide variety of forgery footprints. Therefore, we need a more general approach for image forgery localization that can work well on a variety of forgery conditions. A key assumption in underlying forged region localization is that there remains a difference of feature distribution between untampered and manipulated regions in each forged image sample, irrespective of the forgery type. In this paper, we aim to leverage this difference of feature distribution to aid in image forgery localization. Specifically, we use contrastive loss to learn mapping into a feature space where the features between untampered and manipulated regions are well-separated for each image. Also, our method has the advantage of localizing manipulated region without requiring any prior knowledge or assumption about the forgery type. We demonstrate that our work outperforms several existing methods on three benchmark image manipulation datasets.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/wacv22_kishor.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
    </div>
  
    
    <div class="year-section" id="year-2022">
      <h4 style="margin-top:40px"><b>2022</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>A Novel Transformer-based Approach for Rotated Object Detection in Aerial Images</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jinbeom Kim,
                            
                          
                            
                              Giljun Lee,
                            
                          
                            
                              Taejune Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      추계 공동학술대회
                      
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>매우 복잡한 배경과 임의로 회전 되어있고 조밀하게 배열되어 잇는 객체로 인해 항공 이미지에서 회전된 객체를 탐지하는 것은 매우 어려운 작업이다. 기존의 회전 객체 탐지 기법들은 CNN 기반 방법론을 채택하고 있으며, 이들은 세가지 카테고리 two-stage, one-stage, 그리고 anchor-free로 분류할 수 있다. 이들 모두 중복된 예측을 제거하기 위해 비최대 억제(NMS)가 필요하다. 최근 transformer를 기반으로 한 객체 탐지 모델은 이분 매칭을 통해 set prediction proble을 직접 해결하여 수작업으로 설계된 구성 요소들을 제거하면서 일반적인 객체 탐지 분야에서 최첨단 성능을 달성하였다. 이 연구에 자극을 받아, 우리는 방향 경계 상자(OBB) 라벨을 사용하는 transformer 기반 모델인 Rotated DETR를 제안한다.또한 우리는 proposal generator와 iterative proposal refinement를 적용하여 transformer decoder에 각도 정보를 제공한다. Rotated DETR은 10%의 feature token 만으로 DOTA 데이터 세트의 one-stage와 anchor-free 모델들에서 최첨단 성능을 달성한다. 우리는 실험을 통해 scoring network와 iterative proposal refinement의 효과를 보여준다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Effective Deepfake Detection using Mask Attention</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Saebyeol Shin and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      추계 공동학술대회
                      
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>다양한 딥페이크 데이터셋에 대한 최신 딥페이크 탐지 모델은 놀라운 성능을 달성했습니다. 그러나 대부분의 접근 방식은 각 딥페이크 입력 이미지가 서로다른 지역적인 부분에서 구별되는 특징을 가지고 있다는 사실을 활용하지 않습니다. 따라서 본 논문은 입력 이미지의 서로 다른 세부적인 부분에 동적으로 초점을 맞추고 실제 이미지와 딥페이크 이미지의 미묘하고 세부적인 차이를 이용하는 효과적인 딥페이크 탐지 방법인 MaskDF를 제안합니다. 특히 중요하지 않은 특성을 제거하여 입력의 귀중한 정보를 보존할 수 있는 학습 가능한 어텐션 마스크를 제안합니다. 입력 피쳐는 제안된 게이팅 함수를 통과하여 어텐션 마스크 벡터를 생성하므로 딥페이크 탐지에 영향을 미치는 중요한 특징을 결정할 수 있습니다. 우리의 방법은 입력 정보의 절반만 사용하여 DFDC 및 FaceForensics++ 데이터 세트에서 다른 기본 모델보다 더 나은 성능을 보여주었습니다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Analysis of Obfuscation of Deepfake Images in Differential Privacy Settings</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Donggeun Ko and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      추계 공동학술대회
                      
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>소셜 미디어나 감시 시스템에서 매일 수많은 얼굴 사진과 개인 정보가 수집된다. 얼굴 정보를 포함한 소셜 미디어 사용자의 개인 정보는 간단한 거래나 공항 출입국 절차의 간소화와 같은 이점이 있지만 이러한 이점은 항상 개인 정보 보호 문제를 수반한다. 위와 같은 민감한 정보들은 잠재적으로 유해한 목적으로 사용될 위험이 있기때문에 공격자에게 취약하다고 할 수 있다. 이러한 정보를 보호하기 위해 이미지의 프라이버시를 강화하는 솔루션인 DP(Differential Privacy)를 사용하여 높은 수준의 프라이버시를 제공한다. DP(Differential Privacy)를 통해 이미지의 프라이버시가 증가할 수 있지만 이상적인 epsilon-DP를 달성하기 위해서 유틸리티와 프라이버시 사이에는 필연적인 trade-off가 있다. 따라서 난독화 이미지의 최적 매개변수를 선택하는 것이 개인정보 보호의 핵심이며 본 논문에서는 이미지의 프라이버시를 강화하기 위해 각각 DP-Pix, DP-SVD, Snow라는 3가지 DP(Differential Privacy) 난독화 방법을 제시한다. 또한 딥 러닝 모델의 견고성을 평가하는 딥페이크 이미지 데이터셋에서 DP 방법을 구현하는 다양한 방법을 시연한다. 실험의 결과는 훈련 단계에서 데이터 세트 증대가 epsilon-DP(Differential Privacy를 사용하여 딥페이크를 탐지할 때 모델의 성능을 쉽게 향상시킬 수 있음을 나타낸다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Evaluation of Deepfakes with Generated Facemasks</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Donggeun Ko and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      추계 공동학술대회
                      
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>최근들어 딥페이크(Deepfake) 기술의 발전으로 인해 국제사회의 우려가 점점 커지고 있다. 딥페이크 기술은 이미지나 영상 속 얼굴을 손쉽게 생성, 조작하여 왜곡된 정보를 전파할 수 있기 때문이다. 이에 따라 최첨단 성능을 갖춘 다양한 딥페이크 탐지 모델이 제안되어 왔다. 그러나 지금까지 제안된 딥페이크 탐지 모델은 펜데믹 위기 동안 발생했을 마스크가 착용된 얼굴에 대한 정보는 고려하지 않고 있다. 마스크가 착용된 얼굴 이미지의 경우 얼굴의 중요한 랜드마크가 마스크 속에 숨겨져 있기 때문에 딥페이크 탐지기의 성능을 보장하기 어렵다. 따라서 본 논문에서는 이러한 문제를 해결할 수 있는 두 가지 간단한 방법론을 제시하고 기존 방법론들과의 비교실험을 통해 마스크가 착용된 얼굴 이미지와 마스크가 착용되지 않은 얼굴 이미지 사이에서 나타날 수 있는 딥페이크 탐지 모델의 문제점과 제시된 방법론의 효과를 살펴보고자 한다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>RCRL: Replay-based Continual Representation Learning in Multi-task Super-Resolution</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jinyong Park,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/avss56176.2022.9959552" target="_blank">
                      IEEE International Conference on Advanced Video and Signal Based Surveillance
                      (AVSS)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Super-resolution (SR) aims to recover the highresolution (HR) images from low-resolution (LR) images. Recently, various attempts, e.g., unsupervised SR models and domain-specific SR have achieved outstanding performance for various real-world applications. However, they significantly suffer from low generalization performance when trained on another domain dataset. Furthermore, they often exhibit performance degradation when the model continually learns multiple tasks; so-called catastrophic forgetting degrades the SR performance. In this paper, we are the first to propose a novel approach for continual multi-task SR named Replay-based Continual Representation Learning framework that can be applicable to GAN-based SR models, which utilizes feature memory for preserving the learned features from the previous task. Our experimental results demonstrate the effectiveness of RCRL in continual multi-task SR at improving generalization performance and alleviating catastrophic forgetting.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/jinyong_rcrl.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>STL-DP: Differentially Private Time Series Exploring Decomposition and Compression Methods</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Kyunghee Kim,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM CIKM Workshop on Privacy Algorithms in Systems
                      (PAS)
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>As time series data is collected and used in a variety of fields, the importance of preserving privacy on time series is also on the increase. This paper is a preliminary study of the Differential Privacy (DP) algorithm specially designed to provide privacy to time series data by integrating the time series decomposition technique. In particular, this study extends the Fourier Perturbation Algorithm (FPA) with Seasonal and Trend decomposition using LOESS (STL). In this work, we propose STL-DP, which first performs STL decomposition to the original data. Then we apply the FPA only to the core part of the time series, particularly trend or seasonal components, to provide privacy. In this preliminary study, we show that our approach consistently outperforms other baselines in terms of utility according to the experimental results.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cikmw22_minha.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>A<sup>2</sup>: Adaptive Augmentation for Mitigating Dataset Bias</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jaeju An,
                            
                          
                            
                              Taejun Kim,
                            
                          
                            
                              Donggeun Ko,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Asian Conference on Computer Vision
                      (ACCV)
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The trained networks can often suffer from overfitting issues due to the unintended bias in a dataset causing inaccurate, unreliable, and untrustworthy results. To tackle this problem, we propose a novel augmentation framework, Adaptive Augmentation (A^2), based on a generative model and few-shot adaptation for augmenting bias-conflict samples that help classifiers learn debiased representations without any prior knowledge about bias types. Our framework consists of three steps: 1) extracting bias-conflict samples from a biased dataset in an unsupervised manner, 2) training a generative model with the biased dataset and adapting biased distribution from the generative model to the extracted bias-conflict samples' distribution, and 3) augmenting bias-conflict samples by translating bias-align samples with the trained generative model. Therefore, our classifier can effectively learn the debiased representation without human supervision.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/accv22_jaeju.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Discussion about Attacks and Defenses for Fair and Robust Recommendation System Design</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Mirae Kim and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.48550/arxiv.2210.07817" target="_blank">
                      ACM RecSys Workshop on Responsible Recommendation
                      (FAccTRec)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Information has exploded on the Internet and mobile with the advent of the big data era. In particular, recommendation systems are widely used to help consumers who struggle to select the best products among such a large amount of information. However, recommendation systems are vulnerable to malicious user biases, such as fake reviews to promote or demote specific products, as well as attacks that steal personal information. Such biases and attacks compromise the fairness of the recommendation model and infringe the privacy of users and systems by distorting data.Recently, deep-learning collaborative filtering recommendation systems have shown to be more vulnerable to this bias. In this position paper, we examine the effects of bias that cause various ethical and social issues, and discuss the need for designing the robust recommendation system for fairness and stability.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/facctrec_mirae22.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Accelerating CNN via Dynamic Pattern-based Pruning Network</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Gwanghan Lee,
                            
                          
                            
                              Saebyeol Shin,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3511808.3557225" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Most dynamic pruning methods fail to achieve actual acceleration due to the extra overheads caused by indexing and weight-copying to implement the dynamic sparse patterns for every input sample. To address this issue, we propose Dynamic Pattern-based Pruning Network, which preserves the advantages of both static and dynamic networks. Unlike previous dynamic pruning methods, our novel method dynamically fuses static kernel patterns, enhancing the kernel's representational power without additional overhead. Moreover, our dynamic sparse pattern enables an efficient process using BLAS libraries, accomplishing actual acceleration. We demonstrate the effectiveness of the proposed network on CIFAR and ImageNet, outperforming the state-of-the-art methods achieving better accuracy with lower computational cost.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cikm22_gwanhan.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Samba: Identifying Inappropriate Videos for Young Children on YouTube</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Binh M. Le,
                            
                          
                            
                              Rajat Tandon,
                            
                          
                            
                              Chingis Oinar,
                            
                          
                            
                              Jeffrey Liu,
                            
                          
                            
                              Uma Durairaj,
                            
                          
                            
                              Jiani Guo,
                            
                          
                            
                              Spencer Zahabizadeh,
                            
                          
                            
                              Sanjana Ilango,
                            
                          
                            
                              Jeremy Tang,
                            
                          
                            
                              Fred Morstatter,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Jelena Mirkovic
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this paper, we propose a fusion model, called Samba, which uses both metadata and video subtitles for content classifying YouTube videos for kids. Previous studies utilized metadata, such as video thumbnails, title, comments, ect., for detecting inappropriate videos for young viewers.  Such metadata-based approaches achieve high accuracy but still have significant misclassifications due to the reliability of input features. By adding representation features from subtitles, which are pretrained with a self-supervised contrastive framework, our Samba model can outperform other state-of-the-art classifiers by at least 7%. We also publish a large-scale, comprehensive dataset of 70K videos for future studies.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cikm22_binh.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Towards an Awareness of Time Series Anomaly Detection Models' Adversarial Vulnerability</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Binh M. Le,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3511808.3557073" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Time series anomaly detection is studied in statistics, ecology, and computer science. Numerous time series anomaly detection strategies have been presented utilizing deep learning. Many of these methods exhibit state-of-the-art performance on benchmark datasets, giving the false impression that they are robust and deployable in a wide variety of real-world scenarios. In this study, we demonstrate that adding modest adversarial perturbations to sensor data severely weakens anomaly detection systems.   Under well-known adversarial attacks such as Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD), we demonstrate that the performance of state-of-the-art deep neural networks (DNNs) and graph neural networks (GNNs), which claim to be robust against anomalies and possibly be used in real-world systems, drops to 0%. We demonstrate for the first time, to our knowledge, the vulnerability of anomaly detection systems to adversarial attacks. This study aims to increase awareness of the adversarial vulnerabilities of time series anomaly detectors.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cikm22_shah.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Sliding Cross Entropy for Self-Knowledge Distillation</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hanbeen Lee,
                            
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3511808.3557453" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Knowledge distillation (KD) is a powerful technique for improving the performance of a small model by leveraging the knowledge of a larger model. Despite its remarkable performance boost, KD has a drawback with the substantial computational cost of pre-training larger models in advance. Recently, a method called self-knowledge distillation has emerged to improve the model's performance without any supervision. In this paper, we present a novel plug-in approach called Sliding Cross Entropy (SCE) method, which can be combined with existing self-knowledge distillation to significantly improve the performance. Specifically, to minimize the difference between the output of the model and the soft target obtained by self-distillation, we split each softmax representation by a certain window size, and reduce the distance between sliced parts. Through this approach, the model evenly considers all the inter-class relationships of a soft target during optimization. The extensive experiments show that our approach is effective in various tasks, including classification, object detection, and semantic segmentation. We also demonstrate SCE consistently outperforms existing baseline methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cikm22_hanbeen.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Selective Tensorized Multi-layer LSTM for Orbit Prediction</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Youjin Shin,
                            
                          
                            
                              Eun-Ju Park,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Okchul Jung,
                            
                          
                            
                              and Daewon Chung
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3511808.3557138" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Although the collision of space objects not only incurs a high cost but also threatens human life, the risk of collision between satellites has increased, as the number of satellites has rapidly grown due to the significant interests in many space applications. However, it is not trivial to monitor the behavior of the satellite in real-time since the communication between the ground station and spacecraft are dynamic and sparse, and there is an increased latency due to the long distance. Accordingly, it is strongly required to predict the orbit of a satellite to prevent unexpected contingencies such as a collision. Therefore, the real-time monitoring and accurate orbit prediction is required. Furthermore, it is necessarily to compress the prediction model, while achieving a high prediction performance in order to be deployable in the real systems. Although several machine learning and deep learning-based prediction approaches have been studied to address such issues, most of them have applied only basic machine learning models for orbit prediction without considering the size, running time, and complexity of the prediction model. In this research, we propose Selective Tensorized multi-layer LSTM (ST-LSTM) for orbit prediction, which not only improves the orbit prediction performance but also compresses the size of the model that can be applied in practical deployable scenarios. To evaluate our model, we use the real orbit dataset collected from the Korea Multi-Purpose Satellites (KOMPSAT-3 and KOMPSAT-3A) of the Korea Aerospace Research Institute (KARI) for 5 years. In addition, we compare our ST-LSTM to other machine learning-based regression models, LSTM, and basic tensorized LSTM models with regard to the prediction performance, model compression rate, and running time.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cikm22_youjin.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>GLAMD: Global and Local Attention Mask Distillation for Object Detectors</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Younho Jang,
                            
                          
                            
                              Wheemyung Shin,
                            
                          
                            
                              Jinbeom Kim,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Sung-Ho Bae
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-031-20080-9_27" target="_blank">
                      European Conference on Computer Vision
                      (ECCV)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=2</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Knowledge distillation (KD) is a well-known model compression strategy to improve models' performance with fewer parameters. However, recent KD approaches for object detection have faced two limitations. First, they distill nearby foreground regions, ignoring potentially useful background information. Second, they only consider global contexts, thereby the student model can hardly learn local details from the teacher model. To overcome such challenging issues, we propose a novel knowledge distillation method, GLAMD, distilling both global and local knowledge from the teacher. We divide the feature maps into several patches and apply an attention mechanism for both the entire feature area and each patch to extract the global context as well as local details simultaneously. Our method outperforms the state-of-the-art methods with 40.8 AP on COCO2017 dataset, which is 3.4 AP higher than the student model (ResNet50 based Faster R-CNN) and 0.7 AP higher than the previous global attention-based distillation method.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/jinpum_eccv22.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>다중 스케일 특성 생성 네트워크</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Gwanghan Lee,
                            
                          
                            
                              Saebyeol Shin,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      한국컴퓨터종합학술대회
                      (KCC)
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>조기 종료 네트워크(early-exit network)는 추론 시 동적으로 모델 복잡도를 낮춤으로써 신경망의 효율성을 높인다. 기존 연구들은 입력 샘플이나 모델 구조의 중복성(redundancy)을 줄이는 데 집중하였으나 고차원 특징 정보가 부족한 초기 분류기들이 전체 네트워크 성능에 치명적인 영향을 끼치는 문제를 해결하지 못했다. 본 연구는 중복성을 줄이는 것뿐만 아니라 합성곱 커널(convolution kernel) 중앙에서 가중치들을 공유하면서 효율적으로 다중 스케일(multi-scale) 특징을 생성하여 조기 종료 네트워크의 성능을 향상시킨다. 또한 이 논문의 게이팅 네트워크(gating network)는 네트워크의 서로 다른 위치에 있는 각 합성곱 레이어에 따라 최적의 다중 스케일 특징 비율을 결정하도록 학습된다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>이미지 전처리 방법을 통한 딥페이크 탐지 회피 연구</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              Jeonghyun Kim,
                            
                          
                            
                              Taejune Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      한국컴퓨터종합학술대회
                      (KCC)
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>오늘날 국제사회에서 딥페이크(Deepfake) 기술에 대한 우려가 점점 커지고 있다. 딥페이크는 여러 종류의 이미지, 영상들의 얼굴을 짧은 시간 만에 바꿀 수 있는 기술로, 손쉽게 왜곡된 정보를 전파할 수 있기 때문이다. 이에따라딥페이크이미지,영상에대응하기위한탐지기술연구및시도가이뤄졌다. 그러나,탐지기술연구를 가능케 만들어 줄 수 있는 고품질의 데이터셋(dataset)을 생성하는 연구는 더디게 이뤄졌다. 본 논문에서는 딥페 이크 탐지 기술 발전에 필수 불가결한 요소인 고품질 데이터 생성에 대한 새로운 방법론을 제시하고 이를 통해 딥페이크 탐지 기술의 한계 및 발전 방향성에 대해 살펴보고자 한다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Deep Learning Algorithm for Postmortem Face Reconstruction (딥러닝 기술을 활용한 사후 시신 얼굴 복원)</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hajin Kim,
                            
                          
                            
                              Chingis Oinar,
                            
                          
                            
                              UiHyeon Shin,
                            
                          
                            
                              Woo Simon S,
                            
                          
                            
                              and Moon-Young Kim
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      제29회 대한기초의학 학술대회
                      (대한법의학회)
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>As the number of lonely deaths increases due to the aging population and the increase in single-person households, the frequency of discovery of decomposed corpses in death cases is gradually increasing. In the wake of the strengthening of on-site guidelines by the National Police Agency and the adjustment of the prosecution and police investigation rights, the need for identification and autopsy at the scene is being emphasized. Although the existing forensic face restoration technology using face bones has accumulated a number of previous studies, there is a limitation in that the restoration results may vary due to many factors such as the thickness and nature of facial soft tissue, shape of eyes or nose, and distribution of body hair. Based on the fact that facial recognition technology using facial landmarks is becoming common all over the world, this study aims to help quickly and accurately identify the faces of corrupt bodies that expand due to postmortem-change.
In this study, living data such as ID cards and post-mortem data were collected for bodies identified with fingerprints, and compared pairs were formed, and face recognition technology used the MTCNN model, which is currently widely used in the field. The artificial intelligence model, which determines whether live data and post-data match, selected and analyzed Arcface, which is the same among a total of seven open-source models (VGG-Face, FaceNet, OpenFace, DeepFace, DeepID, ArcFace, Dlib).
The performance of the artificial intelligence model (Arcface) was evaluated by comparing the results of the judgment of the expert group, the general public group, and the entire human group. As a result of comparison using 107 pairs of original data, the same person judgment rate was found to be 51.4% in the expert group, 22.4% in the general population, and 29.0% in the total human group, and the artificial intelligence model was 47.7%. As a result of reviewing the original data, it was determined that changes in skin color due to decomposition could affect the performance of artificial intelligence models According to this judgment, when the original data were preprocessed in gray scale, the judgment rate of the same person as the artificial intelligence model was 50.5%, which showed an improvement in performance of about 3%. 
Through this study, it was found that only the currently developed artificial intelligence model showed facial recognition performance close to that of a group of experts. It is expected that face recognition performance can be further improved if various pretreatment technologies reflecting the characteristics of the postmortem change are developed and applied in the future.
인구 고령화 및 1인 가구의 증가는 고독사의 증가로 이어져 변사사건에서 부패 시신이 발견되는 빈도가 점차 높아지고 있다. 경찰청의 현장 지침 강화 및 검경 수사권 조정 등을 계기로 현장에서는 신원 확인 및 부검의 필요성이 강조되고 있다. 얼굴뼈를 활용한 기존의 법의학적 얼굴 복원 기술은 다수의 선행연구 결과가 축적되어 있지만, 얼굴 연부조직의 두께나 성상, 눈이나 코의 형태, 체모의 분포 등의 고려 요소가 많아 복원 결과가 달라질 수 있다는 한계가 존재한다. 본 연구는 얼굴의 특징점(face landmark)을 활용하는 얼굴 인식 기술이 전세계적으로 보편화되고 있다는 점에 착안하여, 사후변화로 인해 연부조직이 팽창된 부패 시신의 얼굴을 복원하거나 생전의 사진과 비교하여 동일인 여부를 판정함으로써 신속하고 정확한 신원확인에 도움을 주고자 한다. 
 본 연구에서는 지문 등으로 신원이 확인된 시신을 대상으로 신분증 등의 생전 데이터와 검안 또는 부검 당시 촬영된 사후데이터를 수집한 뒤 각각 짝을 지어 비교쌍을 구성하였으며, 얼굴 인식 기술은 현재 해당 분야에서 많이 활용되고 있는 MTCNN 모델을 활용하였다. 생전데이터와 사후데이터의 일치 여부를 판단하는 인공지능모델은 총 7개의 open source 모델(VGG-Face, FaceNet, OpenFace, DeepFace, DeepID, ArcFace, Dlib) 중 가장 동일인 판정률의 빈도가 가장 높게 나타난 Arcface를 선정하여 분석하였다.
 인공지능모델(Arcface)의 성능은 전문가집단과 일반인 집단, 전체 사람 집단의 판정 결과와 비교하여 평가하였다. 원본 데이터 107쌍을 이용한 비교 결과, 동일인 판정률은 전문가집단 51.4%, 일반인 22.4%, 전체 사람 집단 29.0%로 조사되었으며, 인공지능모델은 47.7%로 나타났다. 원본 데이터를 검토한 결과, 부패로 인한 피부색의 변화가 인공지능모델의 성능에 영향을 줄 가능성이 있다고 판단되었다. 이러한 판단에 따라 원본 데이터를 회색조(gray scale)로 전처리하였을 때 인공지능모델의 동일인 판정률은 50.5%로, 약 3%의 성능이 향상되는 것을 볼 수 있었다. 
본 연구를 통해 현재 개발되어 있는 인공지능모델만으로도 전문가 집단에 근접한 얼굴 인식 성능을 보이는 것을 알 수 있었다. 향후 사후변화의 특성을 반영한 다양한 전처리 기법을 개발하여 적용할 경우 얼굴 인식 성능을 더욱 향상시킬 수 있을 것으로 기대된다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="dead.JPG" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Learning Sparse Latent Graph Representations for Anomaly Detection in Multivariate Time Series</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Siho Han and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3534678.3539117" target="_blank">
                      ACM SIGKDD Conference on Knowledge Discovery and Data Mining
                      (KDD)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Anomaly detection in high-dimensional time series is typically tackled using either reconstruction- or forecasting-based deep learning algorithms. Both streams of approach have seen enormous success in terms of detection accuracy due to their abilities to learn compressed data representations and model temporal dependencies, respectively. However, most existing methods disregard the relationships between features, information that would be extremely useful when incorporated into the model. How can we effectively combine the best of reconstruction and forecasting models while also capturing feature interdependencies? In this work, we introduce Fused Sparse Autoencoder and Graph Net (FuSAGNet), which jointly optimizes reconstruction and forecasting while explicitly modeling the relationships within multivariate time series. Our approach combines Sparse Autoencoder and Graph Neural Network, the latter of which predicts future time series behavior from sparse latent representations learned by the former as well as graph structures learned through recurrent feature embedding. Experimenting on three real-world cyber-physical system datasets, we empirically demonstrate that the proposed method enhances the overall anomaly detection performance, outperforming baseline approaches. Moreover, we show that mining sparse latent patterns from high-dimensional time series improves the robustness of the graph-based forecasting model. Lastly, we conduct visual analyses to investigate the interpretability of both recurrent feature embedding vectors and sparse latent representations.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/kdd22_sean.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Evading Deepfake Detectors via High Quality Face Pre-Processing Methods</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              Taejune Kim,
                            
                          
                            
                              Jeonghyeon Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/icpr56361.2022.9956520" target="_blank">
                      International Conference on Pattern Recognition
                      (ICPR)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Today, various multimedia content can be accessed and shared from any location via the Internet. In addition to normal content, there is an extensive amount of manipulated multimedia that can raise various social issues and concerns. Among the various types of manipulated media, deepfakes can be abused in impersonation or spreading fake information. Therefore, numerous studies have been performed to detect deepfakes to alleviate these concerns, and studies such as FaceForensics++ (FF++) and DeepFake Detection Challenge (DFDC) have sparked these studies by providing deepfake datasets. The deepfake datasets were utilized for supervised learning in conjunction with developing sophisticated neural networks and showed a high detection performance. Since powerful neural networks can learn even subtle details about an image, they must be trained on realistic deepfakes created by advanced deepfake generation technologies to improve the robustness of existing detectors. In order to boost the performance of deepfake detection models, we propose an approach to creating more realistic deepfake images by removing "detectable" artifacts from existing deepfake datasets' images. By applying the proposed method to the original deepfake dataset, we demonstrate that our technique can significantly reduce the detection performance of existing deepfake detectors. Our experimental results show the vulnerability of deployed detectors and pave the way for further improvement.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ICPR_2022_Evading_deepfake.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Efficient Two-stage Model Retraining for Machine Unlearning</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Junyaup Kim and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/cvprw56347.2022.00482" target="_blank">
                      IEEE/CVF CVPR Workshop on Human-centered Intelligent Services: Safe and Trustworthy
                      (HCIS)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>With the rise of the General Data Protection Regulation (GDPR), user data holders should guarantee the “individual’s right to be forgotten”. It means user data holders must completely remove user data when they receive the request. However, enabling a deep learning model to exclude specific data used during training is challenging. We can’t define what is ”forgetting” in deep learning and how to do it. To address this issue, we propose an efficient machine unlearning architecture to be used for computer vision classification models. Our approach consists of two-stage, where in the first stage we render a deep learning model that loses information with contrastive labels in the requested dataset. Second, we retrain the first stage output model with knowledge distillation (KD). Using this two-stage approach, we can substantiate the removal or forgetness of the requested dataset in the deep learning model. With various datasets used for multimedia applications, we demonstrate that our approach achieves performance on par or even higher accuracy than the original model, while effectively removing the requested data.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/junjyuap_cvprw22.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Negative Adversarial Example Generation Against Naver's Celebrity Recognition API</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Keeyoung Kim and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3494109.3527193" target="_blank">
                      ACM ASIACCS Workshop on Security Implications of Deepfakes and Cheapfakes
                      (WDC)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Deep Neural Networks (DNNs) are very effective in image classification, detection and recognition due to a large number of available data. However, they can be easily fooled by adversarial examples and produce incorrect results, which can cause problems for many applications. In this work, we focus on generating adversarial images and exploring and assessing possible negative impacts caused by these examples. As a case study, we create adversarial images against Naver’s celebrity recognition (NCR) API, as Naver is the leading machine learning APIs service provider in South Korea. We demonstrate that it is extremely easy to fool the online DNN-based APIs using adversarial examples and discuss possibe negative impacts resulting from these adversarial examples.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/wdc_kim.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>A Face Pre-Processing Approach to Evade Deepfake Detector</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Taejune Kim,
                            
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              Jeonghyeon Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3494109.3527190" target="_blank">
                      ACM ASIACCS Workshop on Security Implications of Deepfakes and Cheapfakes
                      (WDC)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Recently, various image synthesis technologies have increased the prevalence of impersonation attacks, and with the development of such technologies, the amount of damage such as defamation has also increased. Deepfake, the representative of the impersonation technique, has already evolved to the point where people cannot distinguish, leading to an urgent need for detection methods. Currently, in order to detect deepfakes, many deepfake datasets are widely used in deep neural networks using supervision learning. However, although this method is robust to the images synthesized by deepfake generation methods already known, it remains undefined whether deepfakes created by unknown techniques can be detected. Accordingly, to detect more challenging deepfakes, we present a pre-processing technique that mitigates the artifacts of deepfakes and makes them appear more natural. The proposed method can be combined with the existing deepfake creation method to generate a more threatening deepfake image. Furthermore, through extensive experiments, we demonstrate that our method can significantly lower the performance of state-of-the-art detectors and expose the vulnerability of deployed detectors.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/wdc2_taejune.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Deepfake Detection for Fake Images with Facemasks</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sangjun Lee,
                            
                          
                            
                              Donggeun Ko,
                            
                          
                            
                              Jinyong Park,
                            
                          
                            
                              Saebyeol Shin,
                            
                          
                            
                              Donghee Hong,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3494109.3527189" target="_blank">
                      ACM ASIACCS Workshop on Security Implications of Deepfakes and Cheapfakes
                      (WDC)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Hyper-realistic face image generation and manipulation have givenrise to numerous unethical social issues, e.g., invasion of privacy,threat of security, and malicious political maneuvering, which re-sulted in the development of recent deepfake detection methodswith the rising demands of deepfake forensics. Proposed deepfakedetection methods to date have shown remarkable detection perfor-mance and robustness. However, none of the suggested deepfakedetection methods assessed the performance of deepfakes withthe facemask during the pandemic crisis after the outbreak of theCovid-19. In this paper, we thoroughly evaluate the performance ofstate-of-the-art deepfake detection models on the deepfakes withthe facemask. Also, we propose two approaches to enhance themasked deepfakes detection:face-patchandface-crop. The experi-mental evaluations on both methods are assessed through the base-line deepfake detection models on the various deepfake datasets.Our extensive experiments show that, among the two methods,face-cropperforms better than theface-patch, and could be a trainmethod for deepfake detection models to detect fake faces withfacemask in real world.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Zoom-DF: A Dataset for Video Conferencing Deepfake</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Geon-Woo Park,
                            
                          
                            
                              Eun-Ju Park,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3494109.3527195" target="_blank">
                      ACM ASIACCS Workshop on Security Implications of Deepfakes and Cheapfakes
                      (WDC)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>With the growth of deep learning studies, the technologies of generating deepfake videos have been advanced. While the manipulated videos are so sophisticated that one cannot differentiate between real and fake, one can create such videos with little effort. These technologies are likely to be abused by people with malicious intent. To address the problem, the algorithms for detecting deepfakes have been researched abundantly. The performance of the detectors, however, depends on the amount and the domain of the training data. In this paper, we introduce a new deepfake dataset generated by an algorithm changing an original image to a sequence of fake images. We evaluate existing models detecting deepfakes on the new dataset and demonstrate that the accuracy of the models degrades. Their performance is recovered when trained with the new dataset.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/wdc22_deonwoo.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>PasswordTensor: Analyzing and explaining password strength using tensor decomposition</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Youjin Shin and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.cose.2022.102634" target="_blank">
                      Computers &amp; Security
                      
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =4.4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>A textual password is widely used for user authentication for a variety of applications. Passwords that are easy to remember are also easy to be guessed, while complex and long passwords that provide strong security are difficult to remember. Also, there has been limited quantitative research to understand the factors that make passwords strong. In this research, we aim to expand our understanding of passwords through the lenses of data-driven analysis by characterizing a large number of password datasets with four different hypotheses. In particular, we use the tensor decomposition method that is effective in analyzing unlabeled high dimensional data. We first obtain 362,805 passwords from four different leaked password datasets. Next, we generate syntactic and semantic features for each password, then classify it into three strength groups using a statistical guessing attack model. Finally, we construct a 3rd-order password tensor and decompose it using the PARAFAC2 algorithm to examine the main characteristics which make passwords strong.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/computer_security_2022_yj.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>A Survey of Deep Learning-Based Object Detection Methods and Datasets for Overhead Imagery</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Junhyung Kang,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Han Oh,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/access.2022.3149052" target="_blank">
                      IEEE Access
                      
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Although extensive studies in deep learning-based object detection have achieved remarkable performance and success, they are still ineffective yielding a low detection performance, due to the underlying difficulties in overhead images. Thus, high-performing object detection in overhead images is an active research field to overcome such difficulties. This survey paper provides a comprehensive overview and comparative reviews on the most up-to-date deep learning-based object detection in overhead images. Especially, our work can shed light on capturing the most recent advancements of object detection methods in overhead images and the introduction of overhead datasets that have not been comprehensively surveyed before.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ieee_access_junhyung.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Am I a Real or Fake Celebrity? Evaluating Face Recognition and Verification APIs under Deepfake Impersonation Attack</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sowon Jeon,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3485447.3512212" target="_blank">
                      ACM Web Conference
                      (WWW)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Recent advancements in web-based multimedia technologies, such as face recognition web services powered by deep learning, have been significant. However, such technologies face persistent threats, as virtually anyone with access to deepfakes can quickly launch impersonation attacks, which pose a serious threat to authentication services. Despite its gravity, deepfake abuse involving commercial web services have not been investigated. Thus, we examine the robustness of black-box commercial face recognition web APIs (Microsoft, Amazon, Naver, and Face++) and open-source tools (VGGFace and ArcFace) against Deepfake Impersonation (DI) attacks. We demonstrate the vulnerability of face recognition technologies to DI attacks, achieving respective success rates of 78.0% for targeted (TA) attacks; we also propose mitigation strategies, lowering respective attack success rates to as low as 1.26% for TA attacks with adversarial training.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/www22_shah.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>BZNet: Unsupervised Multi-scale Branch Zooming Network for Detecting Low-quality Deepfake Videos</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Jaeju An,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3485447.3512245" target="_blank">
                      ACM Web Conference
                      (WWW)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Generating a deep learning-based fake video has become no longer rocket science. The advancement of automated Deepfake (DF) generation tools that mimic certain targets has rendered society vulnerable to fake news or misinformation propagation. In real-world scenarios, DF videos are compressed to low-quality (LQ) videos, taking up less storage space and facilitating dissemination through the web and social media. Such LQ DF videos are much more challenging to detect than high-quality (HQ) DF videos. To address this challenge, we rethink the design of standard deep learning-based DF detectors, specifically exploiting feature extraction to enhance the features of LQ images. We propose a novel LQ DF detection architecture, multi-scale Branch Zooming Network (BZNet), which adopts an unsupervised super-resolution (SR) technique and utilizes multi-scale images for training. We train our BZNet only using highly compressed LQ images and experiment under a realistic setting, where HQ training data are not readily accessible. Extensive experiments on the FaceForensics++ LQ and GAN-generated datasets demonstrate that our BZNet architecture improves the detection accuracy of existing CNN-based classifiers by 4.21\% on average. Furthermore, we evaluate our method against a real-world Deepfake-in-the-Wild dataset collected from the internet, which contains 200 videos featuring 50 celebrities worldwide, outperforming the state-of-the-art methods by 4.13%.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/www22_jaeju.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Residual Size is Not Enough for Anomaly Detection: Improving Detection Performance using Residual Similarity in Multivariate Time Series</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jeong-Han Yun,
                            
                          
                            
                              Jonguk Kim,
                            
                          
                            
                              Won-Seok Hwang,
                            
                          
                            
                              Young Geun Kim,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Byung-Gil Min
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Unsupervised anomaly detection is commonly performed by identifying unusual data samples (or anomalies) from the residual size produced by machine learning algorithms based on normal data (e.g., the residuals of regression models or reconstruction errors of autoencoder models), assuming that anomalies cause large residuals. Unfortunately, anomalies do not always cause large residuals. Anomaly detection algorithms based on residual size can miss anomalies that cause only small or noisy residuals for each variable in a multivariate time-series. To overcome this issue, we propose "neighbors to residuals" (N2RE), a novel anomaly scoring function based on residual similarity using nearest neighbor distance (NND). Even if residuals of anomalies are small, they show patterns that are different from those of residuals of normal data. Using N2RE can improve anomaly detection performance and reduce the variation in anomaly detection performance due to threshold changes. Experiments with various models on three cyber-physical system datasets verify that N2RE can achieve 19% higher anomaly detection performance than previous approaches without changes to the models.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>PTD: Privacy-Preserving Human Face Processing Framework using Tensor Decomposition</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2022
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Training data may include personal information such as human faces, which requires anonymization to provide user privacy. However, after anonymization, the performance of the original machine learning (ML) model degrades due to the reduced or missing information. In this work, we introduce a novel privacy-preserving tensor decomposition (PTD) method to anonymize human faces. Further, we evaluate
real vs. fake human face detection task as a practical use case scenario. Our approach achieves high performance as well as training data efficiency, where the essence of our approach is based on tensor decomposition to ensure face data privacy. In particular, we demonstrate that the core tensor of Tucker decomposition generated from the original face input can effectively represent the underlying characteristics of the original face data; that is, learning only from the core tensors is sufficient for differentiating real human face images from deepfakes. Also, we show that the original human face inputs are anonymized and cannot be recovered from the core tensors under different attacker models from the randomized HOOI algorithm. Through extensive experiments and analysis, we demonstrate that our method can result in high detection performance comparable to those of popular anonymization methods. Therefore, we show that our work strikes the balance between privacy and performance through the novel use of tensor decomposition.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/tensor.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>ADD: Frequency Attention and Multi-View Based Knowledge Distillation to Detect Low-Quality Compressed Deepfake Images</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Binh M. Le and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1609/aaai.v36i1.19886" target="_blank">
                      AAAI Conference on Artificial Intelligence
                      (AAAI)
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Despite significant advancements of deep learning-based forgery detectors for distinguishing manipulated deepfake images, most detection approaches suffer from moderate to significant performance degradation with low-quality compressed deepfake images.
Because of the limited information in low-quality images, detecting low-quality deepfake remains an important challenge. In this work, we apply frequency domain learning and optimal transport theory in knowledge distillation (KD) to specifically improve the detection of low-quality compressed deepfake images. We explore transfer learning capability in KD to enable a student network to learn discriminative features from low-quality images effectively. In particular, we propose the Attention-based Deepfake detection Distiller (ADD), which consists of two novel distillations: 1) frequency attention distillation that effectively retrieves the removed high-frequency components in the student network, and 2) multi-view attention distillation that creates multiple attention vectors by slicing the teacher’s and student’s tensors under different views to transfer the teacher tensor’s distribution to the student more efficiently. Our extensive experimental results demonstrate that our approach outperforms state-of-the-art baselines in detecting low-quality compressed deepfake images.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/aaai22_binh.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>ORVAE: One-Class Residual Variational Autoencoder for Voice Activity Detection in Noisy Environment</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hasam Khalid,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              TaeSoo Kim,
                            
                          
                            
                              Jong Hwan Ko,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/s11063-021-10695-4" target="_blank">
                      Neural Processing Letters
                      
                      , 2022
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =2.9</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Detecting human speech is foundational for a wide range of emerging intelligent applications. However, accurately detecting human speech is challenging, especially in the presence of unknown noise patterns. Generally, deep learning-based methods have shown to be more robust and accurate than statistical methods and other existing approaches. However, typically creating a noise-robust and more generalized deep learning-based Voice Activity Detection (VAD) system requires the collection of an enormous amount of annotated audio data. In this work, we develop a generalized model trained on limited types of human speeches with noisy backgrounds. Yet, it can detect human speech in the presence of various unseen noise types, which were not present in the training set. To achieve this, we propose a One-Class Residual connections-based Variational Autoencoder (ORVAE), which only requires a limited number of human speech data with noisy background for training, thereby eliminating the need for collecting data with diverse noise patterns. Evaluating ORVAE with three different datasets (synthesized TIMIT and NOI
SEX-92, synthesized LibriSpeech and NOISEX-92, and a Publicly Recorded dataset), our method outperforms other one-class baseline methods, achieving 1-scores of over 90% for multiple Signal-to-Noise Ratio (SNR) levels.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/orvae_npl.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
    </div>
  
    
    <div class="year-section" id="year-2021">
      <h4 style="margin-top:40px"><b>2021</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Evaluation of an Audio-Video Multimodal Deepfake Dataset using Unimodal and Multimodal Detectors</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hasam Khalid,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3476099.3484315" target="_blank">
                      ACM MM Workshop on Synthetic Multimedia - Audiovisual Deepfake Generation and Detection
                      (ADGD)
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Significant advancements made in the generation of deepfakes have caused security and privacy issues. Attackers can easily impersonate a person's identity in an image by replacing his face with the target person's face. Moreover, a new domain of cloning human voices using deep-learning technologies is also emerging. Now, an attacker can generate realistic cloned voices of humans using only a few seconds of audio of the target person. With the emerging threat of potential harm deepfakes can cause, researchers have proposed deepfake detection methods. However, they only focus on detecting a single modality, i.e., either video or audio. On the other hand, to develop a good deepfake detector that can cope with the recent advancements in deepfake generation, we need to have a detector that can detect deepfakes of multiple modalities, i.e., videos and audios. To build such a detector, we need a dataset that contains video and respective audio deepfakes. We were able to find a most recent deepfake dataset, Audio-Video Multimodal Deepfake Detection Dataset (FakeAVCeleb), that contains not only deepfake videos but synthesized fake audios as well. We used this multimodal deepfake dataset and performed detailed baseline experiments using state-of-the-art unimodal, ensemble-based, and multimodal detection methods to evaluate it. We conclude through detailed experimentation that unimodals, addressing only a single modality, video or audio, do not perform well compared to ensemble-based methods. Whereas purely multimodal-based baselines provide the worst performance.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ADGD21_hasam.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hasam Khalid,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.48550/arxiv.2108.05080" target="_blank">
                      Conference on Neural Information Processing Systems
                      (NeurIPS)
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Dataset Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>While the significant advancements have made in the generation of deepfakes using deep learning technologies, its misuse is a well-known issue now. Deepfakes can cause severe security and privacy issues as they can be used to impersonate a person's identity in a video by replacing his/her face with another person's face. Recently, a new problem of generating synthesized human voice of a person is emerging, where AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake audios and videos, a new generation of deepfake detectors is needed to focus on both video and audio collectively. To develop a competent deepfake detector, a large amount of high-quality data is typically required to capture real-world (or practical) scenarios. Existing deepfake datasets either contain deepfake videos or audios, which are racially biased as well. As a result, it is critical to develop a high-quality video and audio deepfake dataset that can be used to detect both audio and video deepfakes simultaneously. To fill this gap, we propose a novel Audio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake videos but also respective synthesized lip-synced fake audios. We generate this dataset using the most popular deepfake generation methods. We selected real YouTube videos of celebrities with four ethnic backgrounds to develop a more realistic multimodal dataset that addresses racial bias, and further help develop multimodal deepfake detectors. We performed several experiments using state-of-the-art detection methods to evaluate our deepfake dataset and demonstrate the challenges and usefulness of our multimodal Audio-Video deepfake dataset.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/fakeceleb_nips2021.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>VFP290K: A Large-Scale Benchmark Dataset for Vision-based Fallen Person Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jaeju An,
                            
                          
                            
                              Jeong‐Ho Kim,
                            
                          
                            
                              Hanbeen Lee,
                            
                          
                            
                              Jinbeom Kim,
                            
                          
                            
                              Junhyung Kang,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              Saebyeol Shin,
                            
                          
                            
                              Dong-Hee Hong,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Neural Information Processing Systems
                      (NeurIPS)
                      , 2021
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Dataset Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Detection of fallen persons due to, for example, health problems, violence, or accidents, is a critical challenge. Accordingly, detection of these anomalous events is of paramount importance for a number of applications, including but not limited to CCTV surveillance, security, and health care. Given that many detection systems rely on a comprehensive dataset comprising fallen person images collected under diverse environments and in various situations is crucial. However, existing datasets are limited to only specific environmental conditions and lack diversity. To address the above challenges and help researchers develop more robust detection systems, we create a novel, large-scale dataset for the detection of fallen persons composed of fallen person images collected in various real-world scenarios, with the support of the South Korean government. Our Vision-based Fallen Person (VFP290K) dataset consists of 294,714 frames of fallen persons extracted from 178 videos, including 131 scenes in 49 locations. We empirically demonstrate the effectiveness of the features through extensive experiments analyzing the performance shift based on object detection models. In addition, we evaluate our VFP290K dataset with properly divided versions of our dataset by measuring the performance of fallen person detecting systems. We ranked first in the first round of the anomalous behavior recognition track of AI Grand Challenge 2020, South Korea, using our VFP290K dataset, which can be found here. Our achievement implies the usefulness of our dataset for research on fallen person detection, which can further extend to other applications, such as intelligent CCTV or monitoring systems. The data and more up-to-date information have been provided at our VFP290K site.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/vfp290_nips.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>IVDR: Imitation learning with Variational inference and Distributional Reinforcement learning to find Optimal Driving Strategy</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Kihyung Joo and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/icmla52953.2021.00047" target="_blank">
                      IEEE International Conference on Machine Learning and Applications
                      (ICMLA)
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Current state-of-the-art autonomous driving technology significantly advanced, leveraging reinforcement learning (RL) algorithms, because it is not easy to apply a rule-based driving method that reflects all the various traffic conditions. Indeed, reinforcement learning can produce the possible optimal driving strategy of urban, rural, and motorway roads in various environmental conditions such as speed limits and school zones. However, it is challenging to adjust the parameters of the reward mechanism in RL, because the driving style of each user is very different. And it takes a massive amount of time and resources to conduct RL by reflecting all complex traffic conditions. However, if RL imitates the driving behavior of an expert, RL algorithm can proceed more quickly. Therefore, we propose a novel imitation learning framework, which combines an expert's driving behavior with a continuous behavior of an agent. Further, a deep reinforcement learning approach is used to mimic the expert's driving behavior. Therefore, we propose imitation learning with variational inference and distributional reinforcement learning (IVDR) algorithm. Our results show that IVDR achieves 80% better learning speed than the learning speed of other approaches and outperforms 12% higher in average reward. Our work shows great promise of using RL for autonomous driving and real vehicle driving simulation.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ivdr.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Efficient Multi-Scale Feature Generation Adaptive Network</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Gwanghan Lee,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              Minha Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3459637.3482337" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Recently, an early exit network, which dynamically adjusts the model complexity during inference time, has achieved remarkable performance. However, they were unsuccessful at resolving the performance drop of early classifiers that make predictions with insufficient high-level feature information. Consequently, the performance degradation of early classifiers had a devastating effect on the entire network performance sharing the backbone. In this paper, we propose an Efficient Multi-Scale Feature Generation Adaptive Network (EMGNet), which not only reduced the redundancy of the architecture but also generates multi-scale features to improve the performance of the early exit network.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cikm22_gh.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Crew Resource Management in Industry 4.0: Focusing on Human-Autonomy Teaming</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Sunny Yun and Simon Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.46246/kjasem.210013" target="_blank">
                      Korean Journal of Aerospace and Environmental Medicine
                      
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In the era of the 4th industrial revolution, the aviation industry is also growing remarkably with the development of artificial intelligence and networks, so it is necessary to study a new concept of CRM, which is required in the process of operating state-of-the-art equipment. The automation system, which has been treated only as a tool, is changing its role as a decision-making agent with the development of AI, and it is necessary to set clear standards for the role and responsibility in the safety-critical field. We present a new perspective on the automation system in the CRM program through the understanding of the autonomous system. In the future, autonomous system will develop as an agent for human pilots to cooperate, and accordingly, changes in role division and reorganization of regulations are required.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>DLPNet: Dynamic Loss Parameter Network using Reinforcement Learning for Aerial Imagery Detection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Junhyung Kang and Simon S Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3488933.3489031" target="_blank">
                      International Conference on Artificial Intelligence and Pattern Recognition
                      (AIPR)
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We propose DLPNet, a novel RL module to enable robust and stable training while achieving high performance in practical small mini-batch size conditions. DLPNet observes input image patches and acts to select the optimal parameters of the dynamic focal loss function for the baseline detector with every mini-batch training iteration during the training phase.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/dlpnet_icpr21.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Minha Kim,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3474085.3475535" target="_blank">
                      ACM International Conference on Multimedia
                      (MM)
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we apply continuous learning to neural networks' learning dynamics, emphasizing its potential to increase data efficiency significantly. We propose Continual Representation using Distillation (CoReD) method that employs the concept of Continual Learning (CoL), Representation Learning (ReL), and Knowledge Distillation (KD). We design CoReD to perform sequential domain adaptation tasks on new deepfake and GAN-generated synthetic face datasets, while effectively minimizing the catastrophic forgetting in a teacher-student model setting. Our extensive experimental results demonstrate that our method is efficient at domain adaptation to detect low-quality deepfakes videos and GAN-generated images from several datasets, outperforming the-state-of-art baseline methods.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/acmm21_minha.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>SmartConDetect: Highly Accurate Smart Contract CodeVulnerability Detection Mechanism using BERT</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sowon Jeon,
                            
                          
                            
                              Gilhee Lee,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM KDD workshop on programming language processing
                      (PLP)
                      , 2021
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this paper, we propose SmartConDetect to detect security vulnerabilities in smart contracts written in Solidity, which the most popular programming language for writing smart contracts on the Ethereum platform. SmartConDetect is designed as a static analysis tool to extract code fragments from smart contracts in Solidity and analyze code patterns using a pre-trained BERT model and a bidirectional LSTM model.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/smartcondetect.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Exploring the Asynchronous of the Frequency Spectra of GAN-generated Facial Images</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Binh M. Le and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.48550/arxiv.2112.08050" target="_blank">
                      IJCAI Workshop on Safety and Security of Deep Learning
                      
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The rapid progression of Generative Adversarial Networks (GANs) has raised a concern of their misuse for malicious purposes, especially in creating fake face images. Although many proposed methods succeed in detecting GAN-based synthetic images, they are still limited by the need for large quantities of the training fake image dataset and challenges for the detector's generalizability to unknown facial images. In this paper, we propose a new approach that explores the asynchronous frequency spectra of color channels, which is simple but effective for training both unsupervised and supervised learning models to distinguish GAN-based synthetic images. We further investigate the transferability of a training model that learns from our suggested features in one source domain and validates on another target domains with prior knowledge of the features' distribution. Our experimental results show that the discrepancy of spectra in the frequency domain is a practical artifact to effectively detect various types of GAN-based generated images.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ijcai2021_overall_diag.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>FReTAL: Generalizing Deepfake Detection using Knowledge Distillation and Representation Learning</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Minha Kim,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/cvprw53098.2021.00111" target="_blank">
                      IEEE/CVF CVPR Workshop on Media Forensics
                      (CVPRW)
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>As GAN-based video and image manipulation technologies become more sophisticated and easily accessible, there is an urgent need for effective deepfake detection technologies. Moreover, various deepfake generation techniques have emerged over the past few years.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/fretalgd.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Neural network laundering: Removing black-box backdoor watermarks from deep neural networks</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              William Aiken,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              Simon Woo,
                            
                          
                            
                              and Jungwoo Ryoo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.cose.2021.102277" target="_blank">
                      Computers &amp; Security
                      
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =3.58</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Creating a state-of-the-art deep-learning system requires vast amounts of data, expertise, and hardware, yet research into embedding copyright protection for neural networks has been limited. One of the main methods for achieving such protection involves relying on the susceptibility of neural networks to backdoor attacks, but the robustness of these tactics has been primarily evaluated against pruning, fine-tuning, and model inversion attacks.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/nb.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Will EU’s GDPR Act as an Effective Enforcer to Gain Consent?</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Junhyoung Oh,
                            
                          
                            
                              Jinhyoung Hong,
                            
                          
                            
                              Changsoo Lee,
                            
                          
                            
                              Jemin Justin Lee,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Kyungho Lee
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/access.2021.3083897" target="_blank">
                      IEEE Access
                      
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =3.67</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this study, we analyze GDPR provisions and recitals as well as relevant EU guidelines to propose quantifiable consent conditions to check whether website providers are compliant with the GDPR. We then evaluate the extent to which various popular web service providers meet these conditions.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="gdpr.PNG" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web APIs under Deepfake Impersonation Attack</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sowon Jeon,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.48550/arxiv.2103.00847" target="_blank">
                      arXiv
                      
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This work provides a measurement study on the robustness of black-box commercial face recognition APIs against Deepfake Impersonation (DI) attacks using celebrity recognition APIs as an example case study We achieved maximum success rates of 78.0% and 99.9% for targeted (ie, precise match) and non-targeted (ie, match with any celebrity) attacks, respectively. Moreover, we propose practical defense strategies to mitigate DI attacks, reducing the attack success rates to as low as 0% and 0.02% for targeted and non-targeted attacks, respectively.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/airor.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Revitalizing Self-Organizing Map: Anomaly Detection Using Forecasting Error Patterns</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Young Geun Kim,
                            
                          
                            
                              Jeong-Han Yun,
                            
                          
                            
                              Siho Han,
                            
                          
                            
                              Hyoung Chun Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-030-78120-0_25" target="_blank">
                      IFIP International Conference on ICT Systems Security and Privacy Protection
                      (IFIP SEC)
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we focus on improving the anomaly detection performance by leveraging the forecasting error patterns generated from prediction models, such as Sequence-to-Sequence (seq2seq), Mixture Density Networks (MDNs), and Recurrent Neural Networks (RNNs). To this end, we introduce Self-Organizing Map-based Anomaly Detector (SOMAD), an anomaly detection framework based on a novel test statistic, SomAnomaly, for Cyber-Physical System (CPS) security.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/rsom.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>TAR: Generalized Forensic Framework to Detect Deepfakes Using Weakly Supervised Learning</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Junyaup Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-030-78120-0_23" target="_blank">
                      IFIP International Conference on ICT Systems Security and Privacy Protection
                      (IFIP SEC)
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This work introduces a practical digital forensic tool to detect different types of deepfakes simultaneously
                                and proposes Transfer learning-based Autoencoder with Residuals (TAR). The ultimate goal
                                of this work is to develop an uni fied model to detect various types of deepfake videos
                                with high accuracy, with only a small number of training samples that can work well in
                                real-world settings. To achieve this, this work develops an autoencoder-based detection
                                model with Residual blocks and sequentially performs transfer learning to detect
                                different types of deepfakes simultaneously. The detection model shows a high detection
                                performance not only on the FF++ dataset but also on 200 real-world Deepfake-in-the-wild
                                videos.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/tgddw.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Detecting handcrafted facial image manipulations and GAN-generated facial images using Shallow-FakeFaceNet</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Youjin Shin,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.asoc.2021.107256" target="_blank">
                      Applied Soft Computing
                      
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =5.47</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we introduce a novel Handcrafted Facial Manipulation (HFM) image dataset and soft computing neural network models (Shallow-FakeFaceNets) with an efficient facial manipulation detection pipeline. Our neural network classifier model, Shallow-FakeFaceNet (SFFN), shows the ability to focus on the manipulated facial landmarks to detect fake images. This study is targeted for developing an automated defense mechanism to combat fake images used in different online services and applications, leveraging our state-of-the-art handcrafted fake facial dataset (HFM) and the neural network classifier Shallow-FakeFaceNet (SFFN).</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/dhfi.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Exploring Racial Bias in Classifiers for Face Recognition</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jaeju An,
                            
                          
                            
                              Jeongho Kim,
                            
                          
                            
                              Bosung Yang,
                            
                          
                            
                              Geonwoo Park,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      WWW Workshop on Fairness, Accountability, Transparency, Ethics and Society on the Web
                      (FATES)
                      , 2021
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Recent advancements in deep learning have allowed, among others,various applications of face recognition
                                systems, where a largeamount of face image data are typically required for training.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ExploringRacialBias.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>One Detector to Rule Them All: Towards a General Deepfake Attack Detection Framework</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sang Yup Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM Web Conference
                      (WWW)
                      , 2021
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Beyond detecting a single type of DF from benchmark deepfake datasets, we focus on developing a generalized approach to detect multiple types of DFs, including deepfakes from unknown generation methods such as DeepFake-in-the-Wild (DFW) videos. To better cope with unknown and unseen deepfakes, we introduce a Convolutional LSTM-based Residual Network (CLRNet), which adopts a unique model training strategy and explores spatial as well as the temporal information in a deepfakes. Through extensive experiments, we show that existing defense methods are not ready for real-world deployment. Whereas our defense method (CLRNet) achieves far better generalization when detecting various benchmark deepfake methods (97.57% on average). Furthermore, we evaluate our approach with a high-quality DeepFake-in-the-Wild dataset, collected from the Internet containing numerous videos and having more than 150,000 frames. Our CLRNet model demonstrated that it generalizes well against high-quality DFW videos by achieving 93.86% detection accuracy, outperforming existing state-of-the-art defense methods by a considerable margin.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/odtr.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>A Security Analysis of Blockchain-Based Did Services</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Bong Gon Kim,
                            
                          
                            
                              Young-Seob Cho,
                            
                          
                            
                              Seok-Hyun Kim,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/access.2021.3054887" target="_blank">
                      IEEE Access
                      
                      , 2021
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =4.09</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Decentralized identifiers (DID) has shown great potential for sharing user identities across different domains and services without compromising user privacy. DID is designed to enable the minimum disclosure of the proof from a user’s credentials on a need-to-know basis with a contextualized delegation.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/secBlock.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>BertLoc: Duplicate Location Record Detection in a Large-Scale Location Dataset</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sujin Park,
                            
                          
                            
                              Sangwon Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2021
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we propose BertLoc, a novel deep learning-based architecture to detect the duplicate location represented in different ways (e.g., Cafe vs. Coffee House) and effectively merge them into a single and consistent location record. BertLoc is based on Multilingual Bert Model followed by BiLSTM and CNN to effectively compare and determine whether given location strings are the same location or not. We evaluate BertLoc trained with more than half a million location data used in real service in South Korea and compare the results with other popular baseline methods. Our experimental results show that BertLoc outperforms other popular baseline methods with 0.952 F1-score, and shows great promise in detecting duplicate records in a large-scale location dataset.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/bertLoc.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
    </div>
  
    
    <div class="year-section" id="year-2020">
      <h4 style="margin-top:40px"><b>2020</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Image hashing algorithm to defend FGSM attacks on Neural Network</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Junyaup Kim,
                            
                          
                            
                              Siho Han,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://dash-lab.github.io/Publications/jy.pdf" target="_blank">
                      Cyber Defence Next Generation Technology and Science Conference
                      (CDNG)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this research, we present a performance evaluation of existing image hashing algorithms on defending deep learning models against adversarial attacks as an initial work to developing a new, time efficient image hashing algorithm. Upon experimenting with existing image hashing algorithms, we conclude that the wavelet hashing algorithm achieves the highest accuracy (75%) when detecting images generated from Neural Networks attacked by the FGSM, with a time complexity of 𝑂(𝑁).</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/jy.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>오픈소스 기반 격자 방식 PQC 알고리즘 분석 (Open-Source Code Analysis on Lattice-Based Post Quantum Cryptography)</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Minha Kim,
                            
                          
                            
                              Hakjun Moon,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://dash-lab.github.io/Publications/pqc.pdf" target="_blank">
                      Conference on Information Security and Cryptography-Winter
                      (CISC-W)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Currently used cryptography algorithms like RSA are vulnerable to quantum computers and are at risk of being deciphered in polynomial time. As the commercialization of quantum computers is soon to be realized, there is an urgent need for developing post-quantum cryptography(PQC) algorithms. In this paper, we analyze several lattice-based PQC algorithms from NIST Post-Quantum Cryptography Standardization project and test them in some representative security protocols to show their practicality.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/pqc.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Compensating for the Lack of Extra Training Data by Learning Extra Representation</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hyeonseong Jeon,
                            
                          
                            
                              Siho Han,
                            
                          
                            
                              Sangwon Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-030-69544-6_32" target="_blank">
                      Asian Conference on Computer Vision
                      (ACCV)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We introduce a novel framework, Extra Representation (ExRep), to surmount the problem of not having access to the JFT-300M data by instead using ImageNet and the publicly available model that has been pre-trained on JFT-300M. We take a knowledge distillation approach, treating the
                                model pre-trained on JFT-300M as well as on ImageNet as the teacher network and that pre-trained only on ImageNet as the student network. Our proposed method is capable of learning additional representation effects of the teacher model, bolstering the student model’s performance to a similar level to that of the teacher model, achieving high classification performance even without extra training data.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ctle.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>ITAD: Integrative Tensor-based Anomaly Detection System for Reducing False Postives of Satellite Systems</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Youjin Shin,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Myeong Shin Lee,
                            
                          
                            
                              Okchul Jung,
                            
                          
                            
                              Daewon Chung,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2020
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Reducing false positives while detecting anomalies is of growing importance for various industrial applications and mission-critical infrastructures, including satellite systems. Undesired false positives can be costly for such systems, bringing the operation to a halt for human experts to determine if the anomalies are true anomalies that need to be mitigated</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/itad.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>ZoomNet: Detecting Low-Quality Deepfakes In The Wild by Zooming In</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Jinhwan Kim,
                            
                          
                            
                              and Okyeop Jeon
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Proceedings of the Korean Information Science Society Conference
                      (한국법과학회 2020 추계학술대회)
                      , 2020
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Deepfakes have become a critical social problem, and detecting them is of utmost importance. Detecting high-quality deepfake videos from widely released datasets is more straightforward to detect than low-quality ones. Most of the prior research achieve above 90% accuracy for detecting the high-quality deepfake videos from the open dataset. However, in real life, many deepfake videos that are leaked through social networks such as YouTube and instant messaging applications are highly compressed. As a result, the distributed video's resolution becomes extremely lower, making the state-of-the-art detection methods harder. In this work, we propose ZoomNet, a practical framework to detect low-quality deepfakes with high accuracy. We build ZoomNet to have the ability to zoom into low-quality images effectively and can learn to distinguish deepfakes from real videos.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/zoomnet_2020.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Who is Delivering My Food? Detecting Food Delivery Abusers using Variational Reward Inference Networks</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          DaeYoung Yoon and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://dl.acm.org/doi/10.1145/3340531.3412750" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>The recent paramount success of the gig economy has introduced new business opportunities in different areas such as food delivery service. However, there are food delivery ride abusers who break the company rule by driving unauthorized vehicles that are not stated in the contract</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/yoon.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Can We Create a Cross-Domain Federated Identity for the Industrial Internet of Things without Google?</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Eunsoo Kim,
                            
                          
                            
                              Young-Seob Cho,
                            
                          
                            
                              Bedeuro Kim,
                            
                          
                            
                              Woojoong Ji,
                            
                          
                            
                              Seok-Hyun Kim,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Hyoungshick Kim
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/iotm.0001.2000050" target="_blank">
                      IEEE Internet of Things Magazine
                      
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Providing a cross-domain federated identity is essential for next-generation Internet services because information about user identity should be seamlessly exchanged across different domains for authentication and authorization.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/bc.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Applying Deep Learning to Reconstruct Pottery from Thousands Shards,</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Keeyoung Kim,
                            
                          
                            
                              Jinseok Hong,
                            
                          
                            
                              Sang-Hoon Rhee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://link.springer.com/chapter/10.1007/978-3-030-67670-4_3" target="_blank">
                      European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases
                      (ECML-PKDD)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>A great deal of time, patience, and effort are required to excavate pottery. For example, archaeologists dig hundreds to thousands of pottery shards from an excavation site. However, restoring pottery is a time-consuming and challenging process, requiring considerable amounts of expertise, experience, and time. </small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>OC-FakeDect: Classifying Deepfakes Using One-class Variational Autoencoder</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Hasam Khalid and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/cvprw50498.2020.00336" target="_blank">
                      IEEE Biometrics Council newsletter
                      
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>An image forgery method called Deepfakes can cause security and privacy issues by changing the identity of a person in a photo through the replacement of his/her face with a computer-generated image or another person’s face.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ocvae.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Forecasting Error Pattern-Based Anomaly Detection in Multivariate Time Series</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Seoyoung Park,
                            
                          
                            
                              Siho Han,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-030-67667-4_10" target="_blank">
                      European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases
                      (ECML-PKDD)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We propose novel Functional Data Analysis (FDA) and Autoencoder-based approaches for anomaly detection in the Secure Water Treatment (SWaT) dataset, which realistically represents a scaled-down industrial water treatment plant. We demonstrate that our methods can capture the underlying forecasting error patterns of the SWaT dataset generated by Mixture Density Networks (MDNs).</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/fepb.jpg" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>국내 딥페이크 기술 현황 및 제도적 대응방안 연구</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Sowon Jeon,
                            
                          
                            
                              Junhyung Kang,
                            
                          
                            
                              Jinhee Hwang,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Information Security and Cryptography-Summer
                      (CISC-S)
                      , 2020
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>최근 한국에서 ‘가짜 연예인 음란  동영상’ 및 ‘지인 능욕’에 사용되는 딥페이크(Deepfakes) 포르노 문제가 사회적인 이슈로 불거지고 있다. 딥페이크 기술은 인공지능 기술의 발전에 맞추어 더욱더 빠르게 발전하고 있으나 관련 규제와 대응방안이 부족한 실정이다. 따라서 본 논문에서는 딥페이크 기술의 현황과 딥페이크 관련 국내외 법적 규제 및 현행법의 한계점을 살펴보고, 이로부터 각 개인 및 기관의 역할과 대응방안을 제안한다.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/sowon1.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>T-GD: Transferable GAN-generated Images Detection Framework</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hyeonseong Jeon,
                            
                          
                            
                              Youngoh Bang,
                            
                          
                            
                              Junyaup Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.48550/arxiv.2008.04115" target="_blank">
                      International Conference on Machine Learning
                      (ICML)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we present the Transferable GAN-images Detection framework (T-GD), a robust transferable framework for an effective detection of GAN-images. T-GD is composed of a teacher and a student model that can iteratively teach and evaluate each other to improve the detection performance.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/tgd.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Real Time Localized Air Quality Monitoring and Prediction Through Mobile and Fixed IoT Sensing Network</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Dan Zhang and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/access.2020.2993547" target="_blank">
                      IEEE Access
                      
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =4.09</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Air pollution and its harm to human health has become a serious problem in many cities around the world.In recent years, research interests in measuring and predicting the quality of air around people has spiked.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>CAN-ADF: The controller area network attack detection framework</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Huy Kang Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.cose.2020.101857" target="_blank">
                      Computers &amp; Security
                      
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =3.58</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In recent years, there has been significant interest in developing autonomous vehicles such as self-driving cars. In-vehicle communications, due to simplicity and reliability, a Controller Area Network (CAN) bus is widely used as the de facto standard to provide serial communications between Electronic Control Units (ECUs)</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/canadf.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>OC-FakeDect: Classifying Deepfakes Using One-class Variational Autoencoder</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Hasam Khalid and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w39/Khalid_OC-FakeDect_Classifying_Deepfakes_Using_One-Class_Variational_Autoencoder_CVPRW_2020_paper.pdf" target="_blank">
                      IEEE/CVF CVPR Workshop on Media Forensics
                      (CVPRW)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In recent years, there has been significant interest in developing autonomous vehicles such as self-driving cars. In-vehicle communications, due to simplicity and reliability, a Controller Area Network (CAN) bus is widely used as the de facto standard to provide serial communications between Electronic Control Units (ECUs)</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ocvae.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Design and Evaluation of Enumeration Attacks on Package Tracking Systems</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hanbin Jang,
                            
                          
                            
                              Woojoong Ji,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Hyoungshick Kim
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-030-55304-3_28" target="_blank">
                      Australasian Conference on Information Security and Privacy
                      (ACISP)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Most shipping companies provide a package tracking system where customers can easily track their package delivery status when the package is being shipped. However, we present asecurity problem called enumeration attacks against package tracking systems...</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>How Do We Create a Fantabulous Password?</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3366423.3380222" target="_blank">
                      ACM Web Conference
                      (WWW)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Although pronounceability can improve password memorability, most existing password generation approaches have not properly integrated the pronounceability of passwords in their designs. In this work, we demonstrate several shortfalls of current pronounceable password generation
                            approaches, and then propose, ProSemPass, a new method of generating passwords that are pronounceable and semantically meaningful.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>I’ve Got Your Packages: Harvesting Customers’ Delivery Order Information using Package Tracking Number Enumeration Attacks</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon Woo,
                            
                          
                            
                              Hanbin Jang,
                            
                          
                            
                              Woojung Ji,
                            
                          
                            
                              and Hyoungshick Kim
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3366423.3380062" target="_blank">
                      ACM Web Conference
                      (WWW)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>A package tracking number (PTN) is widely used to monitor and track a shipment. Through the lenses of security and privacy, however, a package tracking number can possibly reveal certain personal information, leading to security and privacy breaches.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>FDFtNet: Facing Off Fake Images Using Fake Detection Fine-Tuning Network</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hyeonseong Jeon,
                            
                          
                            
                              Youngoh Bang,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-030-58201-2_28" target="_blank">
                      International Conference on Information Security and Privacy Protection
                      (IFIP SEC)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Creating fake images and videos such as Deepfake has become much easier these days due to the advancement in Generative Adversarial Networks (GANs). Moreover, recent research such as the few-shot learning can create highly realistic personalized fake images with only a few images.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>PassTag: A Graphical-Textual Hybrid Fallback Authentication System</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Joon Kuy Han,
                            
                          
                            
                              Xiaojun Bi,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3320269.3384737" target="_blank">
                      ACM Asia Conference on Computer and Communications Security,
                      (ASIACCS)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Designing a fallback authentication mechanism that is both memorable and strong is a challenging problem because of the trade-off between usability and security. Security questions are popularly used as a fallback authentication method for password recovery.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Tale of Two Browsers: Understanding Users’ Web Browser Choices in South Korea</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jihye Woo,
                            
                          
                            
                              Ji Won Choi,
                            
                          
                            
                              Soyoon Jeon,
                            
                          
                            
                              Joon Kuy Han,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-030-54455-3_1" target="_blank">
                      Asian Workshop on Usable Security
                      (AsiaUSEC)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Internet users in South Korea seem to have clearly different web browser choices and usage patterns compared to the rest of the world, heavily using Internet Explorer (IE) or multiple browsers.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>CANTransfer: Transfer Learning based Intrusion Detection on a Controller Area Network using Convolutional LSTM Network</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://dl.acm.org/doi/10.1145/3341105.3373868" target="_blank">
                      ACM SIGAPP Symposium On Applied Computing
                      (SAC)
                      , 2020
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In-vehiclecommunications, due to simplicity and reliability, a Controller Area Network (CAN) bus is widely used as the de facto standard to provide serial communications between Electronic Control Units (ECUs).</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cantransfer.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
    </div>
  
    
    <div class="year-section" id="year-2019">
      <h4 style="margin-top:40px"><b>2019</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Designing for Fallible Humans</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Jelena Mirkovic and Simon Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/cic48465.2019.00042" target="_blank">
                      International Conference on Collaboration and Internet Computing
                      (CIC)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Security and privacy solutions today are designed with an assumption of a rational user. System designers assume that the user is able to review all information shown to them, consider it along with other information they have, and user priorities, and make a conscious, rational decision in their best interest.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Poster: Classifying Genuine Face images from Disguised Face Images</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Junyaup Kim,
                            
                          
                            
                              Siho Han,
                            
                          
                            
                              and Simon S.Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://ieeexplore.ieee.org/abstract/document/9005683" target="_blank">
                      IEEE International Conference on Big Data
                      (IEEE BigData)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this preliminary work, we aim to detect a target person's face from different similar individuals, Doppelgangers, leveraging the dataset from Disguised Faces in the Wild (DFW) 2018. We use well-known off-the-shelf face detection classifiers, such as ShallowNet, VGG-16, and Xception to evaluate the classification performance. In order to further improve the detection performance, we apply data augmentation. Our preliminary result shows that the Xception model can classify one from different individuals with a 62% accuracy.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cgfi.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Poster: Nickel to Lego: Using Foolgle to Create Adversarial Examples to fool Google Cloud Speech-to-Text API,</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Joon Kuy Han,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://dl.acm.org/doi/10.1145/3319535.3363264" target="_blank">
                      ACM Conference on Computer and Communications Security
                      (CCS)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Many companies offer automatic speech recognition or Speech-to-Text APIs for use in diverse applications. However, audio classification algorithms trained with deep neural networks (DNNs) can sometimes misclassify adversarial examples, posing a significant threat to critical applications.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Deep Learning for Blast Furnaces: Skip-Dense Layers Deep Learning Model to Predict the emaining Time to Close Tap-holes for Blast Furnaces</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Keeyoung Kim,
                            
                          
                            
                              Byeongrak Seo,
                            
                          
                            
                              Sang-Hoon Rhee,
                            
                          
                            
                              Seungmoon Lee,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://dl.acm.org/doi/10.1145/3357384.3357803" target="_blank">
                      ACM International Conference on Information and Knowledge Management
                      (CIKM)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Manufacturing steel requires extremely challenging industrial processes. In particular, predicting the exact time instance of opening and closing tap-holes in a blast furnace has a great influence on steel production efficiency and operating cost, in addition to human safety.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>FakeTalkerDetect: Effective and Practical Realistic Neural Talking Head Detection with a Highly Unbalanced Dataset</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hyeonseong Jeon,
                            
                          
                            
                              Youngoh Bang,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/iccvw.2019.00163" target="_blank">
                      IEEE/CVF ICCV Workshop on Human Behavior Understanding
                      (HBU)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Detecting realistic fake images and videos is an increasingly important and urgent problem because they can be maliciously used. In this work, we propose FakeTalkerDetect, which is based on siamese networks to detect the recently proposed realistic talking head with few-shot learning.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Tensor Decomposition for Anomaly Detection in Space</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Youjin Shin,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://milets19.github.io/papers/milets19_poster_6.pdf" target="_blank">
                      ACM KDD Workshop on Tensor Methods for Emerging Data Science Challenges
                      (TMEDSC)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small></small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/tdfad.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Contextual Anomaly Detection by Correlated Probability Distributions using Kullback-Leibler Divergence</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jinwoo Cho,
                            
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Young Geun Kim,
                            
                          
                            
                              Jeong-Han Yun,
                            
                          
                            
                              Jonguk Kim,
                            
                          
                            
                              Hyoung Chun Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://ygeunkim.github.io/publication/kl_poster/" target="_blank">
                      ACM KDD Workshop on Mining and Learning from Time Series
                      
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small></small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/cad.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Detecting Anomalies in Space using Multivariate Convolutional LSTM with Mixtures of Probabilistic PCA</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Youjin Shin,
                            
                          
                            
                              Myeong Shin Lee,
                            
                          
                            
                              Okchul Jung,
                            
                          
                            
                              Daewon Chung,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3292500.3330776" target="_blank">
                      ACM SIGKDD Conference on Knowledge Discovery and Data Mining
                      (KDD)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Detecting an anomaly is not only important for many terrestrial applications on Earth but also for space applications. Especially, satellite missions are highly risky because unexpected hardware and software failures can occur due to sudden or unforeseen space environment changes.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/dais.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Understanding Users' Risk Perceptions about Personal Health Records Shared on Social Networking Services</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Yuri Son,
                            
                          
                            
                              Geumhwan Cho,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Simon Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3321705.3329838" target="_blank">
                      ACM Asia Conference on Computer and Communications Security,
                      (ASIACCS)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>To understand users' risk perceptions about sharing their PHR on SNS, we first conducted a qualitative user study by interviewing 16 participants. Next, we conducted a large-scale online user study with 497 participants in the U.S. to validate our qualitative results from the first study.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>You Walk, We Authenticate: Lightweight Seamless Authentication Based on Gait in Wearable IoT Systems</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Pratik Musale,
                            
                          
                            
                              Duin Baek,
                            
                          
                            
                              Nuwan Werellagama,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Bong Jun Choi
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/access.2019.2906663" target="_blank">
                      IEEE Access
                      
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =3.557</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>With a plethora of wearable IoT devices available today, we can easily monitor human activities, many of which are unconscious or subconscious. Interestingly, some of these activities exhibit distinct patterns for each individual, which can provide an opportunity to extract useful features for user authentication.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>What is in Your Password? Analyzing Memorable and Secure Passwords using a Tensor Decomposition</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Youjin Shin and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3308558.3313690" target="_blank">
                      ACM Web Conference
                      (WWW)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=3</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In the past, there have been several studies in analyzing password strength and structures. However, there are still many unknown questions to understand what really makes passwords both memorable and strong. In this work, we aim to answer some of these questions by analyzing password dataset through the lenses of data science and machine learning perspectives.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Using Episodic Memory for User Authentication</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Ron Artstein,
                            
                          
                            
                              Elsi Kaiser,
                            
                          
                            
                              Xiao Le,
                            
                          
                            
                              and Jelena Mirkovic
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3308992" target="_blank">
                      ACM Transactions on Transactions on Privacy and Security 
                      (TOPS)
                      , 2019
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =2.1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Passwords are widely  used for user authentication, but they are often difficult for a user to recall, easily cracked by automated programs, and heavily reused. Security questions are also used for secondary authentication. They are more memorable than passwords, because the question serves as a hint to the user, but they are very easily guessed. We propose a new authentication mechanism, called life-experience passwords (LEPs).</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>GAN is a Friend or Foe? A Framework to Detect Various Fake Face Images</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Youjin Shin,
                            
                          
                            
                              Ho Young Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM SIGAPP Symposium on Applied Computing
                      (SAC)
                      , 2019
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Creating fake images such as replacing one's face with other person's face has become much easier due to the advancement of sophisticated image editing tools. In addition, Generative Adversarial Networks (GANs) enable creating natural looking human faces. However, fake images can cause many potential problems, as they can be misused to abuse information, hurt people, and generate fake identification.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/ganfof.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
    </div>
  
    
    <div class="year-section" id="year-2018">
      <h4 style="margin-top:40px"><b>2018</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Design and evaluation of 3D CAPTCHAs</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.cose.2018.12.006" target="_blank">
                      Computers &amp; Security,
                      
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =3.06</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Most current 2D CAPTCHAs are vulnerable to automated character recognition attacks and the latest attacks can successfully break the 2D text CAPTCHAs at a rate of more than 90%. In this work, we present two novel 3D CAPTCHAs, which are more secure than current 2D text CAPTCHAs against automated character recognition attacks.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Poster: Memorability and Security of Image and Text Integrated Authentication System</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Joonkyu Han and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Annual Computer Security Applications Conference
                      (ACSAC)
                      , 2018
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small></small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Evaluating and Breaking Naver’s Audio CAPTCHA using Off-the-Shelf Speech-to-text APIs</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Soyoon Jeon,
                            
                          
                            
                              Jihye Woo,
                            
                          
                            
                              Ji Won Choi,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Information Security and Cryptography-Winter
                      (CISC-W)
                      , 2018
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small></small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Understanding Users’ Perception on Digital Certificate and Their Web Browser Usages in Korea</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jihye Woo,
                            
                          
                            
                              Soyoon Jeon,
                            
                          
                            
                              Ji Won Choi,
                            
                          
                            
                              Hyoungshick Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://dl.acm.org/doi/10.1007/978-3-030-54455-3_1" target="_blank">
                      Conference on Information Security and Cryptography-Winter
                      (CISC-W)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small></small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Password typographical error resilience in honey encryption</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Hoyul Choi,
                            
                          
                            
                              Jongmin Jeong,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Kyungtae Kang,
                            
                          
                            
                              and Junbeom Hur
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.cose.2018.07.020" target="_blank">
                      Computers &amp; Security
                      
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              SCIE Journal
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =2.86</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Honey encryption (HE) is a novel password-based encryption scheme that is secure against brute-force attacks even if users’ passwords have min-entropy. However, in HE, decryption with an incorrect key produces fake messages that appear valid. Hence, password typographical errors may confuse even legitimate users.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Poster: Adversarial Product Review Generation with Word Replacements</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Yimin Zhu and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3243734.3278492" target="_blank">
                      ACM Conference on Computer and Communications Security 
                      (CCS)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Machine learning algorithms including Deep Neural Networks (DNNs) have shown great success in many different areas. However, they are frequently susceptible to adversarial examples, which are maliciously crafted inputs to fool machine learning classifiers. On the other hand, humans cannot distinguish between non-adversarial and adversarial inputs.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/aprg.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Detecting In-vehicle CAN Message Attacks Using Heuristics and RNNs</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Huy Kang Kim,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-030-12085-6_4" target="_blank">
                      International workshop on Information &amp; Operational Technology 
                      (IT &amp; OT)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In vehicle communications, due to simplicity and reliability, a Controller Area Network (CAN) bus is used as the de facto standard to provide serial communication between Electronic Control Units (ECUs). However, prior research reveals that several network-level attacks can be performed on the CAN bus due to the lack of underlying security mechanism.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/dican.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Detecting Both Machine and Human Created Fake Face Images In the Wild</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Shahroz Tariq,
                            
                          
                            
                              Sangyup Lee,
                            
                          
                            
                              Hoyoung Kim,
                            
                          
                            
                              Youjin Shin,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3267357.3267367" target="_blank">
                      CCS Workshop on Multimedia Privacy and Security
                      (MPS)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Due to the significant advancements in image processing and machine learning algorithms, it is much easier to create, edit, and produce high quality images. However, attackers can maliciously use these tools to create legitimate looking but fake images to harm others, bypass image detection algorithms, or fool image recognition classifiers.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
          <div style="flex:0 0 auto;margin-left:15px;display:flex;align-items:flex-start;justify-content:center;max-width:493px;">
            <img loading="lazy" src="/Publications/dbmh.png" style="max-height:330px;max-width:493px;margin-bottom:10px;height:auto;aspect-ratio:auto;" />
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>GuidedPass: Guiding users to create both more memorable and strong passwords</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and and Jelena Mirkovic
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://www.researchgate.net/publication/327469039_GuidedPass_Helping_Users_to_Create_Strong_and_Memorable_Passwords_21st_International_Symposium_RAID_2018_Heraklion_Crete_Greece_September_10-12_2018_Proceedings" target="_blank">
                      International Symposium on Research in Attacks, Intrusions and Defenses
                      (RAID)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=2</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Password meters and policies are currently the only tools helping users to create stronger passwords. However, such tools often do not provide consistent or useful feedback to users, and their suggestions may decrease memorability of resulting passwords.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Poster: Leveraging Semantic Transformation to Investigate Password Habits and Their Causes</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Ameya Hanamsagar,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Chris Kanich,
                            
                          
                            
                              and Jelena Mirkovic
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3173574.3174144" target="_blank">
                      Usenix Symposium on Usable Privacy and Security
                      (SOUPS)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>It is no secret that users have difficulty choosing and remembering strong passwords, especially when asked to choose different passwords across different accounts. While research has shed light on password weaknesses and reuse, less is known about user motivations for following bad password practices.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>When George Clooney Is Not George Clooney: Using GenAttack to Deceive Amazon’s and Naver’s Celebrity Recognition APIs</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Keeyoung Kim and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-319-99828-2_25" target="_blank">
                      International Conference on Information Security and Privacy Protection
                      (IFIP SEC)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=1</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In recent years, significant advancements have been made in detecting and recognizing contents of images using Deep Neural Networks (DNNs). As a result, many companies offer image recognition APIs for use in diverse applications. However, image classification algorithms trained with DNNs can misclassify adversarial examples, posing a significant threat to critical applications.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Generating Adversarial Images using Genetic Algorithm</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Keeyoung Kim and Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://www.researchgate.net/publication/339840424_Generating_Adversarial_Images_using_Genetic_Algorithm" target="_blank">
                      IEEE/CVF CVPR Workshop on The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security
                      (CV-COPS)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Workshop
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small></small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Poster: I can’t hear this because I am human: A novel design of audio CAPTCHA system</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jusop Choi,
                            
                          
                            
                              Taekkyung Oh,
                            
                          
                            
                              William Aiken,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Hyoungshick Kim
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://dl.acm.org/doi/10.1145/3196494.3201590" target="_blank">
                      ACM Asia Conference on Computer and Communications Security
                      (ASIACCS)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>A CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) provides the first line of defense to protect websites against bots and automatic crawling. Recently, audio-based CAPTCHA systems are started to use for visually impaired people in many internet services.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Benefits and Challenges of Long Term Self-Tracking to Prevent Lonely Deaths and Detect Signs of Life</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Human Factors in Computing Systems
                      (CHI)
                      , 2018
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We explore the benefit of a new long-term self-tracking application for the elderly population. In the last few years, there has been a significant increase in number of people dying alone or remaining undiscovered for a long period time in Korea and Japan.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Leveraging Semantic Transformation to Investigate Password Habits and Their Causes</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Ameya Hanamsagar,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Chris Kanich,
                            
                          
                            
                              and Jelena Mirkovic
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/3173574.3174144" target="_blank">
                      Conference on Human Factors in Computing Systems
                      (CHI)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small>
                        <b>
                          <span style="color:#003B8E;">
                            
                            
                              Main Track
                            
                          </span>
                        </b>
                      </small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=4</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>It is no secret that users have difficulty choosing and remembering strong passwords, especially when asked to choose different passwords across different accounts. While research has shed light on password weaknesses and reuse, less is known about user motivations for following bad password practices.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Memorablity and Security of Different Passphrase Generation Methods</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and Jelena Mirković
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE07399563" target="_blank">
                      Korea Institute of Information Security and Cryptology
                      (KIISC)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Passphrases are considered to be more secure than passwords since they are longer than passwords. However, users choose predictable word patterns and common phrases to make passphrases memorable, which in turn significantly lowers security. While random passphrases appear to be stronger, surprisingly they are neither strong nor memorable. In this paper, we present the latest passphrase research, and introduce a new way to create a passphrase using mnemonics. Passphrase generation using mnemonics shows promising results in improving both strength and memorability.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Survey on Current Password Composition Policies</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Kyeong Joo Jung,
                            
                          
                            
                              and Bong Jun Choi
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE07399565" target="_blank">
                      Korea Institute of Information Security and Cryptology
                      (KIISC)
                      , 2018
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Textual passwords are widely used for accessing online accounts. Despite the problems of current textual passwords, research has shown that there is no other strong alternatives for a textual password due to its simplicity.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>
          
        </div>
        <hr />
      
    </div>
  

  
  
    
    <div class="year-section" id="year-older">
      <h4 style="margin-top:40px"><b>2017 &amp; Earlier</b></h4>
      <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>CFDP Performance Over Weather-Dependent Ka-Band Channel</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and Jay Gao
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.2514/6.2006-5968" target="_blank">
                      AIAA International Conference on Space Operations
                      (SpaceOps)
                      , 2006
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>This study presentsan analysis of the delay performance of the CCSDS File Delivery Protocol (CFDP) over weather-dependent Ka-band channel. The Ka-band channel condition is determined by the strength of the atmospheric noise temperature, which is weather dependent.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Improved In Situ Communications Using Network Coding</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Mike Cheng,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Kar-Ming Cheung,
                            
                          
                            
                              Sam Dolinar,
                            
                          
                            
                              and Jon Hamkins
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Research and Technology Development
                      (R&amp;TD)
                      , 2007
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Advances in technology have made the large-scale deployment of low-cost networked sensors possible for situational awareness. We developed a Simulation Tool for the Advanced Sensors Collaborative Technology Alliance (ASCTA) Microsensor Network Architecture (STAMINA) to evaluate the performance of networked sensor systems.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Prioritized LT codes</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and Michael K. Cheng
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/ciss.2008.4558589" target="_blank">
                      IEEE Annual Conference on Information Sciences and Systems
                      (CISS)
                      , 2008
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>It is common in data transmissions that some information is more important than others. This is especially true in space communications where mission critical information or science data are high priority. In this work, we propose a simple yet constructive scheme to send high priority data reliably and efficiently using Luby transform (LT) codes.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Interfacing Space Network Communications and Navigation Network Simulation with Distributed System Integration Laboratories (DSIL)</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Esther Jennings,
                            
                          
                            
                              Sam Nguyen,
                            
                          
                            
                              Shin-Ywan Wang,
                            
                          
                            
                              and Simon Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.2514/6.2008-3462" target="_blank">
                      AIAA International Conference on Space Operations
                      (SpaceOps)
                      , 2008
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>NASA’s planned Lunar missions will involve multiple NASA centers where each participating center has a specific role and specialization. In this vision, the Constellation program (CxP)’s Distributed System Integration Laboratories (DSIL) architecture consist of multiple System Integration Labs (SILs), with simulators, emulators, testlabs and control centers interacting with each other over a broadband network to perform test and verification for mission scenarios.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>A Simulation Tool for ASCTA Microsensor Network Architecture</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon Woo,
                            
                          
                            
                              Esther Jennings,
                            
                          
                            
                              and Loren Clare
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/aero.2008.4526447" target="_blank">
                      IEEE Aerospace Conference
                      ( IEEE Aerospace Conf.)
                      , 2008
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Advances in technology have made the large-scale deployment of low-cost networked sensors possible for situational awareness. We developed a Simulation Tool for the Advanced Sensors Collaborative Technology Alliance (ASCTA) Microsensor Network Architecture (STAMINA) to evaluate the performance of networked sensor systems.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Efficient File Sharing by Multicast - P2P Protocol using Network Coding and Rank Based Peer Selection</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and Tudor M. Stoenescu
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/vetecs.2009.5073526" target="_blank">
                      IEEE Vehicular Technology Conference
                      (VTC)
                      , 2009
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we consider information dissemination and sharing in a highly dynamic peer-to-peer (P2P) communication network. In particular, we explore a network coding technique for transmission and a rank based peer selection (RBPS) method for network formation.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Space Communications and Navigation (SCaN) Network Simulation Tool Development and Its Use Cases</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Esther Jennings,
                            
                          
                            
                              Richard Borgen,
                            
                          
                            
                              Sam Nguyen,
                            
                          
                            
                              John Segui,
                            
                          
                            
                              Tudor Stoenescu,
                            
                          
                            
                              Shin-Ywan Wang,
                            
                          
                            
                              Simon Woo,
                            
                          
                            
                              Brian Barritt,
                            
                          
                            
                              Christine Chevalier,
                            
                          
                            
                              and Wesley Eddy
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.2514/6.2009-6036" target="_blank">
                      AIAA Modeling and Simulation Technologies
                      (MST)
                      , 2009
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>In this work, we focus on the development of a simulation tool to assist in analysis of current and future (proposed) network architectures for NASA. Specifically, the Space Communications and Navigation (SCaN) Network is being architected as an integrated set of new assets
                                and a federation of upgraded legacy systems. The SCaN architecture for the initial
                                missions for returning humans to the moon and beyond will include the Space Network (SN)
                                and the Near-Earth Network (NEN).</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>MACHETE: A Protocol Evaluation Tool for Space-Based Networking Architecture and Simulation</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Esther Jennings,
                            
                          
                            
                              John Segui,
                            
                          
                            
                              and Simon S. Woo
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.2514/6.2010-2260" target="_blank">
                      AIAA International Conference on Space Operations
                      (SpaceOps)
                      , 2010
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Space Exploration missions requires the design and implementation of space networking that differs from terrestrial networks. In a space networking architecture, interplanetary communication protocols need to be designed, validated and evaluated carefully to support different mission requirements.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Space Network Time Distribution and Synchronization  Protocol Development  for Mars Proximity Link</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon Woo,
                            
                          
                            
                              Jay Gao,
                            
                          
                            
                              and David Mills
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.2514/6.2010-2360" target="_blank">
                      AIAA International Conference on Space Operations
                      (SpaceOps)
                      , 2010
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Time distribution and synchronization in deep space network are challenging due to long propagation delays, spacecraft movements, and relativistic effects. Further, the Network Time Protocol (NTP) designed for terrestrial networks may not work properly in space</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Analysis of Proximity-1 Space Link Interleaved Time Synchronization Protocol</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon. S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1109/glocom.2011.6134144" target="_blank">
                      IEEE Global Telecommunications Conference
                      (GLOBECOM)
                      , 2011
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>To synchronize clocks between spacecraft in proximity, the Proximity-1 Space Link Interleaved Time Synchronization (PITS) Protocol has been proposed. PITS is based on the NTP Interleaved On-Wire Protocol and is capable of being adapted and integrated into CCSDS Proximity-1 Space Link with minimal modifications.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Optimal application allocation on multiple public clouds</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and Jelena Mirkovic
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1016/j.comnet.2013.12.001" target="_blank">
                      Computer Networks,
                      
                      , 2014
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">SCIE IF =2.52</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Cloud computing customers currently host all of their application components at a single cloud provider. Single-provider hosting eases maintenance tasks, but reduces resilience to failures. Recent research (Li et al., 2010) also shows that providers offers differ greatly in erformance and price, and no single provider is the best in all service categories.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Life-Experice Passwords</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Jelena Mikovic,
                            
                          
                            
                              Ron Artstein,
                            
                          
                            
                              and Elsi Kaiser
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Who are you?! Adventures in Authentication: ACM SOUPS-WAY Workshop
                      (WAY)
                      , 2014
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Passwords are widely used for user authentication, but they are often difficult for a user to recall, easily cracked by automated programs and heavily reused. Security questions are also used for secondary authentication. They are more memorable than passwords, but are very easily guessed. We propose a new authentication mechanism, called life-experience passwords (LEPs), which outperforms passwords and security questions, both at recall and at security.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Poster: 3DOC: 3D Object CAPTCHA</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and B. Kim
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Information Sciences Institute Graduate Student Symposium 
                      (ISI-GSS)
                      , 2014
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Current 2D CAPTCHA mechanisms can be easily defeated by character recognition and segmentation attacks by automated machines. Recently, 3D CAPTCHA schemes have been proposed to overcome the weaknesses of 2D CAPTCHA for a few websites.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>3DOC: 3D Object CAPTCHA</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and B. Kim
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM Web Conference
                      (WWW)
                      , 2014
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Current 2D CAPTCHA mechanisms can be easily defeated by character recognition and segmentation attacks by automated machines. Recently, 3D CAPTCHA schemes have been proposed to overcome the weaknesses of 2D CAPTCHA for a few websites.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Life Experience-Passwords</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Jelena Mirkovic,
                            
                          
                            
                              and Elsi Kaiser
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Network and Distributed System Security
                      (NDSS)
                      , 2014
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Empirical Data Analysis on User Privacy and Sentiment in Personal Blogs</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and Harsha Manjunatha
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                       ACM SIGIR Workshop on Privacy-Preserving Information Retrieval
                      (PPIR)
                      , 2015
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Engaging Novices in Cybersecurity Competitions: A Vision and Lessons Learned at ACM Tapia 1025</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Jelena Mirković,
                            
                          
                            
                              Aimee Tabor,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Portia Pusey
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://steel.isi.edu/members/simonwoo/pub/pir.pdf" target="_blank">
                      USENIX Summit on Gaming, Games, and Gamification in Security Education
                      (3GSE)
                      , 2015
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Cybersecurity competitions are popular tools for attracting students to cybersecurity field. Yet, many competitions require extensive preparation, strong coding skills and solid background knowledge, not just in security, but also in system administration, networking and operating systems. As such, competitions may discourage novices that lack in one of these required areas. In this paper we discuss our experience in using Class Capture-theFlag Exercises (CCTFs) to bridge this gap in classes, and in 2015 ACM Richard Tapia Security workshop. We recount lessons learned and map a way forward, towards collaborative, more structured cybersecurity competitions that better support and engage novices, and offer a positive learning experience to all.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Exploration of 3D Texture and Projection for New CAPTCHA Design</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Jingul Kim,
                            
                          
                            
                              Duoduo Yu,
                            
                          
                            
                              and Beomjun Kim
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1007/978-3-319-56549-1_30" target="_blank">
                      World Conference on Information Security Applications
                      (WISA)
                      , 2016
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Most of current text-based CAPTCHAs have been shown to be easily breakable. In this work, we present two novel 3D CAPTCHA designs, which are more secure than current 2D text CAPTCHAs, against automated attacks. Our approach is to display CAPTCHA characters onto 3D objects to improve security.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Good Automatic Authentication Question Generation</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon Woo,
                            
                          
                            
                              Zuyao Li,
                            
                          
                            
                              and Jelena Mirkovic
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.18653/v1/w16-6632" target="_blank">
                      International Natural Language Generation conference
                      (INLG)
                      , 2016
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>We explore a novel application of Question Generation (QG) for authentication use, where questions are widely used to verify user identity for online accounts. In our approach, we prompt users to provide a few sentences about their personal life events.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Life-experience passwords (LEPs)</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon Woo,
                            
                          
                            
                              Elsi Kaiser,
                            
                          
                            
                              Ron Artstein,
                            
                          
                            
                              and Jelena Mirkovic
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/2991079.2991107" target="_blank">
                      Annual Conference on Computer Security Applications
                      (ACSAC)
                      , 2016
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;">
                      <small><b><span style="color:#1E90FF;">BK Computer Science IF=2</span></b></small>
                    </p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Passwords are widely used for user authentication, but they are often difficult for a user to recall, easily cracked by automated programs and heavily reused. Security questions are also used for secondary authentication. They are more memorable than passwords, but are very easily guessed. We propose a new authentication mechanism, called life-experience passwords (LEPs), which outperforms passwords and security questions, both at recall and at security.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Improving Recall and Security of Passphrases Through Use of Mnemonics</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo and Jelena Mirkovic
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      International Conference on Passwords 
                      (Password)
                      , 2016
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
                <tr>
                  <td>
                    <!-- thumbs.png 버튼 -->
                    <span class="abstract-toggle">
                      <img src="/Publications/paper.svg" alt="Toggle abstract" />
                      <small>Show abstract</small>
                    </span>
                    <!-- 실제 abstract (처음엔 숨김) -->
                    <div class="abstract-content">
                      <p style="margin:0;margin-top:4px;text-align:justify;">
                        <small>Passphrases are regarded as more secure than passwords because they are longer than passwords. Yet, users use predictable word patterns and common phrases to make passphrases memorable, which in turn significantly lowers security.</small>
                      </p>
                    </div>
                  </td>
                </tr>
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Computer Vision Attacks against 3D CAPTCHAs</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          Simon S. Woo
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      IEEE/CVF CVPR Workshop on The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security
                      (CV-COPS)
                      , 2017
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Toward Machine Generated Passwords</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              Wenzhi Li,
                            
                          
                            
                              and Hyeran Jeon
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      Conference on Information Security and Cryptography-Winter
                      (CISC-W)
                      , 2017
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Lightweight Authentication for IoT</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Pratik Musale,
                            
                          
                            
                              Duin Baek,
                            
                          
                            
                              Simon S. Woo,
                            
                          
                            
                              and Bong Jun Choi
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      
                      ACM Conference on Emerging Networking Experiments and Technologies
                      (CoNEXT)
                      , 2017
                      
                    </b></small></p>
                  </td>
                </tr>
                
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
        <div style="display:flex;flex-direction:row;justify-content:space-between;align-items:flex-start;flex-wrap:nowrap;">
          <div style="margin-left:25px;margin-right:10px;flex:1;min-width:0;">
            <table>
              <thead>
                <tr>
                  <th style="text-align:left;">
                    <b>Life-experience passwords (LEPs)</b>
                  </th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small>
                      
                        
                        
                          
                            
                              Simon Woo,
                            
                          
                            
                              Elsi Kaiser,
                            
                          
                            
                              Ron Artstein,
                            
                          
                            
                              and Jelena Mirkovic
                            
                          
                        
                      
                    </small></p>
                  </td>
                </tr>
                <tr>
                  <td>
                    <p style="margin:0;text-align:justify;"><small><b>
                      <a href="https://doi.org/10.1145/2991079.2991107" target="_blank">
                      Usenix Symposium on Usable Privacy and Security
                      (SOUPS)
                      , 2017
                      </a>
                    </b></small></p>
                  </td>
                </tr>
                
                
              </tbody>
            </table>
          </div>

          
        </div>
        <hr />
      
    </div>
  
</div>

<script>
let currentYear = null;
let years = [];

// 1) DOM에서 연도 목록 자동 수집 (버튼 우선, 없으면 섹션에서)
function collectYears() {
  const btnYears = Array.from(document.querySelectorAll('.year-btn'))
    .map(b => b.dataset.year)
    .filter(Boolean);

  const sectionYears = Array.from(document.querySelectorAll('.year-section[id^="year-"]'))
    .map(sec => sec.id.replace('year-', ''))
    .filter(Boolean);

  const raw = btnYears.length ? btnYears : sectionYears;
  const uniq = Array.from(new Set(raw));

  // 숫자 연도 내림차순 정렬, 'older'는 맨 뒤
  uniq.sort((a, b) => {
    if (a === 'older') return 1;
    if (b === 'older') return -1;
    return Number(b) - Number(a);
  });

  years = uniq;
}

function showYear(year) {
  // 모든 연도 섹션 숨기기
  document.querySelectorAll('.year-section').forEach(section => {
    section.classList.remove('active');
  });
  // 해당 연도 섹션 보이기
  const selected = document.getElementById('year-' + year);
  if (selected) selected.classList.add('active');

  // 버튼 active 토글
  document.querySelectorAll('.year-btn').forEach(btn => btn.classList.remove('active'));
  const activeBtn = document.querySelector(`.year-btn[data-year="${year}"]`);
  if (activeBtn) activeBtn.classList.add('active');

  currentYear = year;
  updateNavigationButtons();

  // URL 해시에 반영(깊은 링크/뒤로가기 지원)
  if (location.hash !== '#' + year) {
    history.replaceState(null, '', '#' + year);
  }
}

function previousYear() {
  const i = years.indexOf(currentYear);
  if (i > 0) showYear(years[i - 1]);
}

function nextYear() {
  const i = years.indexOf(currentYear);
  if (i >= 0 && i < years.length - 1) showYear(years[i + 1]);
}

function updateNavigationButtons() {
  const i = years.indexOf(currentYear);
  const prevBtn = document.getElementById('prevBtn');
  const nextBtn = document.getElementById('nextBtn');
  if (prevBtn) prevBtn.disabled = i <= 0;
  if (nextBtn) nextBtn.disabled = i === -1 || i >= years.length - 1;
}

// 해시 변경 시에도 연도 전환(뒤로/앞으로 버튼 대응)
window.addEventListener('hashchange', () => {
  const y = (location.hash || '').replace('#','');
  if (y && years.includes(y)) showYear(y);
});

// 초기화
document.addEventListener('DOMContentLoaded', () => {
  collectYears();

  // 초기 표시 연도: URL 해시 > .year-btn 중 active 가진 것 > 첫 번째 연도
  const hashYear   = (location.hash || '').replace('#','');
  const activeBtn  = document.querySelector('.year-btn.active');
  const btnDefault = activeBtn ? activeBtn.dataset.year : null;
  const fallback   = years.length ? years[0] : null;

  const initial = hashYear || btnDefault || fallback;
  if (initial) showYear(initial);
});
//--------------------------

// =============================
// 트랙: Main / Dataset / SCIE Journals / Etc
// 규칙 요약
//  - Main, Dataset: Factor[0] != "" AND Number(Factor[1]) > 0 인 항목만 카운트, venue별 집계
//  - SCIE Journals, Etc: 조건 없이 집계, 트랙명 단일 배지로 표시
// =============================

// 색상
const COLORS = {
  main: "brightgreen",
  dataset: "purple",
  journal: "red",
  etc: "gray",
};

// 라벨
const TRACK_LABEL = {
  main: "Main",
  short: "Short",
  workshop: "Workshop",
  dataset: "Dataset",
  journal: "SCIE Journal",
  etc: "Etc.",
};
/* 
function inferTrack(p) {
  const tRaw = (p.track || "").trim();
  const t = tRaw.toLowerCase(); // "SCIE Journal" → "scie journal"
  const vf   = (p.venue_full || "").toLowerCase();
  const v    = (p.venue || "").toLowerCase();

  // 명시 track 우선
   // 1) 명확한 SCIE Journal 케이스
  if (t === "scie journal") return "journal";
  if (t === "main")     return "main";
  if (t === "dataset")  return "dataset";
  if (t === "short")    return "short";
  if (t === "workshop") return "workshop";
  if (t === "etc.")      return "etc";
 // 저널 트랙을 명시로 받는 경우 (예: "journal", "scie", "sci", "sciejournal")
  if (t === "journal" || t.includes("scie") || t === "sci" || t.replace(/\s+/g,"") === "sciejournal") return "journal";

  // 3) 휴리스틱
  if (p.is_journal === true) return "journal";
  if (/journal|transactions|letters/.test(vf) || /journal|transactions|letters/.test(v))
    return "journal";
  if (/dataset|benchmark/.test(vf)) return "dataset";

  return "etc";
}


// Factor 유효성: ["문자열", 숫자]이며 0 초과
function factorValid(p) {
  const F = p.Factor || p.factor || null;
  if (!Array.isArray(F) || F.length < 2) return false;
  const f0 = String(F[0] ?? "").trim();
  const f1 = Number(F[1]);
  return f0 !== "" && Number.isFinite(f1) && f1 > 0;
} 

// venue 정규화
function venueLabel(p) {
  return (p.venue || p.venue_full || "Unknown").replace(/\s+/g, " ").trim();
}
function mergeCounts(...dicts) {
  const out = {};
  for (const d of dicts) {
    for (const [k, v] of Object.entries(d)) {
      out[k] = (out[k] || 0) + v;
    }
  }
  return out;
}

// 배지 HTML
function makeShield(label, count, color) {
  const encoded = encodeURIComponent(label);
  const url = `https://img.shields.io/badge/${encoded}-${count}-${color}?style=social`;
  return `<img loading="lazy" alt="${label}" src="${url}" height="20" style="margin-right:6px;">`;
}

// 카운트: 트랙별
// - main/dataset: venue별 dict
// - journal/etc: 단일 키(트랙표시 라벨)로 집계
function countForTrack(items, track) {
  const dict = {};
  for (const p of items || []) {
    const tr  = inferTrack(p);
    // ★ 트랙 필터 필수
    if (tr !== track) continue;

    if (track === "main" || track === "dataset" || track === "short") {
          // Factor 조건 필수(기존 규칙 유지)
          if (!factorValid(p)) continue;
          const key = venueLabel(p);          // venue 단위 집계
          dict[key] = (dict[key] || 0) + 1;
        } else {
          // journal / etc: 조건 없이 합산, 트랙명 단일 라벨
          const key = TRACK_LABEL[track];
          dict[key] = (dict[key] || 0) + 1;
        }
  }
  return dict;
}

// 정렬: count desc > 알파벳
function sortCounts(dict, track) {
  const arr = Object.entries(dict);
  if (arr.length <= 1) return arr;

  // --- Main / Short / Dataset: BK factor 기준 + 알파벳 정렬 ---
  if (track === "main" || track === "short" || track === "dataset") {
    const bkMap = {}; // bkMap[venueLabel] = 최대 BK 값(Factor[1])

    (window.PUBS || []).forEach(p => {
      const label = venueLabel(p);
      if (!Object.prototype.hasOwnProperty.call(dict, label)) return;

      const F = p.Factor || p.factor || null;
      if (!Array.isArray(F) || F.length < 2) return;

      const f1 = Number(F[1]);
      if (!Number.isFinite(f1)) return;

      if (!(label in bkMap) || f1 > bkMap[label]) {
        bkMap[label] = f1;
      }
    });

    return arr.sort((a, b) => {
      const [labelA] = a;
      const [labelB] = b;

      const bkA = bkMap[labelA] || 0;
      const bkB = bkMap[labelB] || 0;

      // 1) BK 내림차순
      if (bkB !== bkA) return bkB - bkA;
      // 2) 같은 BK 안에서는 학회명 알파벳순
      return labelA.localeCompare(labelB);
    });
  }

  // --- 나머지 트랙들(journal, etc): 알파벳만 ---
  return arr.sort((a, b) => {
    const [labelA] = a;
    const [labelB] = b;
    return labelA.localeCompare(labelB);
  });
}

function badgeListFor(track, suffix, color) {
  const dict = countForTrack(window.PUBS, track);
  return sortCounts(dict,track)
    .map(([label, n]) => makeShield(label + suffix, n, color))
    .join("&nbsp;");
}

function renderDatasetSection(mountId) {
  const el = document.getElementById(mountId);
  if (!el) return;

  const color = COLORS.dataset;

  // 1) 각 트랙 카운트
  const dictDataset = countForTrack(window.PUBS, "dataset");
  const dictShort   = countForTrack(window.PUBS, "short");

  // 2) 트랙별 정렬 후 라벨에 접미사 부여
  //    → 여기서 sortCounts가 BK 기준 정렬을 이미 해줌
  const entriesDataset = sortCounts(dictDataset, "dataset")
    .map(([label, n]) => [label + " Dataset", n]);

  const entriesShort   = sortCounts(dictShort, "short")
    .map(([label, n]) => [label + " Short", n]);

  // 3) 병합: Dataset 먼저, 그다음 Short
  //    (각 그룹 내부는 BK 기준으로 정렬된 상태)
  const merged = [...entriesDataset, ...entriesShort];

  // 4) 렌더
  const html = merged
    .map(([label, n]) => makeShield(label, n, color))
    .join("&nbsp;");
  el.innerHTML = html;
}


function renderBadges(track, mountId) {
  const el = document.getElementById(mountId);
  if (!el) return;

  const dict  = countForTrack(window.PUBS, track);
  const color = COLORS[track];

  let html = "";

  if (track === "main") {
    // 메인: venue별 배지
    html = sortCounts(dict,"main")
      .map(([label, n]) => makeShield(label + " Main", n, color))
      .join("&nbsp;");

  } else if (track === "dataset") {
    // 참고: 실제 화면은 renderDatasetSection에서 처리하지만,
    // 혹시 호출되더라도 안전하게 동작하도록 기본 렌더 유지
    html = sortCounts(dict,"dataset")
      .map(([label, n]) => makeShield(label + " Dataset", n, color))
      .join("&nbsp;");

  } else {
    // journal / etc: 단일 배지
    const total = Object.values(dict).reduce((s, v) => s + v, 0);
    html = total > 0 ? makeShield(TRACK_LABEL[track], total, color) : "";
  }

  el.innerHTML = html;
}

function renderDomesticWorkshopsBadges(mountId) {
  const el = document.getElementById(mountId);
  if (!el) return;

  // track 값: 'etc'와 'workshop'을 그대로 카운트
  const etcDict = countForTrack(window.PUBS, "etc");
  const wksDict = countForTrack(window.PUBS, "workshop");

  const etcTotal = Object.values(etcDict).reduce((s,v)=>s+v,0);
  const wksTotal = Object.values(wksDict).reduce((s,v)=>s+v,0);

  // 표시용 라벨을 JSON track 대신 고정 텍스트로
  const html = [
    makeShield("Other Papers", etcTotal, COLORS.etc),
    makeShield("Workshops",       wksTotal, COLORS.etc),
  ].join("&nbsp;");

  el.innerHTML = html;
}
*/
document.addEventListener("DOMContentLoaded", () => {
  //renderBadges("main", "badges-main");
  //renderDatasetSection("badges-dataset");
  //renderBadges("journal", "badges-journal");
  //renderDomesticWorkshopsBadges("badges-etc");

  // -------------------------
  // thumbs 버튼 클릭 → abstract 토글
  // -------------------------
  document.querySelectorAll(".abstract-toggle").forEach(btn => {
    const content = btn.nextElementSibling;  // 바로 뒤 div.abstract-content
    if (!content) return;

    btn.addEventListener("click", () => {
      content.classList.toggle("open");
    });
  });
});

</script>



</div>

    </main>
    
      <footer class="page-footer Green">
    <div class="container">
      <div class="row">
        <div class="col l6 s12">
          <h5 class="white-text">DASH Lab</h5>
          <p class="grey-text text-lighten-4">N Center 86401, Sungkyunkwan University, 2066 Seobu-ro Jangan-gu Suwon, South Korea</p>
<!--           &nbsp;&nbsp;&nbsp;<a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fdash-lab.github.io&count_bg=%23004274&title_bg=%23004274&icon=android.svg&icon_color=%23E7E7E7&title=Visits&edge_flat=false"/></a> <br> -->
<!--           <div id="id3241554892540" a='{"t":"b","v":"1.2","lang":"en","locs":[873,879],"ssot":"c","sics":"ds","cbkg":"#FFFFFF00","cfnt":"rgba(255,255,255,1)","ceb":"#ffffff00","cef":"rgba(255,255,255,1)","slmw":172,"slbr":0,"slfs":13}'><a href="https://sharpweather.com/widgets/">Weather widget html for website by sharpweather.com</a></div> -->
          <div class="col l4 s12">
          <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=202&t=tt&d=cQzNXuve8H7E3SzSJQ8IOQknuoDwXvZEnWvx94wdKDQ&co=004274&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>
          </div>
          <div class="col l6 s12" style="display: flex;  align-items: center; justify-content: center;">
              <br>
              <br>
          <div id="id3241554892540" a='{"t":"b","v":"1.2","lang":"en","locs":[873,879],"ssot":"c","sics":"ds","cbkg":"#FFFFFF00","cfnt":"rgba(255,255,255,1)","ceb":"#ffffff00","cef":"rgba(255,255,255,1)","slmw":172,"slbr":0,"slfs":13}'><a href="https://sharpweather.com/widgets/">Weather widget html for website by sharpweather.com</a></div>
          </div>
        
        </div>
        <div class="col l3 s12">
          <h5 class="white-text">Link</h5>
          <ul>
            <li><a class="white-text" href="https://twitter.com/TheDASHLab">DASH LAB Twitter</a></li>
            <li><a class="white-text" href="https://gradschool.skku.edu/grad/">SKKU Graduate shcool</a></li>
            <li><a class="white-text" href="https://sci-cube.skku.edu/sci-cube/index.do">Applied Data Science dept</a></li>
            <li><a class="white-text" href="https://cs.skku.edu/">Computer Science and Engineering dept</a></li>
            <li><a class="white-text" href="https://ai.skku.edu/ai/index.do">Department of Artificial Intelligence</a></li>
            <li><a class="white-text" href="https://skb.skku.edu/skkuaai/index.do">Department of Applied Artificial Intelligence</a></li>
          </ul>
        </div>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <!-- Add font awesome icons -->
<!-- Twitter Embed Block with Smaller Size -->
        <div class="col l3 s12">
      <h5 class="white-text">Tweets from DASH-Lab</h5>
     <div style="margin:-50px -50px;max-width: 500px; max-height: 600px; overflow: hidden;transform: scale(0.7)">
        <blockquote class="twitter-tweet" style="transform: scale(0.8); transform-origin: top left;">
          <p lang="en" dir="ltr">
            Exploring the Impact of Moire Pattern on Deepfake Detectors<br><br>Razaib Tariq, Shahroz Tariq, Simon S. Woo<br>tl;dr: if you capture the video by using a phone looking at screen with video, this doesn&#39;t help you computer vision performance, quite contrary :)
            <a href="https://t.co/u1ITzJ8PNf">https://t.co/u1ITzJ8PNf</a> 
            <a href="https://t.co/5QL6AqqanF">pic.twitter.com/5QL6AqqanF</a>
          </p>
          &mdash; Dmytro Mishkin 🇺🇦 (@ducha_aiki) 
          <a href="https://twitter.com/ducha_aiki/status/1813542029286429178?ref_src=twsrc%5Etfw">July 17, 2024</a>
        </blockquote>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
      </div>
        </div>

        <!-- <a href="https://twitter.com/TheDASHLab" class="fa fa-twitter fa-stack-2x"></a> -->
        <!-- <a class="twitter-timeline" href="https://twitter.com/TheDASHLab?ref_src=twsrc%5Etfw">Tweets by TheDASHLab</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> -->
        
      </div>
    </div>
    <div class="footer-copyright">
        <div class="container">
            <div>
                Made by Dash lab</a>
                <span style="text-align: right; float: right">
                  Copyright © <span id="current-year"></span> <a href="https://x.com/TheDASHLab" class="white-text text-lighten-3">DASH-lab</a>. Powered by <a target="_blank" href="http://jekyllrb.com/" class="orange-text text-lighten-3">Jekyll</a>
                </span>
                <script>
                  // Automatically updates the year
                  document.getElementById('current-year').textContent = new Date().getFullYear();
                </script>
            </div>
        </div>
    </div>
</footer>

      
    <!--  Scripts-->                                                                               
<script src="/js/jquery.min.js"></script>
<script src="/js/materialize.min.js"></script>
<script src="/js/init.js"></script>
<script async src="https://static1.sharpweather.com/widgetjs/?id=id3241554892540"></script>
<!--Script for the Pop-up advertisement-->



<!-- This is the code for 1 pop-up window.  -->

<script>
    setTimeout(function() {
        // Show only the second popup
        document.querySelector("#popup2").style.opacity = '1';
        document.querySelector("#popup2").style.transform = 'scale(1)';
        document.querySelector("#popup2").style.right = 0;
		
        document.querySelector("#popup1").style.opacity = '1';
        document.querySelector("#popup1").style.transform = 'scale(1)';
        // document.querySelector("#popup1").style.right = 0;
        document.querySelector("#popup1").style.left = 0;


		
        document.querySelector("#popup3").style.opacity = '1';
        document.querySelector("#popup3").style.transform = 'scale(1)';
        // document.querySelector("#popup2").style.right = 0;
    }, 10);


    // Close second popup
    document.querySelector("#close2").addEventListener("click", function(){
        document.querySelector("#popup2").style.transform = 'scale(0)';
        document.querySelector("#popup2").style.display = "none";
    });

	document.querySelector("#close1").addEventListener("click", function(){
        document.querySelector("#popup1").style.transform = 'scale(0)';
        document.querySelector("#popup1").style.display = "none";
    });

document.querySelector("#close3").addEventListener("click", function(){
	document.querySelector("#popup3").style.transform = 'scale(0)';
	document.querySelector("#popup3").style.display = "none";
});
</script>

<script>


    // Close second popup
    
</script>


<!-- Script for randomly choose an thumbnail image on home page -->
<script>
	// Array of image URLs
	var images = [
	"../img/dash_gallery/coverPage.jpg"
	];
  
	// Select a random image from the array
	var randomImage = images[Math.floor(Math.random() * images.length)];
  
	// Get the div element
	var div = document.getElementById('index-banner');
  
	// Set the div's background image to the random image
	div.style.backgroundImage = " linear-gradient(rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5)), url('" + randomImage + "')";
  </script>
<!-- flickity courasel -->
<script src="../js/flickity.pkgd.min.js"></script>

<!-- Script of forensics Inpectation -->
<script>
	const upload = document.getElementById('upload');
	const canvas = document.getElementById('canvas');
	const preview = document.getElementById('preview');
	const ctx = canvas.getContext('2d');
	const previewCtx = preview.getContext('2d');
	const toggleSwitch = document.getElementById('toggle-switch');
	let showOriginal = false;
	toggleSwitch.addEventListener('change', () => {
        if (toggleSwitch.checked) {
            // Switch to the right
            toggleSwitch.nextElementSibling.style.backgroundColor = '#007bff';
            toggleSwitch.nextElementSibling.firstElementChild.style.transform = 'translateX(26px)';
        } else {
            // Switch to the left
            toggleSwitch.nextElementSibling.style.backgroundColor = '#ccc';
            toggleSwitch.nextElementSibling.firstElementChild.style.transform = 'translateX(0)';
        }
		showOriginal = toggleSwitch.checked;
    	});
	upload.addEventListener('change', (event) => {
		const file = event.target.files[0];
		if (file) {
			const reader = new FileReader();
			reader.onload = function(e) {
				const img = new Image();
				img.onload = function() {
					const windowHeight = window.innerHeight;
					const windowWidth = window.innerWidth;
					const maxHeight = windowHeight * 0.8;
					const maxWidth = windowWidth * 0.8;

					if (img.height > maxHeight) {
						const scaleFactor = maxHeight / img.height;
						img.width *= scaleFactor;
						img.height = maxHeight;
					}
					if (img.width > maxWidth) {
						const scaleFactor = maxWidth / img.width;
						img.height *= scaleFactor;
						img.width = maxWidth;
					}
					canvas.width = img.width;
					canvas.height = img.height;
					ctx.drawImage(img, 0, 0, img.width, img.height);
					preview.style.display = 'block'; // Show preview canvas
				}
				img.src = e.target.result;
			}
			reader.readAsDataURL(file);
		}
	});

	canvas.addEventListener('mousemove', (event) => {
		const rect = canvas.getBoundingClientRect();
		const x = event.clientX - rect.left;
		const y = event.clientY - rect.top;

		const cropX = Math.max(0, Math.min(x - 32, canvas.width - 64));
		const cropY = Math.max(0, Math.min(y - 32, canvas.height - 64));

		const imageData = ctx.getImageData(cropX, cropY, 64, 64);
		if (showOriginal) {
	            // Show original image area
	            previewCtx.clearRect(0, 0, preview.width, preview.height);
	            previewCtx.imageSmoothingEnabled = false;
	            previewCtx.drawImage(canvas, cropX, cropY, 64, 64, 0, 0, 256, 256);
	        } else {
			const data = imageData.data;
	
			// Separate the channels
			const r = [], g = [], b = [];
			for (let i = 0; i < data.length; i += 4) {
				r.push(data[i]);
				g.push(data[i + 1]);
				b.push(data[i + 2]);
			}
	
			// Apply histogram equalization to each channel
			const equalize = (channel) => {
				const histogram = new Array(256).fill(0);
				for (let i = 0; i < channel.length; i++) {
					histogram[channel[i]]++;
				}
	
				const cdf = new Array(256).fill(0);
				cdf[0] = histogram[0];
				for (let i = 1; i < 256; i++) {
					cdf[i] = cdf[i - 1] + histogram[i];
				}
	
				const cdfMin = cdf.find(value => value > 0);
				const equalized = channel.map(value => Math.round((cdf[value] - cdfMin) / (channel.length - cdfMin) * 255));
				return equalized;
			};
	
			const rEqualized = equalize(r);
			const gEqualized = equalize(g);
			const bEqualized = equalize(b);
	
			// Combine the channels back
			for (let i = 0; i < data.length; i += 4) {
				data[i] = rEqualized[i / 4];
				data[i + 1] = gEqualized[i / 4];
				data[i + 2] = bEqualized[i / 4];
			}
	
			// Create a temporary canvas to draw the transformed image data
			const tempCanvas = document.createElement('canvas');
			const tempCtx = tempCanvas.getContext('2d');
			tempCanvas.width = 64;
			tempCanvas.height = 64;
	
			// Put the transformed image data into the temporary canvas
			const transformedImageData = new ImageData(data, 64, 64);
			tempCtx.putImageData(transformedImageData, 0, 0);
	
			// Clear the preview canvas and draw the scaled image
			previewCtx.clearRect(0, 0, preview.width, preview.height);
			previewCtx.imageSmoothingEnabled = false; // Disable smoothing
			previewCtx.drawImage(tempCanvas, 0, 0, 64, 64, 0, 0, 256, 256);
		}
		const offsetX = window.innerWidth * 0.12;  // 12% of the viewport width
		const offsetY = window.innerHeight * 0.12; // 12% of the viewport height
		
		preview.style.left = `${event.pageX - offsetX}px`;
		preview.style.top = `${event.pageY - offsetY}px`;
	});
	canvas.addEventListener('mouseleave', () => {
		preview.style.display = 'none'; // Hide preview canvas
	});
	canvas.addEventListener('mouseenter', () => {
		preview.style.display = 'block'; // Show preview canvas
	});
</script>


  </body>

</html>
